<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Loss Fun Ctions | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Loss Fun Ctions" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MSELoss()" />
<meta property="og:description" content="MSELoss()" />
<link rel="canonical" href="https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html" />
<meta property="og:url" content="https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-09T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"MSELoss()","dateModified":"2020-09-09T00:00:00-05:00","datePublished":"2020-09-09T00:00:00-05:00","headline":"Loss Fun Ctions","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html"},"url":"https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastMLBlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/fastMLBlog/feed.xml" title="Supra_PolyglotCoder" /><link rel="shortcut icon" type="image/x-icon" href="/fastMLBlog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Loss Fun Ctions | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Loss Fun Ctions" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MSELoss()" />
<meta property="og:description" content="MSELoss()" />
<link rel="canonical" href="https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html" />
<meta property="og:url" content="https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-09T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"MSELoss()","dateModified":"2020-09-09T00:00:00-05:00","datePublished":"2020-09-09T00:00:00-05:00","headline":"Loss Fun Ctions","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html"},"url":"https://www.vanhp.com/fastMLBlog/2020/09/09/Loss-Fun-ctions.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/fastMLBlog/feed.xml" title="Supra_PolyglotCoder" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><!--
      edit by vanh original code header.html
    <a class="site-title" rel="author" href="/fastMLBlog/">Supra_PolyglotCoder</a>
    replace with custom code below
    -->
    <a class="site-title" rel="author" href="/fastMLBlog/"><img src="/fastMLBlog/images/gradlogo1.jpg" alt="Supra_PolyglotCoder"></img></a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastMLBlog/about/">About Me</a><a class="page-link" href="/fastMLBlog/ML-section/">ML</a><a class="page-link" href="/fastMLBlog/Coding-section/">Programming</a><a class="page-link" href="/fastMLBlog/index.markdown.org">Main</a><a class="page-link" href="/fastMLBlog/search/">Search</a><a class="page-link" href="/fastMLBlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <!-- add feature image on top of post --><img src="/fastMLBlog/images/dev-tool-pic.jpgalt=" class=featured-image-post" /><h1 class="post-title p-name" itemprop="name headline">Loss Fun Ctions</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-09T00:00:00-05:00" itemprop="datePublished">
        Sep 9, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="mseloss">MSELoss()</h2>

<p>Measure the mean square error (L2 norm) between each element in input x and target y</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%5C%20%7Bl_1%2C...%2Cl_n%5C%7D%5ET%20%2C%20ln%20%3D%20\(x_n%20-%20y_n\)%5E2#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image36.png" alt="" /></a> where N is the batch size</p>

<h2 id="l1loss">L1Loss()</h2>

<p>Basically the absolute value of the different between desired output and actual output</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%5C%20%7Bl_1%2C...%2Cl_n%5C%7D%5ET%20%2C%20ln%20%3D%20%7Cx_n%20-%20y_n%7C#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image32.png" alt="" /></a> where N is the batch size</p>

<p>Use for robust regression where small error to count more and large not as much to reduce noise in data e.g. to reduce the influent of a few outlier in dataset but L1 loss is not differentiable at the bottom when gradient get closer to the bottom normally is done with soft shrink, some fix of this problem e.g. smooth L1 loss to make bottom smooth like L2 with Hubert loss to protect against outliers</p>

<p>SmoothL1Loss()</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=loss\(x%2Cy\)%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%7D%20Z_i%20%7B%200.5\(x_i%20-%20y_i\)%5E2%2C%20if%20%7Cx_i%20-%20y_i%7C%20%3C1%2C%7Cx_i%20-%20y_i%7C%20-%200.5#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image21.png" alt="" /></a> otherwise</p>

<p>Note the average value make image blur</p>

<p>NLLLoss()</p>

<p>The negative log likelihood loss useful for reduce discrepancy among dataset</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%20%5C%7Bl_1%2C...%2Cl_n%5C%7D%5ET%20%2C%20l_n%20%3D%20-w_y%2Cx_n%2Cy_n%2C%20w_c%20%3D%20weight%5Bc%5D%20*%201%5C%7B%20where%20C%20%3D%20num%20%5Chspace%7B5%7D%20of%20class%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image10.gif" alt="" /></a></p>

<p>Try to pick the correct value in the output vector as large as possible</p>

<p>If there is a negative in front it can be interpret x as energy</p>

<p>Useful for widely different frequency (size) data category this by assign larger weight for the category that have small dataset and smaller weight to category that have large among of data, but better solution is to increase the frequency of minimal category instead by reuse the same set of smaller dataset over until the larger dataset exhaust to equalize them out this method exploit SGD better.Never leave any data unused</p>

<h2 id="crossentropyloss">CrossEntropyLoss()</h2>

<p>Merging the log softmax(do log after the softmax) and NLLLoss is for numerical reasons</p>

<p>Since the log softmax step might have gradient value infinite in the middle of the step (because some values in softmax might be very small close to 0 value take log of this value result in negative infinity ) and gradient of this value also close to infinity Alway do the softmax and log in the same time to avoid this problem to get a more stable numerical value. By combining these 2 it makes the correct value largest as possible and suppresses all the others scores as small as possible due to normalization. The loss take and X factor and a category a desired category a class then compute the negative log of softmax applied to vector of scores the numerator is the X of the index of the correct class that is the loss the negative log of exponential score of the correct class divided by the sum of the exponential of all the scores. The X can be thought of as negative energy. The negative score of the correct class and to make this value small make the score large and all the sum of exponential of other value small this make the edge small</p>

<p>Cross entropy is the cross of distribution of the softmax vector and target value vector (one hot)that have one value that close to 1</p>

<p>AdaptiveLogSoftmaxWithLoss()</p>

<p>Extension to log softmax useful with the dataset that have large categories e.g. in NLP by use some like ignore the small categories and focus on the large one</p>

<h2 id="bceloss">BCELoss()</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%20%5C%7Bl_1%2C...%2Cl_n%5C%7D%5ET%20%2C%20l_n%20%3D%20-w_n%5By_n%20*%20log%20x_n%20%2B%20\(1-y_n\)*%20log\(1-x_n\)%5D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image30.png" alt="" /></a></p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=l\(x%2Cy\)%20%3D%20%5C%7Bmean\(L\)%2C%20if%20%20reduction%20%3D%20mean%20%2Csum\(L\)%20if%20reduction%20%3D%20sum%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image15.png" alt="" /></a></p>

<h2 id="kldivloss">KLDivLoss()</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%20%5C%7Bl_1%2C...%2Cl_n%5C%7D%20%2C%20l_n%20%3D%20y_n%20*%20\(log%20y_n%20-%20x_n\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image19.png" alt="" /></a> where N span all dimension of input and L have the same shape as input if reduction is not none</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=l\(x%2Cy\)%20%3D%5C%7Bmean\(L\)%2C%20if%20%5Chspace%7B5%7Dreduction%20%3D%20mean%2Csum\(L\)%20if%5Chspace%7B5%7D%20reduction%20%3D%20sum%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image22.gif" alt="" /></a></p>

<p>May have numerical issue</p>

<h2 id="bcewithlogitsloss">BCEWithLogitsLoss()</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L\(x%2Cy\)%20%3D%20L%20%3D%5C%7Bl_1%2C...%2Cl_n%5C%7D%5ET%20%2C%20l_n%20%3D%20-w_n%5By_n%20*%20log%20%5Csum\(x_n\)%20%2B%20\(1-y_n\)*%20log\(1-%5Csum\(x_n\)\)%5D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image18.png" alt="" /></a></p>

<p>where N span all dimension of input and L have the same shape as input if reduction is not none</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=l\(x%2Cy\)%20%3D%20%5C%7Bmean\(L\)%2C%20if%5Chspace%7B5%7D%20reduction%20%3D%20mean%2C%20sum\(L\)%20if%20%5Chspace%7B5%7Dreduction%20%3D%20sum%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image6.gif" alt="" /></a></p>

<p>It take value and pass to sigmoid and make sure the value is between 0 and 1</p>

<h2 id="marginrankingloss">MarginRankingLoss()</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=loss\(x%2Cy\)%20%3D%20max\(0%2C%20-y%20*%20\(x_1%20-%20x_2\)%20%2B%20margin\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image23.png" alt="" /></a></p>

<p>Make one of the input larger than the other by value of the margin, y is the value that control x1 and x2</p>

<h2 id="tripletmarginloss">TripletMarginLoss()</h2>

<p>Use to measuring a relative similarity between samples. It compose of anchor,p positive,n negative</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=L\(a%2Cp\)%20%3D%20max%5C%7Bd\(a_i%2Cp_i\)%20-%20d\(a_i%2Cn_i\)%20%2B%20margin%2C%200%5C%7D%20where%5Chspace%7B5%7D%20d\(x_i%2Cy_i\)%20%3D%20%7C%7C%20x_i%20-%20y_i%7C%7C_p%20%20#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image12.gif" alt="" /></a></p>

<p>Try to get the distance (value ) good pair smaller than the distance of the bad pair</p>

<h2 id="softmarginloss">SoftMarginLoss()</h2>

<p>Optimize a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1)</p>

<blockquote>
  <p><a href="https://www.codecogs.com/eqnedit.php?latex=loss\(x%2Cy\)%20%3D%20%5Csum_%7Bi%7D%20%5Cfrac%7B%20log\(1%2Bexp\(-y%5Bi%5D%20*%20x%5Bi%5D\)\)%7D%7Bx.nelement\(\)%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image39.png" alt="" /></a></p>
</blockquote>

<p>Which the positive value closer and negative value further</p>

<h2 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss()</h2>

<p>Optimize a multi-label one-versus-all loss based on max-energy, between input x and target y of size (N,C). for each sample in the minibatch</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=loss\(x%2Cy\)%20%3D%20-%5Cfrac%7B1%7D%7BC%7D%20*%20%20%5Csum_%7Bi%7D%20y%5Bi%5D%20*%20log\(1%2Bexp\(-x%5Bi%5D\)\)%5E%7B-1%7D\"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image25.png" alt="" /></a>%20%2B%20(1-y%5Bi%5D)%20*log(%20%5Cfrac%7B%20exp(-x%5Bi%5D)%7D%7B(1%2Bexp(-x%5Bi%5D))%7D%20)#0)</p>

<p>Where <a href="https://www.codecogs.com/eqnedit.php?latex=i%20%5Cin%20%5C%7B0%2C...%2Cx.nelement\(\)%20-1%5C%7D%2Cy%5Bi%5D%20%5Cin%20%5C%7B0%2C1%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image33.png" alt="" /></a></p>

<p>Allow multiple correct output (want multiple categories can have high score and all lower score get suppressed) which for the desire category want the value to be higher and the undesired value suppress</p>

<h2 id="multimarginloss">MultiMarginLoss()</h2>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=loss\(x%2Cy\)%20%3D%5Cfrac%7B%20%5Csum_%7Bi%7D%20%5Cfrag%7Bmax\(0%2Cmargin%20-%20x%5By%5D%20%2B%20x%5Bi%5D\)\"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image8.gif" alt="" /></a>%5Ep%7D%7D%7B%7B%20x.size(0)%7D%7D#0)</p>

<p>Where <a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=%20x%20%5Cin%20%5C%7B0%2C...%2Cx.size\(0\)-%201%20%5C%7D%20and%20%5Chspace%7B10%7Di%20%20%5Cneq%20y#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image5.gif" alt="" /></a></p>

<h2 id="hingeembeddingloss">HingeEmbeddingLoss()</h2>

<p>Measure the loss given an input tensor x and a label tensor y (containing 1 or -1). This is usually used for measuring whether two input are similar or dissimilar, e.g. using the L1 pairwise distance as x, and is typically used for learning nonlinear embeddings or semi-supervised learning</p>

<p>The loss function for n-th sample in the mini-batch is</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=L_n%20%3D%5C%7Bx_n%2C%20if%20%5Chspace%7B5%7Dy_n%20%3D%201%20max%5C%7B0%2C%5CDelta%20-%20x_n%5C%7D%2C%20if%5Chspace%7B6%7D%20y_n%20%3D%20-1#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image28.gif" alt="" /></a></p>

<p>And the total loss function is</p>

<p><a href="https://latex-staging.easygenerator.com/eqneditor/editor.php?latex=L\(x%2Cy\)%20%3D%20%5C%7Bmean\(L\)%20if%5Chspace%7B6%7D%20reduction%20%3D%20'mean'%20%2C%20sum\(L\)%2C%20if%5Chspace%7B6%7D%20reduction%20%3D%20'sum'#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image9.gif" alt="" /></a></p>

<p>Where <a href="https://www.codecogs.com/eqnedit.php?latex=L%20%3D%20%5C%7Bl%2C...%2Cln%5C%7D%5ET#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image26.png" alt="" /></a></p>

<h2 id="cosinembeddingloss">CosinEmbeddingLoss()</h2>

<p>Measure the loss given input tensors x1,x2 and a Tensor label y with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning</p>

<p>The loss function for each sample is:</p>

<blockquote>
  <p>1 - cos(x_1,x_2), &amp; \quad \text{if} y = 1 \\</p>
</blockquote>

<p>max(0,cos(x_1,x_2) - margin) &amp; \quad \text{if} y = -1</p>

<p>f(n) =</p>

<p>\begin{cases}</p>

<p>n/2 &amp; \quad \text{if } n \text{ is even}\\</p>

<p>-(n+1)/2 &amp; \quad \text{if } n \text{ is odd}</p>

<p>\end{cases}</p>

<p>For positive case make the two vector align a much as possible</p>

<p>For negative try to make the cos value orthogonal to each other</p>

<p>CTCLoss() (connectionist Temporal Classification loss)</p>

<p>Calculate loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many to one”, which limits the length of the target sequence such that it must be ≤ the input length</p>

<p>Use in waveform classification where the speed may play a role e.g. a word was pronoun slowly may generate multiple similar waveform/fast may generate less waveform should be map to the same word regardless of speed of speaker</p>

<h1 id="architecture-and-loss-function">Architecture and Loss Function</h1>

<p>General form of energy base model</p>

<h3 id="family-of-energy-function">Family of energy function</h3>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cvarepsilon%20%3D%5C%7BE\(W%2CY%2CX\)%3A%20W%20%5Cin%20%5Cmathcal%7BW%7D%20%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image7.png" alt="" /></a></p>

<h3 id="training-set">Training set</h3>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%20S%20%3D%20%5C%7B%20\(X%5Ei%2CY%5Ei\)%3A%20i%20%3D%201...P%5C%7D#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image2.png" alt="" /></a></p>

<h3 id="loss-functionalloss-function">Loss functional/Loss function</h3>

<p>Functional is function of a function</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cmathcal%7BL%7D\(E%2CS\)%3B%20%5Cmathcal%7BL%7D\(W%2CS\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image13.png" alt="" /></a></p>

<p>Measure the quality of an energy function on training set</p>

<h3 id="training">Training</h3>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=W%5E*%20%3D%20min_%7B%5Csubstack%7BW%20%5Cin%20W%7D%7D%20%5Cmathcal%7BL%7D\(W%2CS\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image1.png" alt="" /></a></p>

<h3 id="form-of-the-loss-function">Form of the loss function</h3>

<p>Invariant under permutations and repetition of the samples</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cmathcal%7BL%7D\(E%2CS\)%3D%20%5Cfrac%7B1%7D%7BP%7D%5Csum_%7Bi%20%3D%201%7DL\(Y%5Ei%2CE\(W%2CY%2CX%5Ei\)\)%2BR\(W\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image31.png" alt="" /></a></p>

<p>Where L per sample loss</p>

<p>Y desired answer</p>

<p>E energy surface for a given Xi as Y varies</p>

<p>R regularizer</p>

<h2 id="energy-loss-function-example">Energy Loss function example</h2>

<p>Energy loss</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L_%7Benergy%7D\(Y%5Ei%2CE\(W%2CY%2CX%5Ei\)\)%20%3D%20E\(W%2CY%5Ei%2CX%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image20.png" alt="" /></a></p>

<p>Simply pushes down on the energy o the correct answer</p>

<h2 id="negative-log-likelihood-loss">Negative Log-Likelihood Loss</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cmathcal%7BL%7D_%7Bnll%7D\(W%2CS\)%20%3D%20%5Cfrac%7B1%7D%7BP%7D%20%5Csum_%7Bi%3D1%7D%5EP\(E\(W%2CY%5Ei%2CX%5Ei\)%20%2B%20%5Cfrac%7B1%7D%7B%5Cbeta%7Dlog%20%5Cint_%7By%20%5Cin%20Y%7De%5E%7B-%5Cbeta%20E\(W%2Cy%2CX%5Ei\)%7D\)%20#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image38.png" alt="" /></a></p>

<p>Have a behavior of pushing down the correct answer push up on energy of incorrect value in proportion to their probability</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cfrac%7B%5Cpartial%20L_%7Bnll%7D\(W%2CY%5Ei%2CX%5Ei\)%7D%7B%5Cpartial%20W%7D%3D%20%5Cfrac%7B%5Cpartial%20E\(W%2CY%5Ei%2CX%5Ei\)%7D%7B%5Cpartial%20W%20%7D%20-%20%5Cint_%7By%20%5Cin%20Y%7D%20%5Cfrac%7B%5Cpartial%20E\(W%2CY%2CX%5Ei\)%7D%7B%5Cpartial%20W%7D%20P\(Y%7CX%5Ei%2CW\)%20%20#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image35.png" alt="" /></a></p>

<h2 id="a-simpler-loss-function-perceptron-loss">A simpler Loss function: Perceptron Loss</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=L_%7Bperceptron%7D\(Y%5Ei%2CE\(W%2CY%2CX%5Ei\)\)%20%3D%20E\(W%2CY%5Ei%2CX%5Ei\)%20-%20min_%7By%20%5Cin%20Y%7D%20E\(W%2CY%2CX%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image37.png" alt="" /></a></p>

<ul>
  <li>
    <blockquote>
      <p>Push down the energy of right answer</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Push up on energy of wrong answer</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Always, positive</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Zero when answer is correct</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>No margin does not prevent energy surface from being almost flat</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Work well in practice, particularly if the energy parameterization does not allow flat surfaces</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Call “discriminative Viterbi training” in speech and hand written literature</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Not work well with non linear system</p>
    </blockquote>
  </li>
</ul>

<h2 id="generalized-margin-loss">Generalized Margin Loss</h2>

<p>Most offending Incorrect answer: discrete case</p>

<p>Definition 1 Let Y be a discrete variable. Then for a training sample<a href="https://www.codecogs.com/eqnedit.php?latex=%20\(X%5Ei%2CY%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image4.png" alt="" /></a></p>

<p>The most offending incorrect answer <a href="https://www.codecogs.com/eqnedit.php?latex=%20%5Cbar%7BY%7D%5Ei%20#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image3.png" alt="" /></a> is the answer that has the lowest energy among all the answer that are incorrect:</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cbar%7BY%7D%5Ei%20%3D%20argmin_%7By%20%5Cin%20Y%20and%20Y%5Cneq%20Y%5Ei%7D%20E\(W%2CY%2CX%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image29.png" alt="" /></a></p>

<p>Most offending Incorrect answer: discrete continuous case</p>

<p>Definition 2 Let Y be a continuous variable. Then for a training sample <a href="https://www.codecogs.com/eqnedit.php?latex=\(X%5Ei%2CY%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image17.png" alt="" /></a> the most offending incorrect answer <a href="https://www.codecogs.com/eqnedit.php?latex=%5Cbar%7BY%7D%5Ei#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image16.png" alt="" /></a> is the answer that ha the lowest energy among all answer that are at least \in away from the correct answer;</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cbar%7BY%7D%5Ei%20%3D%20argmin_%7By%20%5Cin%20Y%2C%7C%7CY-Y%5Ei%7C%7C%3E%5Cepsilon%7D%20E\(W%2CY%2CX%5Ei\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image27.png" alt="" /></a></p>

<h2 id="hinge-loss">Hinge Loss</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%20L_%7Bhinge%7D\(W%2CY%5Ei%2CX%5Ei\)%20%3D%20max\(0%2Cm%20%2B%20E\(W%2CY%5Ei%2CX%5Ei\)%20-%20E\(W%2C%5Cbar%7BY%7D%5Ei%2CX%5Ei\)\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image11.png" alt="" /></a></p>

<p>With the linearly-parameterized binary classifier architecture, we get linear SVM</p>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%20L_%7Blog%7D\(W%2CY%5Ei%2CX%5Ei\)%20%3D%20log\(1%20%2Be%5E%7B%20E\(W%2CY%5Ei%2CX%5Ei\)%20-%20E\(W%2C%5Cbar%7BY%7D%5Ei%2CX%5Ei\)%7D\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image34.png" alt="" /></a></p>

<p>Log loss a soft hinge loss</p>

<p>With the linearly-parameterized binary classifier architecture, we get linear Logistic Regression</p>

<h2 id="square-square-loss">Square-Square Loss</h2>

<p>[<img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image14.png" alt="" />](https://www.codecogs.com/eqnedit.php?latex=%20L_%7Bsq-sq%7D(W%2CY%5Ei%2CX%5Ei)%20%3D%20E(W%2CY%5Ei%2CX%5Ei)%5E2%20%2B%20(%20max(0%2Cm%20%2B%20E(W%2CY%5Ei%2CX%5Ei)%20)%5E2%20#0)</p>

<p>Appropriate for positive energy function</p>

<h2 id="a-more-general-form-of-hinge-type-loss">A more general form of Hinge type Loss</h2>

<p><a href="https://www.codecogs.com/eqnedit.php?latex=%20L\(W%2CX%5Ei%2CY%5Ei\)%20%3D%20%5Csum_y%20H\(E\(W%2CY%5Ei%2CX%5Ei\)%20-%20E\(W%2Cy%2CX%5Ei\)%20%2B%20C\(Y%5Ei%2Cy\)\)#0"><img src="https://www.vanhp.com/fastMLBlog//assets/img/2020-09-09-Loss-Fun-ctions/media/image24.png" alt="" /></a></p>

<p>H is Hinge energy</p>

<p>E another energy</p>

<p>C margin</p>

   
  </div><a class="u-url" href="/fastMLBlog/2020/09/09/Loss-Fun-ctions.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastMLBlog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Supra_PolyglotCoder</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Vanh Phomsavanh</li><li><a class="u-email" href="mailto:vanhphom@gmail.com">vanhphom@gmail.com</a></li>
            <li>&copy; VSP LLC 2020 </li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://www.twitter.com/SupraCoder"><svg class="svg-icon"><use xlink:href="/fastMLBlog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">SupraCoder</span></a></li><li><a href="https://github.com/vanhp"><svg class="svg-icon"><use xlink:href="/fastMLBlog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">vanhp</span></a></li><li><a href="https://www.facebook.com/vanh+phom"><svg class="svg-icon"><use xlink:href="/fastMLBlog/assets/minima-social-icons.svg#facebook"></use></svg> <span class="username">vanh phom</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about my journey into Machine Learning and coding experience in general</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
