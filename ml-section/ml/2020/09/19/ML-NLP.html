<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Natural Language Process in Machine Learning | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Natural Language Process in Machine Learning" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How machine learning work with Human Language" />
<meta property="og:description" content="How machine learning work with Human Language" />
<link rel="canonical" href="https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html" />
<meta property="og:url" content="https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:image" content="https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"How machine learning work with Human Language","dateModified":"2020-09-19T00:00:00-05:00","datePublished":"2020-09-19T00:00:00-05:00","headline":"Natural Language Process in Machine Learning","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html"},"image":"https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg","url":"https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/feed.xml" title="Supra_PolyglotCoder" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Natural Language Process in Machine Learning | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Natural Language Process in Machine Learning" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How machine learning work with Human Language" />
<meta property="og:description" content="How machine learning work with Human Language" />
<link rel="canonical" href="https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html" />
<meta property="og:url" content="https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:image" content="https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"How machine learning work with Human Language","dateModified":"2020-09-19T00:00:00-05:00","datePublished":"2020-09-19T00:00:00-05:00","headline":"Natural Language Process in Machine Learning","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html"},"image":"https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg","url":"https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/feed.xml" title="Supra_PolyglotCoder" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><!--
      edit by vanh original code header.html
    <a class="site-title" rel="author" href="/">Supra_PolyglotCoder</a>
    replace with custom code below
    -->
    <a class="site-title" rel="author" href="/"><img src="/images/gradlogo1.jpg" alt="Supra_PolyglotCoder"></img></a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/Resource-section/">Resources</a><a class="page-link" href="/ML-section/">ML</a><a class="page-link" href="/Coding-section/">Programming</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <!-- add feature image on top of post --><img src="/ML-section/assets/images/neuralNet1.jpg" alt="" class="featured-image-post" /><h1 class="post-title p-name" itemprop="name headline">Natural Language Process in Machine Learning</h1><p class="page-description">How machine learning work with Human Language</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-19T00:00:00-05:00" itemprop="datePublished">
        Sep 19, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#ML-section">ML-section</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#ML">ML</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="Toast Toast--warning googoo">
   <span class="Toast-icon"><svg class="octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>
   <span class="Toast-content">Warning: This page is under heavy construction and constant changing</span>
</div>

<hr />
<p>Human Language compose of texts in sequence. This sequence imply meaning word that come at $t-1$ of current word at $t$ may convey different meaning for example, when the same word come at $t+1$. To tackle this problem a model should some form of remembering the previous occurrance of the word. Let take a look at how a model can work with human language.</p>

<h2 id="nlp-deep-dive-rnn">NLP Deep Dive: RNN</h2>
<p>Self-supervise learning</p>

<p>Language Model is model that train to predict the next word in the sentence would be. This type of model is called self-supervise model since it’s doesn’t require label to guide it’s prediction as oppose to supervise-learning which require label that the model uses to compare its prediction against the actual label which is the ground truth. It simply learn from huge amount of texts.</p>

<p>In a sense, it has it own highly complicate way of extract the label from the data. It’s also has its own way to understand the context of the language that it works with.
Self-supervise is a highly active field in research community. Since is applicable in many domains other than text.</p>

<h3 id="pretrained-nlp-model">Pretrained NLP Model</h3>
<p>Using pretrained Language model is widely prefer by researcher and practictioner alike. Instead of train them from scratch since it’s highly resource and time-consuming. A pretrained language model in one task maybe retrain on different task e.g. retrain a model that was trained in Wikipedia on movies reviews database such as IMDb. As with all pretrained model it’s helpful to get insign into the dataset and familarity with the style of data e.g. techical,formal,casual type.</p>

<h3 id="the-ulmfit-approach">The ULMFit approach</h3>
<p>Have a good understanding of the language model foundation is useful to adapt and fine-tuning the pretrained model for new task that may not easily relate to original task. This technique of adapting or refining the pretrained model with data relate to new task prior to training it for the new task (do classification) is call Universal Language Model Fine-tuning. The <a href="https://arxiv.org/abs/1801.06146">ULMFIt</a> paper indicate that this help improve the performance of the model remarkedbly.
<img src="/images/ulmfit1.png" alt="ulmfit_step" /></p>

<h2 id="text-preprocessing-from-scatch">Text preprocessing from scatch</h2>

<p>Start from knowledge of using categorical variables as an independent variable for NN.</p>

<ul>
  <li>Make a list of all possible levels (words) of that categorical variable (we’ll call this list the vocab).</li>
  <li>Replace each level (word) with its index in the vocab.</li>
  <li>Create an embedding matrix for this containing a row for each level(word) (i.e., for each item of the vocab).</li>
  <li>Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)</li>
</ul>

<p>These are the basic approach for dealing with text.</p>

<h3 id="dealing-with-time-sequence-in-text">Dealing with time sequence in Text</h3>
<p>However, to handle sequencing problem in text here are some solutions:</p>
<ol>
  <li>concaternate all of the document in the dataset into one giant long string.</li>
  <li>split the string into list of words</li>
  <li>assign the independent variable as the sequence of words start from first word to second to last word</li>
  <li>assign the dependent variable as the sequence of words start from second word ending at last word this will create an offset by 1 of independent variable and dependent variable</li>
</ol>

<h3 id="text-tokenization">Text Tokenization</h3>

<p>A process of converting text which is sequence of characters into group such as word (group of character) in order to assign numerical value. There are three approach:</p>
<ol>
  <li>word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning “don’t” into “do n’t”). Generally, punctuation marks are also split into separate tokens.</li>
  <li>Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, “occasion” might be tokenized as <em>“o c ca sion”</em>.</li>
  <li>Character-based: Split a sentence into its individual characters.</li>
</ol>

<p>The vocab will contain both old words that were used to pretrain the model and new words that we’ve just created which the exception that new words won’t have any corresponding embedding matrix which will be filled with random value. Summary of
the prodcess of create vocab:</p>
<ol>
  <li>tokenize the text by convert them into words</li>
  <li>Numericalize by assign the new unique word an index (int) value</li>
  <li>create a loader to these data with LMDataLoader class from fastai</li>
  <li>create language model with RNN that can handle arbitrary size input list</li>
</ol>

<h3 id="tokenization-with-fastai">Tokenization with fastai</h3>

<h3 id="word-tokenization">Word Tokenization</h3>
<p>fastai does not provide its own tokenizer but instead provide interface API such as <em>WordTokenizer</em> for third party tokenizer which let user choose their own tokenizer if prefered. The default tokenizer currently is <em>spaCy</em> library. It’s one of more sophistacated and flexible to handle English words. It could split word like it’s int it and s and many subtle task.</p>

<p>fastai extends this library by adding it’s own functionality.
It’s add special token and rule such as <em>xx</em> in front of an uncommon word e.g. <em>xxbos</em> signal the start of document this tell the model to clear its own memory for new task.</p>

<p>There are rules such as replace 4 consecutive sequence of !!!! with 1 ! follow by repeat character token and number 4.
So the model can encode in its embeding matrix the info about general concept about repeat punctuation instead of separate token for each ! if it run into the same sequence again it doesn’t need to do anything saving computation time and memory.</p>

<p>There are also rule for capitalization. The capitalize word will be replace with special capitalization token follow by lowercase of the word so there’s only 1 lowercase version in the embedding matrix.
example of special token:</p>
<ul>
  <li>xxbos:: Indicates the beginning of a text (here, a review)</li>
  <li>xxmaj:: Indicates the next word begins with a capital (since we lowercased everything)</li>
  <li>xxunk:: Indicates the next word is unknown</li>
  <li>fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)</li>
  <li>replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character
replace_wrep::
    <ul>
      <li>Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word</li>
      <li>spec_add_spaces:: Adds spaces around / and #</li>
      <li>rm_useless_spaces:: Removes all repetitions of the space character</li>
      <li>replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxcap) in front of it</li>
      <li>replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it
lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from fastai.text.all import *
path = untar_data(URLs.IMDB)

files = get_text_files(path, folders = ['train', 'test', 'unsup'])
txt = files[0].open().read(); txt[:75]
spacy = WordTokenizer()
toks = first(spacy([txt]))
print(coll_repr(toks, 30))
# how it split this sentence
first(spacy(['The U.S. dollar $1 is $1.00.']))
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt), 31))
# check the rule
defaults.text_proc_rules
coll_repr(tkn('&amp;copy;   Fast.ai www.fast.ai/INDEX'), 31)
</code></pre></div></div>

<h3 id="subword-tokenization">Subword Tokenization</h3>
<p>Word tokenization work well for English and language that relie on space as a word separator. But for languages that don’t relie on space such as Thai, Chinese, Japanese and many Asian languages subword tokenizer work better.
It uses a two step process:</p>
<ol>
  <li>Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.</li>
  <li>Tokenize the corpus using this vocab of subword units.</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># create vocab size    2000 words
txts = L(o.open().read() for o in files[:2000])
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])

subword(1000)
subword(200)
subword(10000)
</code></pre></div></div>
<p>for subword tokenization fastai use “__” to indicate space in the text</p>

<p>Vocab size</p>

<p>larger vocab means fewer tokens per sentence which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn.</p>

<h3 id="numericalization-with-fastai">Numericalization with fastai</h3>
<p>Numericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST:</p>

<ul>
  <li>Make a list of all possible levels of that categorical variable (the vocab).</li>
  <li>Replace each level with its index in the vocab.</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>toks = tkn(txt)
print(coll_repr(tkn(txt), 31))
toks200 = txts[:200].map(tkn)
toks200[0]
num = Numericalize()
# call setup to create the vocab
num.setup(toks200)
coll_repr(num.vocab,20)
</code></pre></div></div>
<p>In the output:</p>

<ul>
  <li>Our special rules tokens appear first, and then every word appears once, in frequency order.</li>
  <li>The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk.</li>
  <li>fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># see if it converts to tensor
nums = num(toks)[:20]; nums
' '.join(num.vocab[o] for o in nums)

</code></pre></div></div>

<h3 id="batching-texts">Batching texts</h3>
<p>Since text needs to be in sequential order so the model can predict the next word in sequence which means the new batch must begin right after the last one get cut-off. The whole array of text string after tokenization will be 
divided into sequence of the same equal length according to the number of batch and sequence length specify.</p>

<p>Here the code to process a sample of small text using 6 batch with each sequence length of 15</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>stream = "In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\nThen we will study how we build a language model and train it for a while."
tokens = tkn(stream)
bs,seq_len = 6,15
d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
</code></pre></div></div>

<p>If the sequence length is too long to fit into memory of GPU it needs to be subdivide into shorter length there are multiple way to do this in fastai as shown here:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># method 1
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
# method 2
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
# method 3
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
</code></pre></div></div>
<p>Each batch is considered a mini-stream. These mini-batch must be in sequence so it retain the meaning which the model can learn by reading them in in sequence.
These process is automatically done by <code class="highlighter-rouge">LMDataLoader</code> as shown in sample code here:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nums200 = toks200.map(num)
dl = LMDataLoader(nums200)
x,y = first(dl)
x.shape,y.shape
# look at independent variable
' '.join(num.vocab[o] for o in x[0][:20])
# look at dependent variable
' '.join(num.vocab[o] for o in y[0][:20])
</code></pre></div></div>

<h2 id="training-text-classifier">Training Text Classifier</h2>

<p>The <code class="highlighter-rouge">DataBlock</code> API will auto tokenize and numericalize when <code class="highlighter-rouge">TextBlock</code> is passed in.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])

dls_lm = DataBlock(
    blocks=TextBlock.from_folder(path, is_lm=True),
    get_items=get_imdb, splitter=RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=2)

</code></pre></div></div>
<p>TextBlock internally does</p>
<ul>
  <li>It saves the tokenized documents in a temporary folder, so it doesn’t have to tokenize them more than once</li>
  <li>It runs multiple tokenization processes in parallel, to take advantage of your computer’s CPUs</li>
</ul>

<p>Fine-Tuning the Language Model for new task</p>

<p>Using embedding matrix to convert integer into activation value for NN.
The RNN network using is <em>AWD-LSTM</em>
Merge the embedding of pretrained model with new embedding that fill with random value is done by <code class="highlighter-rouge">language_model_learner</code></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># auto call freeze for pretrained model
learn = language_model_learner(
    dls_lm, AWD_LSTM, drop_mult=0.3, 
    metrics=[accuracy, Perplexity()]).to_fp16()

learn.fit_one_cycle(1, 2e-2) 
learn.save('1epoch')
learn = learn.load('1epoch')
learn.unfreeze()
learn.fit_one_cycle(10, 2e-3)
learn.save_encoder('finetuned')
</code></pre></div></div>
<ul>
  <li>Using cross-entropy as loss function and perplexity metric which is torch.exp(cross_entropy)</li>
  <li>At the end save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the <em>encoder</em>.</li>
</ul>

<h4 id="text-generation">Text Generation</h4>

<p>A sample test try to train it generate text after a sentence and a random word base on probability return by the model</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TEXT = "I liked this movie because"

# length of sentence (40 words)
N_WORDS = 40
# numb of sentence
N_SENTENCES = 2
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) 
         for _ in range(N_SENTENCES)]

print("\n".join(preds))
</code></pre></div></div>
<h3 id="fine-tune-the-classifier">Fine-tune the Classifier</h3>
<p>The last step is to fine-tune the classifier</p>

<p>Create a dataloader for the classifier which very similar to vision version</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dls_clas = DataBlock(
    # use this vocab that already fine-tune, don't generate new one
    #is_lm=False tell TextBlock don't use next token as label
    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),
    get_y = parent_label,
    get_items=partial(get_text_files, folders=['train', 'test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path, path=path, bs=128, seq_len=72)
dls_clas.show_batch(max_n=3)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
</code></pre></div></div>

<ul>
  <li>Special padding token is used To but ignore by the model make all the batch of same length require by <em>PyTorch</em> DataLoader.</li>
  <li>Sorting by length to batch together document of the same length prior to each epoch.</li>
  <li>all batchs don’t have to be in same size only the document in the batch</li>
  <li>will pad all document to same length of the largest one</li>
  <li>all these are done by <code class="highlighter-rouge">DataBlock</code> when <code class="highlighter-rouge">TextBlock</code> is passed-in and <code class="highlighter-rouge">is_lm=False</code> is set</li>
</ul>

<p>Tip:</p>

<p>When training uses discriminative learning rate and gradual unfreeze work well</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, 
                                metrics=accuracy).to_fp16()

learn = learn.load_encoder('finetuned')
learn.fit_one_cycle(1, 2e-2)

# try with new value with gradual unfreeze
learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))

learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))

# the whole body
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
</code></pre></div></div>

<p>Disinformation and Language Model</p>

<p>This will make it easier to generate fake news,info.</p>

<h3 id="jagons">Jagons:</h3>
<ul>
  <li>Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels</li>
  <li>a word is a categorical variable</li>
  <li>corpus: database of text</li>
  <li>vocab: database of text that have been indexed</li>
  <li>Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)</li>
  <li>Token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character.</li>
  <li>Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab</li>
  <li>Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required</li>
  <li>Language model creation:: We need a special kind of model that does something we haven’t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the &lt;&gt;, but for now, you can think of it as just another deep neural network.</li>
  <li>RNN: recurrent neural network a kind of neural network that have momory to handle long sequence of text</li>
  <li>BOS: is a standard NLP acronym that means “beginning of stream” of text</li>
  <li>Document: contain stream of text that relate to each other and made up of that story. 
_ each epoch these ducument should be shuffle</li>
  <li>encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but “encoder” tends to be more used for NLP and generative models.</li>
  <li>temperature: refer to degree of randomization</li>
  <li>language model predicts the next word of a document, it doesn’t need any external labels</li>
  <li>classifier, predicts some external label</li>
  <li>Wikipedia103 contain large amount of English text will cover almost all vocabular, slangs, idioms use currently. It’s a good starting point for model to learn from</li>
</ul>


   
  </div><a class="u-url" href="/ml-section/ml/2020/09/19/ML-NLP.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Supra_PolyglotCoder</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Vanh Phomsavanh</li><li><a class="u-email" href="mailto:vanhphom@gmail.com">vanhphom@gmail.com</a></li>
            <li>&copy; VSP LLC 2020 </li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://www.twitter.com/SupraCoder"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">SupraCoder</span></a></li><li><a href="https://github.com/vanhp"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">vanhp</span></a></li><li><a href="https://www.facebook.com/vanh+phom"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#facebook"></use></svg> <span class="username">vanh phom</span></a></li><li><a href="https://www.linkedin.com/in/vanh-phomsavanh-1bba668"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">vanh-phomsavanh-1bba668</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about my journey into Machine Learning and coding experience in general</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
