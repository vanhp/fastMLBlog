<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Training Model | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Training Model" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An experience with train model" />
<meta property="og:description" content="An experience with train model" />
<link rel="canonical" href="https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html" />
<meta property="og:url" content="https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:image" content="https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-12T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"An experience with train model","dateModified":"2020-05-12T00:00:00-05:00","datePublished":"2020-05-12T00:00:00-05:00","headline":"Training Model","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html"},"image":"https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg","url":"https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/feed.xml" title="Supra_PolyglotCoder" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Training Model | Supra_PolyglotCoder</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Training Model" />
<meta name="author" content="Vanh Phomsavanh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An experience with train model" />
<meta property="og:description" content="An experience with train model" />
<link rel="canonical" href="https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html" />
<meta property="og:url" content="https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html" />
<meta property="og:site_name" content="Supra_PolyglotCoder" />
<meta property="og:image" content="https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-12T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Vanh Phomsavanh"},"description":"An experience with train model","dateModified":"2020-05-12T00:00:00-05:00","datePublished":"2020-05-12T00:00:00-05:00","headline":"Training Model","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html"},"image":"https://www.vanhp.com/ML-section/assets/images/neuralNet1.jpg","url":"https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://www.vanhp.com/feed.xml" title="Supra_PolyglotCoder" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><!--
      edit by vanh original code header.html
    <a class="site-title" rel="author" href="/">Supra_PolyglotCoder</a>
    replace with custom code below
    -->
    <a class="site-title" rel="author" href="/"><img src="/images/gradlogo1.jpg" alt="Supra_PolyglotCoder"></img></a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/Resource-section/">Resources</a><a class="page-link" href="/ML-section/">ML</a><a class="page-link" href="/Coding-section/">Programming</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <!-- add feature image on top of post --><img src="/ML-section/assets/images/neuralNet1.jpg" alt="" class="featured-image-post" /><h1 class="post-title p-name" itemprop="name headline">Training Model</h1><p class="page-description">An experience with train model</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-12T00:00:00-05:00" itemprop="datePublished">
        May 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#ML-section">ML-section</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#ML">ML</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!-- ![](/ML-section/assets/images/neuralNet1.jpg) -->
<hr />

<h2 id="-basic-step-in-training-modelspan"><span style="color:green"> Basic step in training model:&lt;/span<span style="color:red">&gt;</span></span></h2>

<ol>
  <li>Initialize the weights</li>
  <li>For each input image use these weights to predict whether the outcome e.g. (0…9)</li>
  <li>Based on the prediction calculate how good the predict is what’s the loss (error)</li>
  <li>Calculate the gradient derive from each weight to see how changing the weight affect the loss</li>
  <li>Update the weights base on the calculation</li>
  <li>Repeat step 2-5 for all images</li>
  <li>Keep doing these until outcome is satisfy or quit</li>
</ol>

<p>All images</p>

<ul>
  <li>must be in the same dimension(size)can collate into tensors to be passed to the GPU</li>
  <li>will be converted into matrice</li>
</ul>

<h3 id="initialization"><span style="color:coral">Initialization</span></h3>

<ul>
  <li>must be initialized with random value</li>
</ul>

<p>Matrix multiplication is usually in dot-product e.g. m dot x, or m@x in Python</p>

<p>gradient is calculated using the Calculus chain-rule:</p>

<p>def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def simpleNN(xb):
    l1 = xb@w1 + b1
    l1 = l1.max(tensor(0.0)) make neg value into 0 ignore + (relu)
    l1 = l1@w2 + b2
    return l1
</code></pre></div></div>

<p>this simple little has some interesting features</p>
<ol>
  <li>It work like universal approximation theorem whhich can be used to approx any function no matter how complicate it is</li>
  <li>It call Neural Network</li>
  <li>It build via composition which take the result of one function then pass it into a new function on and on PyTorch has name this as sequential</li>
</ol>

<p>Linear = x@w + bias</p>

<p>non linear</p>

<p>Non linearity is needed to turn the network into more useful function, since linear + linear -&gt; another linear function</p>

<p>reLu: max(tensor(0,0))</p>

<p>sigmoid:
output binary categories between 0 and 1
torch.where can be used to pick from one of these (output must contain only 2 categories)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def sigmoid(x):
    return 1/(1 + torch.exp(x)) 
</code></pre></div></div>

<p>softmax:
output more than 2 categories which have value all add up to 1
has a side effect of exagerate the large value and diminist smaller value ‘cause of exponential effect</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def softmax(x): return exp(x)/exp(x).sum(dim=1,keepdim=true)
</code></pre></div></div>

<p>It works even when our dependent variable has more than two categories.
It results in faster and more reliable training.
In order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like.</p>

<p>Loglikelyhood
is and indexer which use to pick a value from the list of value output from softmax
PyTorch provide nll_loss assumes that you already took the log of the softmax, so it doesn’t actually do the logarithm must use log_softmax (faster and more accurate to take a log at this stage) before nll_loss
Negativeloglikelyhood NLL this function simply apply minus to the value to remove negative value</p>

<p>Cross-Entropy Loss nn.CrossEntropyLoss (does log_softmax and then nll_loss)
Cross-entropy loss is a loss function that is similar to the one we used in the previous chapter, but (as we’ll see) has two benefits:
When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss.</p>

<p>DataLoader will iterate over the collection input and return tuple of data in collection</p>

<p>Helpers function for code simplication:
DataLoaders
learner</p>

<p>create a simple neural network by hand</p>

<p><em>Note:</em>
 Pytorch Dataset return a tuple when indexing into it</p>

<h2 id="data-handling"><span style="color:green">Data Handling</span></h2>

<h3 id="data-augmentation"><span style="color:coral">Data augmentation:</span></h3>
<p>Process to increase dataset where the data is hard to collect and help improve model performance by manipulate the images in a natural ways.</p>

<h4 id="data-cleaning"><span style="color:orange">Data cleaning</span></h4>

<p>A process to remove incorrect,missing,unnecessary info in the data set collect and make them usable by the model</p>

<p>tip:
fastai can help in data cleaning by let the model work on the data then fix the error where the model has problem with</p>

<h4 id="datablock"><span style="color:red">DataBlock</span></h4>
<p>use summary to debug</p>
<h3 id="presizing"><span style="color:coral">Presizing:</span></h3>
<p>A special technique use by fastai to improve image manipulation with high efficiency and minimal degradation
It uses strategies:</p>

<ol>
  <li>Resize images to relatively “large” dimensions—that is, dimensions significantly larger than the target training dimensions to have spare margin for more transformation with no empty zone
    <ul>
      <li>by resizing to a square using a large random crop size that span the smaller width or height</li>
    </ul>
  </li>
  <li>Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times.</li>
</ol>

<h4 id="training-the-model"><span style="color:orange">Training the model:</span></h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def train_epoch(model, lr, params):
     for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
</code></pre></div></div>

<h3 id="visualizing-data-and-result"><span style="color:coral">Visualizing Data and result</span></h3>

<p>To understand and diagnose the output from the model various visualization are available</p>
<ul>
  <li>Confusion matrix show grid of predict vs actual value
    <ul>
      <li>most_confused for model get the most often wrong answer (interp.most_confused(min_val=5)) to show model confusion the diagonal line is correct a 1 is any place else show mis-prediction</li>
    </ul>
  </li>
</ul>

<h3 id="how-the-model-learn">How the model learn</h3>
<p>from Zeiler and Fergus paper which show image of how the model learn</p>
<ul>
  <li>The earlier layer learn about structure e.g. line,circle, edge, area</li>
  <li>Each layer after that learn more more semantic by using info from earlier layer
to form meaning</li>
  <li>the lastest layer are closer to actual object</li>
</ul>

<h2 id="improve-model-performance"><span style="color:green">Improve model performance</span></h2>

<h3 id="learning-rate"><span style="color:coral">Learning rate</span></h3>

<p>the right learning rate is important the help improve model performance</p>
<ul>
  <li>Low learning rate may increase training epoch or overfitting</li>
  <li>Large learning rate may overshoot the minimum loss may decrease performance</li>
  <li>Use the learning rate finder (Learner.lr_find) tool will do these step:
    <ul>
      <li>start with a very, very small learning rate then double the size for each mini-batch</li>
      <li>Check the loss if it get worse stop then back-up to the last mini loss that the best one
  Then start with value smaller than this value pick either</li>
      <li>One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10 `1e-6 to 1e-3)</li>
      <li>The last point where the loss was clearly decreasing</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    learn = cnn_learner(dls, resnet34, metrics=error_rate)
    lr_min,lr_steep = learn.lr_find()
</code></pre></div></div>
<p>return the minimum point and steep point in log scale
steep point show the model making great progress (learning)
minimum show the model stop making progress (not learning)</p>

<h2 id="transfer-learning"><span style="color:green">Transfer Learning</span></h2>

<p>the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task</p>
<ul>
  <li>By remove the head which consist of the last layer that do the predicion (softMax layer) which specific to a task</li>
  <li>replace the head with new one (with random weight) that suitable to new task
    <ul>
      <li>tell the optimizer to only update the weights in those randomly added final layers</li>
      <li>Don’t change the weights in the rest of the neural network by freeze them</li>
    </ul>
  </li>
  <li>Keep the body</li>
</ul>

<h4 id="fastai-learnerfine_tune-methods"><span style="color:orange">Fastai Learner.fine_tune methods</span></h4>

<ul>
  <li>Trains the randomly added layers for one epoch, with all other layers frozen</li>
  <li>Unfreezes (learner.unfreeze) all of the layers, and trains them all for the number of epochs requested
fine_tune function does:</li>
  <li>freeze (body)</li>
  <li>fit_one_cycle</li>
  <li>base_lr/2</li>
  <li>unfreeze (body)</li>
  <li>fit_one_cycle (train all parts together)
<em>note</em> after fine_tune you may need to pick a new learning rate to help improve the performance since all parts may not have trained enough together discriminative learning rate is more appropriate at this point</li>
</ul>

<h4 id="fastai-1-cycle-policy"><span style="color:orange">Fastai 1 cycle policy</span></h4>
<p>A fastai technique for training the model
It start with low learning rate then gradually increase the learning rate until it reach the max value specify by the user then it stop for the first 1/3 of the batch, for the rest of the batch it gradually decrease the learning rate
Learner.fit_one_cycle</p>

<p><em>note:</em>
The recommend approach is to retrain with smaller epoch with the model overfit
usally the validation loss start getting worse than train error</p>

<h3 id="discriminative-learning-rates"><span style="color:coral">Discriminative Learning Rates</span></h3>
<p>Since the body has already been trained it doesn’t need the same learning rate the head portion
Discriminative Learning Rates allow for training the head and body with different learning rate</p>
<ul>
  <li>the body is trained with lower learning rate</li>
  <li>the head is trained with higher learning rate</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, 3e-3)
learn.unfreeze()
learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))
</code></pre></div></div>
<p>slice(learning_rate for body,learning_rate for head)</p>

<h3 id="training-period"><span style="color:coral">Training period</span></h3>
<p>if you find that you have overfit, what you should actually do is</p>
<ul>
  <li>retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.</li>
  <li>use a deeper architecture (more layers)
    <ul>
      <li>it takes more resources memory,CPU/GPU cycles</li>
      <li>take more time (use fp16 to speed up)</li>
    </ul>
  </li>
</ul>

<h3 id="choosing-the-right-architect"><span style="color:coral">Choosing the right architect</span></h3>
<p>Many achitect comes with various level of layers e.g. ResNet 18,34,50,101 pretrained on ImageNet
Pick the right one is a trial-error process</p>

<h3 id="pandas-and-dataframes">Pandas and DataFrames</h3>

<p><em>Note</em> 
in Numpy,Panda,PyTorch (trailing : is optional)
 e.g. df.iloc[0,:] -&gt; df.iloc[0]</p>

<h3 id="jargons-"><span style="color:blue">Jargons: </span></h3>

<ul>
  <li>parameter are weight and bias</li>
  <li>SGD : stochastic gradient decent calculate the gradient using a small set of data
      in practice is using a loop over a mini-batch to calculate the GD</li>
  <li>GD: gradient descent calculate using the whole dataset all at once</li>
  <li>broadcasting a matrix calculation to speed up without using loop by multiply the same scalar to all the value in the matrix then add them up</li>
  <li>mini-batch: a small set of data with label</li>
  <li>RelU: function that convert - value to 0 leave + value alone</li>
  <li>forward pass: the process that calculate the prediction value</li>
  <li>loss function: function that calculate the error the different between actual value - predict value</li>
  <li>backward pass: the process that calculate the adjustment value use to update the parameter to reduce the loss</li>
  <li>learning rate: size of SGD value use to update the parameters for every loop</li>
  <li>activation value: these number are calculated from output of linear and nonlinear</li>
  <li>parameter value: adjustable value use to improve the performance</li>
  <li>Tensor: multidimention arrays with regular shape
    <ul>
      <li>rank 0: scalar</li>
      <li>rank 1: vector</li>
      <li>rank 2: marix</li>
      <li>rank 3: 3D tensor</li>
      <li>rank 4: 4D tensor</li>
    </ul>
  </li>
  <li>Neural network: comprise with layers</li>
  <li>layer: groups of neural (node) arrange as linear and non-linear output of a layer is pass-on to the layer next to it</li>
  <li>fit: is same training</li>
  <li>Dataset: a colllection of data that returns a tuple of x,y variable of a single item in the collection</li>
  <li>DataLoader: an iterator that return stream of mini-batch. Where each minibatch is comprise with batch of x,y variables</li>
</ul>

<p><strong>—————————————————————————————————</strong></p>

<hr />

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="highlighter-rouge">YEAR-MONTH-DAY-filename.md</code></p>

<p>Where <code class="highlighter-rouge">YEAR</code> is a four-digit number, <code class="highlighter-rouge">MONTH</code> and <code class="highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="highlighter-rouge">filename</code> is whatever file name you choose, to remind yourself what this post is about. <code class="highlighter-rouge">.md</code> is the file extension for markdown files.</p>

<p>The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “<em>level 1 heading</em>” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line <code class="highlighter-rouge">## File names</code> above.</p>

<h2 id="basic-formatting">Basic formatting</h2>

<p>You can use <em>italics</em>, <strong>bold</strong>, <code class="highlighter-rouge">code font text</code>, and create <a href="https://www.markdownguide.org/cheat-sheet/">links</a>. Here’s a footnote <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. Here’s a horizontal rule:</p>

<hr />

<h2 id="lists">Lists</h2>

<p>Here’s a list:</p>

<ul>
  <li>item 1</li>
  <li>item 2</li>
</ul>

<p>And a numbered list:</p>

<ol>
  <li>item 1</li>
  <li>item 2</li>
</ol>

<h2 id="boxes-and-stuff">Boxes and stuff</h2>

<blockquote>
  <p>This is a quotation</p>
</blockquote>

<div class="Toast Toast--warning googoo">
   <span class="Toast-icon"><svg class="octicon octicon-alert octicon octicon-alert octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>
   <span class="Toast-content">You can include alert boxes</span>
</div>

<p>…and…</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">You can include info boxes</span>
</div>

<h2 id="images">Images</h2>

<p><img src="/images/logo.png" alt="" title="fast.ai's logo" /></p>

<h2 id="code">Code</h2>

<p>You can format text and code per usual</p>

<p>General preformatted text:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Do a thing
do_thing()
</code></pre></div></div>

<p>Python code and output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Prints '2'
</span><span class="k">print</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
</code></pre></div></div>

<p>Formatting text as shell commands:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"hello world"</span>
./some_script.sh <span class="nt">--option</span> <span class="s2">"value"</span>
wget https://example.com/cat_photo1.png
</code></pre></div></div>

<p>Formatting text as YAML:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">key</span><span class="pi">:</span> <span class="s">value</span>
<span class="pi">-</span> <span class="na">another_key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">another</span><span class="nv"> </span><span class="s">value"</span>
</code></pre></div></div>

<h2 id="tables">Tables</h2>

<table>
  <thead>
    <tr>
      <th>Column 1</th>
      <th>Column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A thing</td>
      <td>Another thing</td>
    </tr>
  </tbody>
</table>

<h2 id="tweetcards">Tweetcards</h2>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Altair 4.0 is released! <a href="https://t.co/PCyrIOTcvv">https://t.co/PCyrIOTcvv</a><br />Try it with:<br /><br />  pip install -U altair<br /><br />The full list of changes is at <a href="https://t.co/roXmzcsT58">https://t.co/roXmzcsT58</a> ...read on for some highlights. <a href="https://t.co/vWJ0ZveKbZ">pic.twitter.com/vWJ0ZveKbZ</a></p>&mdash; Jake VanderPlas (@jakevdp) <a href="https://twitter.com/jakevdp/status/1204765621767901185?ref_src=twsrc%5Etfw">December 11, 2019</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is the footnote. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

   
  </div><a class="u-url" href="/ml-section/ml/2020/05/12/ML-train.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Supra_PolyglotCoder</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Vanh Phomsavanh</li><li><a class="u-email" href="mailto:vanhphom@gmail.com">vanhphom@gmail.com</a></li>
            <li>&copy; VSP LLC 2020 </li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://www.twitter.com/SupraCoder"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">SupraCoder</span></a></li><li><a href="https://github.com/vanhp"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">vanhp</span></a></li><li><a href="https://www.facebook.com/vanh+phom"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#facebook"></use></svg> <span class="username">vanh phom</span></a></li><li><a href="https://www.linkedin.com/in/vanh-phomsavanh-1bba668"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">vanh-phomsavanh-1bba668</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about my journey into Machine Learning and coding experience in general</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
