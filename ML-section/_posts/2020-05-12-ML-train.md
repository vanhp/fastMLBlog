---
toc: false
layout: post
description: An experience with train model
category: ML
title: Training Model
image: /ML-section/assets/images/neuralNet1.jpg
---
<!-- ![]({{page.image | relative_url}}) -->

{% include alert.html text="Warning: This page is under heavy construction and constant changing" %}
---



## <span style="color:green"> Basic step in training model:</span<span style="color:red">>

1. Initialize the weights
2. For each input image use these weights to predict whether the outcome e.g. (0...9)
3. Based on the prediction calculate how good the predict is what's the loss (error)
4. Calculate the gradient derive from each weight to see how changing the weight affect the loss
5. Update the weights base on the calculation
6. Repeat step 2-5 for all images
7. Keep doing these until outcome is satisfy or quit

All images 

- must be in the same dimension(size)can collate into tensors to be passed to the GPU
- will be converted into matrice

### <span style="color:coral">Initialization</span>

- must be initialized with random value


Matrix multiplication is usually in dot-product e.g. m dot x, or m@x in Python

gradient is calculated using the Calculus chain-rule:

def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()


```
def simpleNN(xb):
    l1 = xb@w1 + b1
    l1 = l1.max(tensor(0.0)) make neg value into 0 ignore + (relu)
    l1 = l1@w2 + b2
    return l1
```

this simple little has some interesting features
1. It work like universal approximation theorem whhich can be used to approx any function no matter how complicate it is
2. It call Neural Network
3. It build via composition which take the result of one function then pass it into a new function on and on PyTorch has name this as sequential


Linear = x@w + bias

non linear

Non linearity is needed to turn the network into more useful function, since linear + linear -> another linear function

reLu: max(tensor(0,0))

sigmoid:
output binary categories between 0 and 1
torch.where can be used to pick from one of these (output must contain only 2 categories)

``` 
def sigmoid(x):
    return 1/(1 + torch.exp(x)) 
```


softmax:
output more than 2 categories which have value all add up to 1
has a side effect of exagerate the large value and diminist smaller value 'cause of exponential effect

```
def softmax(x): return exp(x)/exp(x).sum(dim=1,keepdim=true)
```


It works even when our dependent variable has more than two categories.
It results in faster and more reliable training.
In order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like.



Loglikelyhood
is and indexer which use to pick a value from the list of value output from softmax
PyTorch provide nll_loss assumes that you already took the log of the softmax, so it doesn't actually do the logarithm must use log_softmax (faster and more accurate to take a log at this stage) before nll_loss
Negativeloglikelyhood NLL this function simply apply minus to the value to remove negative value

Cross-Entropy Loss nn.CrossEntropyLoss (does log_softmax and then nll_loss)
Cross-entropy loss is a loss function that is similar to the one we used in the previous chapter, but (as we'll see) has two benefits:
When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss.



DataLoader will iterate over the collection input and return tuple of data in collection

Helpers function for code simplication:
DataLoaders
learner 

create a simple neural network by hand 


*Note:*
 Pytorch Dataset return a tuple when indexing into it

## <span style="color:green">Data Handling</span>

### <span style="color:coral">Data augmentation:</span>
Process to increase dataset where the data is hard to collect and help improve model performance by manipulate the images in a natural ways.

#### <span style="color:orange">Data cleaning</span>

A process to remove incorrect,missing,unnecessary info in the data set collect and make them usable by the model

tip:
fastai can help in data cleaning by let the model work on the data then fix the error where the model has problem with

#### <span style="color:red">DataBlock
use summary to debug
### <span style="color:coral">Presizing:</span>
A special technique use by fastai to improve image manipulation with high efficiency and minimal degradation
It uses strategies:

1. Resize images to relatively "large" dimensionsâ€”that is, dimensions significantly larger than the target training dimensions to have spare margin for more transformation with no empty zone 
    - by resizing to a square using a large random crop size that span the smaller width or height
2. Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times.


#### <span style="color:orange">Training the model:</span>

```
def train_epoch(model, lr, params):
     for xb,yb in dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()
```



### <span style="color:coral">Visualizing Data and result</span>

To understand and diagnose the output from the model various visualization are available
- Confusion matrix show grid of predict vs actual value
  - most_confused for model get the most often wrong answer (interp.most_confused(min_val=5)) to show model confusion the diagonal line is correct a 1 is any place else show mis-prediction

### How the model learn
from Zeiler and Fergus paper which show image of how the model learn
- The earlier layer learn about structure e.g. line,circle, edge, area
- Each layer after that learn more more semantic by using info from earlier layer
to form meaning
- the lastest layer are closer to actual object


## <span style="color:green">Improve model performance</span>


### <span style="color:coral">Learning rate</span>

the right learning rate is important the help improve model performance
- Low learning rate may increase training epoch or overfitting
- Large learning rate may overshoot the minimum loss may decrease performance
- Use the learning rate finder (Learner.lr_find) tool will do these step:
    - start with a very, very small learning rate then double the size for each mini-batch
    - Check the loss if it get worse stop then back-up to the last mini loss that the best one
    Then start with value smaller than this value pick either
    - One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10 `1e-6 to 1e-3)
    - The last point where the loss was clearly decreasing

```
    learn = cnn_learner(dls, resnet34, metrics=error_rate)
    lr_min,lr_steep = learn.lr_find()
```
return the minimum point and steep point in log scale
steep point show the model making great progress (learning)
minimum show the model stop making progress (not learning)

## <span style="color:green">Transfer Learning</span>

 the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task
 - By remove the head which consist of the last layer that do the predicion (softMax layer) which specific to a task
 - replace the head with new one (with random weight) that suitable to new task
    - tell the optimizer to only update the weights in those randomly added final layers
    - Don't change the weights in the rest of the neural network by freeze them
 - Keep the body

#### <span style="color:orange">Fastai Learner.fine_tune methods</span>

- Trains the randomly added layers for one epoch, with all other layers frozen
- Unfreezes (learner.unfreeze) all of the layers, and trains them all for the number of epochs requested
fine_tune function does:
- freeze (body)
- fit_one_cycle
- base_lr/2
- unfreeze (body)
- fit_one_cycle (train all parts together)
*note* after fine_tune you may need to pick a new learning rate to help improve the performance since all parts may not have trained enough together discriminative learning rate is more appropriate at this point


#### <span style="color:orange">Fastai 1 cycle policy</span>
A fastai technique for training the model
It start with low learning rate then gradually increase the learning rate until it reach the max value specify by the user then it stop for the first 1/3 of the batch, for the rest of the batch it gradually decrease the learning rate
Learner.fit_one_cycle

*note:*
The recommend approach is to retrain with smaller epoch with the model overfit
usally the validation loss start getting worse than train error

### <span style="color:coral">Discriminative Learning Rates</span>
Since the body has already been trained it doesn't need the same learning rate the head portion
Discriminative Learning Rates allow for training the head and body with different learning rate
- the body is trained with lower learning rate
- the head is trained with higher learning rate

```
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, 3e-3)
learn.unfreeze()
learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))
```
slice(learning_rate for body,learning_rate for head)

### <span style="color:coral">Training period</span>
if you find that you have overfit, what you should actually do is 
- retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.
- use a deeper architecture (more layers)
    - it takes more resources memory,CPU/GPU cycles
    - take more time (use fp16 to speed up)

### <span style="color:coral">Choosing the right architect</span>
Many achitect comes with various level of layers e.g. ResNet 18,34,50,101 pretrained on ImageNet
Pick the right one is a trial-error process

### Pandas and DataFrames




*Note* 
in Numpy,Panda,PyTorch (trailing : is optional)
 e.g. df.iloc[0,:] -> df.iloc[0] 

### <span style="color:blue">Jargons: </span>

- parameter are weight and bias 
- SGD : stochastic gradient decent calculate the gradient using a small set of data
        in practice is using a loop over a mini-batch to calculate the GD
- GD: gradient descent calculate using the whole dataset all at once
- broadcasting a matrix calculation to speed up without using loop by multiply the same scalar to all the value in the matrix then add them up
- mini-batch: a small set of data with label
- RelU: function that convert - value to 0 leave + value alone
- forward pass: the process that calculate the prediction value
- loss function: function that calculate the error the different between actual value - predict value
- backward pass: the process that calculate the adjustment value use to update the parameter to reduce the loss
- learning rate: size of SGD value use to update the parameters for every loop
- activation value: these number are calculated from output of linear and nonlinear
- parameter value: adjustable value use to improve the performance
- Tensor: multidimention arrays with regular shape
    - rank 0: scalar
    - rank 1: vector
    - rank 2: marix
    - rank 3: 3D tensor
    - rank 4: 4D tensor
- Neural network: comprise with layers
- layer: groups of neural (node) arrange as linear and non-linear output of a layer is pass-on to the layer next to it
- fit: is same training
- Dataset: a colllection of data that returns a tuple of x,y variable of a single item in the collection
- DataLoader: an iterator that return stream of mini-batch. Where each minibatch is comprise with batch of x,y variables


**---------------------------------------------------------------------------------------------------**

---

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![]({{ site.baseurl }}/images/logo.png "fast.ai's logo")

## Code

You can format text and code per usual 

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

Formatting text as shell commands:

```shell
echo "hello world"
./some_script.sh --option "value"
wget https://example.com/cat_photo1.png
```

Formatting text as YAML:

```yaml
key: value
- another_key: "another value"
```

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Tweetcards

{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}

## Footnotes


[^1]: This is the footnote.
