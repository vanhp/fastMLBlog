---
toc: false
layout: post
description: Look under the hood of Recurrent Nueral Network
category: ML
title: RNN the Engine of NLP
image: /ML-section/assets/images/neuralNet1.jpg
---
<!-- ![]({{page.image | relative_url}}) -->

---
## A Language Model from Scratch


### Create test Data
simplest dataset we can that will allow us to try out methods quickly and easily, and interpret the results.

The Human Number dataset

```
from fastai.text.all import *
path = untar_data(URLs.HUMAN_NUMBERS)
Path.BASE_PATH = path
path.ls()
lines = L()
with open(path/'train.txt') as f: lines += L(*f.readlines())
with open(path/'valid.txt') as f: lines += L(*f.readlines())
lines
text = ' . '.join([l.strip() for l in lines])
text[:100]
tokens = text.split(' ')
tokens[:10]
vocab = L(*tokens).unique()
vocab
word2idx = {w:i for i,w in enumerate(vocab)}
nums = L(word2idx[i] for i in tokens)
nums
```

### First Language Model from Scratch


Model to predict each word based on the previous three words
create a list of every sequence of three words as our independent variables, and the next word after each sequence as the dependent variable




```
L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))
# tensor of numericalized value for model
seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))
seqs
bs = 64
cut = int(len(seqs) * 0.8)
# create batch
dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)
```

create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab

use three standard linear layers, but with two tweaks.
1. the first linear layer will use only the first word's embedding as activations, 
    - the second layer will use the second word's embedding plus the first layer's output activations, and 
    - the third layer will use the third word's embedding plus the second layer's output activations. 
    - The key effect of this is that every word is interpreted in the information context of any words preceding it.
2. each of these three layers will use the same weight matrix
    - The way that one word impacts the activations from previous words should not change depending on the position of a word. eventhough, 
    - activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. 
    - So, a layer does not learn one sequence position; it must learn to handle all positions.

The Language model

![simple linear LM](/ML-section/assets/images/simpleLinear1.png)

```
# 3 layer model
class LMModel1(Module):
    def __init__(self, vocab_sz, n_hidden):
        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  
        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     
        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)
  # input2_hidden is embedding layer
  # hidden2_hidden is linear layer
  # hidden2_output is linear layer   
    def forward(self, x):
        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))
        h = h + self.input2_hidden(x[:,1])
        h = F.relu(self.hidden2_hidden(h))
        h = h + self.input2_hidden(x[:,2])
        h = F.relu(self.hidden2_hidden(h))
        return self.hidden2_output(h)

learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, 
                metrics=accuracy)
learn.fit_one_cycle(4, 1e-3)   

compare simplest model which always prdict the next word is 'thousand' to see how it performs
c = Counter(tokens[cut:])
mc = c.most_common(5)
mc
mc[0][1] / len(tokens[cut:])

n,counts = 0,torch.zeros(len(vocab))
for x,y in dls.valid:
    n += y.shape[0]
    for i in range_of(vocab): counts[i] += (y==i).long().sum()
#index of the most common words ('thousand')
idx = torch.argmax(counts)
idx, vocab[idx.item()], counts[idx].item()/n

```
1. The embedding layer (input2_hidden, for input to hidden) 
2. The linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)
3. A final linear layer to predict the fourth word (hidden2_output, for hidden to output)

They all use the same embedding since they come from same data


Refactor into RNN

```
class LMModel2(Module):
    def __init__(self, vocab_sz, n_hidden):
        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  
        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     
        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)
        
    # refactor to use for loop the RNN!
    def forward(self, x):
        h = 0.               # using broascast
        for i in range(3):
            h = h + self.input2_hidden(x[:,i])
            h = F.relu(self.hidden2_hidden(h))
        return self.hidden2_output(h)

learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, 
                metrics=accuracy)
learn.fit_one_cycle(4, 1e-3)

```
![refactor2RNN](/ML-section/assets/images/refac_rnn.png)

Refactor to add memory to RNN

```
class LMModel3(Module):
    def __init__(self, vocab_sz, n_hidden):
        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  
        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     
        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)
        self.h = 0.  # using broascast
        
    # refactor to use for loop the RNN!
    def forward(self, x):
                   
        for i in range(3):
            self.h = self.h + self.input2_hidden(x[:,i])
            self.h = F.relu(self.hidden2_hidden(h))
            
        out = self.hidden2_output(self.h)
        self.h = self.h.detach() # don't do gradient calc
        return out
    def reset(self): self.h = 0.


```
This model now can remember previous activation and the gradient will be calculate on sequence length token of the past not the new one

### Jargons:
- hidden state: The activations that are updated at each step of a recurrent neural network.
- A neural network that is defined using a loop like this is called a recurrent neural network (RNN)
- Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which "detaches" the history of computation steps in the hidden state every few time steps.



---
Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![]({{ site.baseurl }}/images/logo.png "fast.ai's logo")

## Code

You can format text and code per usual 

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

Formatting text as shell commands:

```shell
echo "hello world"
./some_script.sh --option "value"
wget https://example.com/cat_photo1.png
```

Formatting text as YAML:

```yaml
key: value
- another_key: "another value"
```

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Tweetcards

{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}

## Footnotes


[^1]: This is the footnote.
