---
toc: false
layout: post
description: How machine learning work with Human Language 
category: ML
title: Natural Language Process in Machine Learning
image: /ML-section/assets/images/neuralNet1.jpg
---
{% include alert.html text="Warning: This page is under heavy construction and constant changing" %}
---

## NLP Deep Dive: RNN
Language Model is model that train to predict the next in the sentence in the text would be. This type of model is called self-supervise model since it's doesn't require label to guide it's prediction by compare the prediction against the actual label which is the ground truth. It simply learn from the dataset in this case is text and hugh amount of texts. It has it own highly complicate woy of extract the label from the data. It's also has its own way to understand the context in the language that it works with.
Self-supervise is a highly active field in research community. Since is applicable in many domains other than text.

### Pretrained NLP Model
Using pretrained Language model is widely used instead of train them from scratch since it's highly resource and time-consuming. A pretrained language model in one task maybe retrain on different task e.g. retrain a model that was trained in Wikipedia on movies reviews database such as IMDb. As with all pretrained model it's helpful to get insign into the dataset and familar with the style of data e.g. techical,formal,casual type.

### The ULMFit approach
Have a good understanding of the language model foundation is useful to adapt and fine-tuning the pretrained model for new task that may not easily relate to original task. This technique of adapting or refining the pretrained model with data relate to new task prior to training it for the new task (do classification) is call Universal Language Model Fine-tuning. The [ULMFIt](https://arxiv.org/abs/1801.06146) paper indicate that this help improve the performance of the model remarkedbly.
![ulmfit_step](/images/ulmfit1.png)

## Text preprocessing

From knowledge of using categorical variables as an independent variable for NN.

- Make a list of all possible levels (words) of that categorical variable (we'll call this list the vocab).
- Replace each level (word) with its index in the vocab.
- Create an embedding matrix for this containing a row for each level(word) (i.e., for each item of the vocab).
- Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)

These are the basic approach for dealing with text with the exception of text has sequence to deal with sequence in text 
1. concaternate all of the document in the dataset into one giant long string.
2. split the string into list of words
3. assign the independent variable as the sequence of words start from first word to second to last word
4. assign the dependent variable as the sequence of words start from second word ending at last word this will create an offset by 1 of independent variable and dependent variable 

The vocab will contain both old words that were used to pretrain the model and new words that we've just created which the exception that new words won't have any corresponding embedding matrix which will be filled with random value.
the prodcess of create vocab:
1. tokenize the text by convert them into words
2. Numericalize by assign the new unique word an index (int) value
3. create a loader to these data with LMDataLoader class from fastai
4. create language model with RNN that can handle arbitrary size input list

### Text Tokenization

Process of converting text a sequence of characters into group such as word (group of character) in order to assign numerical value. There are three approach:
1. word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning "don't" into "do n't"). Generally, punctuation marks are also split into separate tokens.
2. Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, "occasion" might be tokenized as "o c ca sion."
3. Character-based:: Split a sentence into its individual characters.

### Word Tokenization with fastai
fastai does not provide its own tokenizer but instead provide interface API such as *WordTokenizer* for third party tokenizer which let user choose their own tokenizer if prefered. The default tokenizer currently is *spaCy* library. It's one of more sophistacated and flexible to handle English words. It could split word like it's int it and s and many subtle task. fastai extends this library by adding it's own functionality.
It's add special token and ruld such as *xx* in front of an uncommon word e.g. xxbos signal the start of document this tell the model to clear its own memory for new task.
There are rules such as replace 4 consecutive sequence of !!!! with 1 ! follow by repeat character token and number 4.
So the model can encode in its embeding matrix the info about general concept about repeat punctuation instead of separate token for each ! if it run into the same sequence again it doesn't need to do anything saving computation time and . There are also rule for capitalization the word will be replace with special capitalization token follow by lowercase of the word so there's only 1 lowercase version in the embedding matrix.
example of special token:
- xxbos:: Indicates the beginning of a text (here, a review)
- xxmaj:: Indicates the next word begins with a capital (since we lowercased everything)
- xxunk:: Indicates the next word is unknown
- fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)
- replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it's repeated, then the character
replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it's repeated, then the word
spec_add_spaces:: Adds spaces around / and #
rm_useless_spaces:: Removes all repetitions of the space character
replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxcap) in front of it
replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it
lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)


```
from fastai.text.all import *
path = untar_data(URLs.IMDB)

files = get_text_files(path, folders = ['train', 'test', 'unsup'])
txt = files[0].open().read(); txt[:75]
spacy = WordTokenizer()
toks = first(spacy([txt]))
print(coll_repr(toks, 30))
# how it split this sentence
first(spacy(['The U.S. dollar $1 is $1.00.']))
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt), 31))
# check the rule
defaults.text_proc_rules
coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)
```

### Subword Tokenization
Word tokenization work well for English and language that relie on space as a word separator. But for language that don't relie on space such as Thai, chinese, Japanese and many Asian languages subword tokenizer work better.
It uses a two step process:
1. Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.
2. Tokenize the corpus using this vocab of subword units.

```
# create vocab size    2000 words
txts = L(o.open().read() for o in files[:2000])
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])

subword(1000)
subword(200)
subword(10000)
```
for subword tokenization fastai use "__" to indicate space in the text

Vocab size

larger vocab means fewer tokens per sentence which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn.

### Numericalization with fastai
Numericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST:

- Make a list of all possible levels of that categorical variable (the vocab).
- Replace each level with its index in the vocab.

```
toks = tkn(txt)
print(coll_repr(tkn(txt), 31))
toks200 = txts[:200].map(tkn)
toks200[0]
num = Numericalize()
# call setup to create the vocab
num.setup(toks200)
coll_repr(num.vocab,20)
```
In the output:

- Our special rules tokens appear first, and then every word appears once, in frequency order. 
- The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. 
- fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.

```
# see if it converts to tensor
nums = num(toks)[:20]; nums
' '.join(num.vocab[o] for o in nums)

```

### Batching texts
Since the text needs to be in sequential order so the model can predict the next word in sequence which means the new batch must begin right after the last one get cut-off. The whole array of text string after tokenization will be 
divided int sequence of the same equal length according to the number of batch and sequence length specify.

Here the code to process a sample of small text using 6 batch with each sequence length of 15

```
stream = "In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\nThen we will study how we build a language model and train it for a while."
tokens = tkn(stream)
bs,seq_len = 6,15
d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
```

If the sequence length is too long to fit into memory of GPU it needs to be subdivide into shorter length there are multiple way to do this in fastai as shown here:

```
# method 1
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
# method 2
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
# method 3
bs,seq_len = 6,5
d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])
df = pd.DataFrame(d_tokens)
display(HTML(df.to_html(index=False,header=None)))
```
Each batch is considered a mini-stream. These mini-batch must be in sequence so it retain the meaning which the model can learn by reading them in in sequence.
These process is automatically done by LMDataLoader as shown in sample code here:

```
nums200 = toks200.map(num)
dl = LMDataLoader(nums200)
x,y = first(dl)
x.shape,y.shape
# look at independent variable
' '.join(num.vocab[o] for o in x[0][:20])
# look at dependent variable
' '.join(num.vocab[o] for o in y[0][:20])
```

## Training Text Classifier

The `DataBlock` API will auto tokenize and numericalize when `TextBlock` is passed in.

```
get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])

dls_lm = DataBlock(
    blocks=TextBlock.from_folder(path, is_lm=True),
    get_items=get_imdb, splitter=RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
dls_lm.show_batch(max_n=2)

```
TextBlock internally does
- It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once
- It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs

Fine-Tuning the Language Model for new task

Using embedding matrix to convert integer into activation value for NN.
The RNN network using is *AWD-LSTM*
Merge the embedding of pretrained model with new embedding that fill with random value is done by `language_model_learner`

```
# auto call freeze for pretrained model
learn = language_model_learner(
    dls_lm, AWD_LSTM, drop_mult=0.3, 
    metrics=[accuracy, Perplexity()]).to_fp16()

learn.fit_one_cycle(1, 2e-2) 
learn.save('1epoch')
learn = learn.load('1epoch')
learn.unfreeze()
learn.fit_one_cycle(10, 2e-3)
learn.save_encoder('finetuned')
```
- Using cross-entropy as loss function and perplexity metric which is torch.exp(cross_entropy)
- At the end save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*.

#### Text Generation


A sample test try to train it generate text after a sentence and a random word base on probability return by the model
```
TEXT = "I liked this movie because"

# length of sentence (40 words)
N_WORDS = 40
# numb of sentence
N_SENTENCES = 2
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) 
         for _ in range(N_SENTENCES)]

print("\n".join(preds))
```
### Fine-tune the Classifier
The last step is to fine-tune the classifier

Create a dataloader for the classifier which very similar to vision version
```
dls_clas = DataBlock(
    # use this vocab that already fine-tune, don't generate new one
    #is_lm=False tell TextBlock don't use next token as label
    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),
    get_y = parent_label,
    get_items=partial(get_text_files, folders=['train', 'test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path, path=path, bs=128, seq_len=72)
dls_clas.show_batch(max_n=3)
nums_samp = toks200[:10].map(num)
nums_samp.map(len)
```

- Special padding token is used To but ignore by the model make all the batch of same length require by *PyTorch* DataLoader.
- Sorting by length to batch together document of the same length prior to each epoch.
- all batchs don't have to be in same size only the document in the batch
- will pad all document to same length of the largest one
- all these are done by `DataBlock` when `TextBlock` is passed-in and `is_lm=False` is set

Tip:

When training uses discriminative learning rate and gradual unfreeze work well
```
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, 
                                metrics=accuracy).to_fp16()

learn = learn.load_encoder('finetuned')
learn.fit_one_cycle(1, 2e-2)

# try with new value with gradual unfreeze
learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))

learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))

# the whole body
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

Disinformation and Language Model

This will make it easier to generate fake news,info.








### Jagons:
- Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels
- a word is a categorical variable
- corpus: database of text
- vocab: database of text that have been indexed
- Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)
- Token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character.
- Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab
- Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required
- Language model creation:: We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the <>, but for now, you can think of it as just another deep neural network.
- RNN: recurrent neural network a kind of neural network that have momory to handle long sequence of text
- BOS: is a standard NLP acronym that means "beginning of stream" of text
- Document: contain stream of text that relate to each other and made up of that story. 
_ each epoch these ducument should be shuffle
- encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but "encoder" tends to be more used for NLP and generative models.
- temperature: refer to degree of randomization
- language model predicts the next word of a document, it doesn't need any external labels
- classifier, predicts some external label
- Wikipedia103 contain large amount of English text will cover almost all vocabular, slangs, idioms use currently. It's a good starting point for model to learn from




---
Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![]({{ site.baseurl }}/images/logo.png "fast.ai's logo")

## Code

You can format text and code per usual 

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

Formatting text as shell commands:

```shell
echo "hello world"
./some_script.sh --option "value"
wget https://example.com/cat_photo1.png
```

Formatting text as YAML:

```yaml
key: value
- another_key: "another value"
```

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Tweetcards

{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}

## Footnotes


[^1]: This is the footnote.
