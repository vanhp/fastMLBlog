---
toc: false
layout: post
description: How machine learning work with Human Language 
category: ML
title: Natural Language Process in Machine Learning
image: /ML-section/assets/images/neuralNet1.jpg
---
<!-- ![]({{page.image | relative_url}}) -->

## NLP Deep Dive: RNN
Language Model is model that train to predict the next in the sentence in the text would be. This type of model is called self-supervise model since it's doesn't require label to guide it's prediction by compare the prediction against the actual label which is the ground truth. It simply learn from the dataset in this case is text and hugh amount of texts. It has it own highly complicate woy of extract the label from the data. It's also has its own way to understand the context in the language that it works with.
Self-supervise is a highly active field in research community. Since is applicable in many domains other than text.

### Pretrained NLP Model
Using pretrained Language model is widely used instead of train them from scratch since it's highly resource and time-consuming. A pretrained language model in one task maybe retrain on different task e.g. retrain a model that was trained in Wikipedia on movies reviews database such as IMDb. As with all pretrained model it's helpful to get insign into the dataset and familar with the style of data e.g. techical,formal,casual type.

### The ULMFit approach
Have a good understanding of the language model foundation is useful to adapt and fine-tuning the pretrained model for new task that may not easily relate to original task. This technique of adapting or refining the pretrained model with data relate to new task prior to training it for the new task (do classification) is call Universal Language Model Fine-tuning. The [ULMFIt](https://arxiv.org/abs/1801.06146) paper indicate that this help improve the performance of the model remarkedbly.
![ulmfit_step](/images/ulmfit1.png)

### Text preprocessing

From knowledge of using categorical variables as an independent variable for NN.

- Make a list of all possible levels (words) of that categorical variable (we'll call this list the vocab).
- Replace each level (word) with its index in the vocab.
- Create an embedding matrix for this containing a row for each level(word) (i.e., for each item of the vocab).
- Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)

These are the basic approach for dealing with text with the exception of text has sequence to deal with sequence in text 
1. concaternate all of the document in the dataset into one giant long string.
2. split the string into list of words
3. assign the independent variable as the sequence of words start from first word to second to last word
4. assign the dependent variable as the sequence of words start from second word ending at last word this will create an offset by 1 of independent variable and dependent variable 

The vocab will contain both old words that were used to pretrain the model and new words that we've just created which the exception that new words won't have any corresponding embedding matrix which will be filled with random value.
the prodcess of create vocab:
1. tokenize the text by convert them into words
2. Numericalize by assign the new unique word an index (int) value
3. create a loader to these data with LMDataLoader class from fastai
4. create language model with RNN that can handle arbitrary size input list


### Jagons:
- Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels
- a word is a categorical variable
- corpus: database of text
- vocab: database of text that have been indexed
- Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)
- Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab
- Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required
- Language model creation:: We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the <>, but for now, you can think of it as just another deep neural network.
- RNN: recurrent neural network a kind of neural network that have momory to handle long sequence of text




---
Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![]({{ site.baseurl }}/images/logo.png "fast.ai's logo")

## Code

You can format text and code per usual 

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

Formatting text as shell commands:

```shell
echo "hello world"
./some_script.sh --option "value"
wget https://example.com/cat_photo1.png
```

Formatting text as YAML:

```yaml
key: value
- another_key: "another value"
```

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Tweetcards

{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}

## Footnotes


[^1]: This is the footnote.
