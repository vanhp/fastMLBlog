---
toc: false
layout: post
description: Info about ML-deep learning
category: ML
title: Multi-label Classification
image: /ML-section/assets/images/ai-pic2.jpg
---
<!-- ![]({{page.image | relative_url}}) -->

---

Refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object or more or no object present. This simple solution is not at all widely understood or appreciated! 


## Pandas and DataFrames

Pandas a Python library for database, it's fast and flexible library but has non-intuitive interface. It's used to stored manage analize structure data type. It's also useful in ML to handle tabular and timeseries data type.

Dataframe is the main class that represent data in table of row and columns. It can import data from CSV file, from directories, and many others sources. And Jupyter is happily work with it.
```
import Pandas as pd
df = pd.read_csv(path/'train.csv')
df.head()
tmp_df = pd.DataFrame({'a':[1,2], 'b':[3,4]})
```


### DataBlock
A fastai versatile,flexible object that manage data.
it has facility to handle Datasets,dataframe. 
It smart enough to understand data set that is working with. It's also auto split data into train 80% validate 20%


```
# empty datablock
dblock = DataBlock()
dsets = dblock.datasets(df)
len(dsets.train),len(dsets.valid)
x,y = dsets.train[0]
#take lambda/ or function as argument to retrieve filename and label
dblock2 = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])
dsets2 = dblock.datasets(df)
dsets2.train[0]
# take default name
dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),
                   get_x = get_x, get_y = get_y)
dsets = dblock.datasets(df)
dsets.train[0]
```

### Binary cross entropy Loss 
One of the problem with multiple label is finding appropiate loss function that can handle the possibility of multi-label and no label may present in data. The regular cross entropy loss is only able to deal with single label. Finding loss function that suitable for case where there more than one label or no label present is needed.

### Learner object

The Learner object inherit from nn.Module comprise with:
1. The model
2. DataLoaders object
3. Optimizer
4. Loss function

It can receive x as a mini-batch and return activation value

```
learn = cnn_learner(dls, resnet18)
activ = learn.model(x)
activ.shape

def binary_cross_entropy(inputs, targets):
    inputs = inputs.sigmoid()
    return -torch.where(targets==1, inputs, 1-inputs).log().mean()
# using PyTorch version
# by default fastai will use this version
loss_func = nn.BCEWithLogitsLoss()
loss = loss_func(activs, y)
loss
```

due to one-hot-encoded dependent variable because there may be more than object the cross_entropy is not appropriate
In the case where more than one object is possible, or no object present may want the sum to be < 1
this excluse softmax and nll_loss

softmax: limitation:
- all value must sum to 1
- the largest value get exagerate
nll_loss limitation:
- it return only one value correspond to the single label
we need to return more than 1 label

#### Comparable PyTorch versions that work with one-hot encoding:
- F.binary_cross_entropy (no sigmoid)
- nn.BCELoss  (nosigmoid)
- F.binary_cross_entropy_with_logits
- nn.BCEWithLogitsLoss
Single label version:
F.nll_loss
nn.NLLLoss (no softmax)
F.cross_entropy
nn.CrossEntropyLoss


### Measurement metric
Since there maybe more than one or object present need a new metric for accuracy

```
# value > thresh is consider 1, else consider 0, default threshold = 0.5
def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):
    "Compute accuracy when `inp` and `targ` are the same size."
    if sigmoid: inp = inp.sigmoid()
    return ((inp>thresh)==targ.bool()).float().mean()

# train the model
learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))
learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)

```
High threshold select the high confident of model
Low threshold select low confident of model

find the best threshold by trying a few levels and seeing what works best
```
# this apply sigmoid by default
preds,targs = learn.get_preds()
# no sigmoid
accuracy_multi(preds, targs, thresh=0.9, sigmoid=False)
# plot it
xs = torch.linspace(0.05,0.95,29)
accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]
plt.plot(xs,accs)
```



*Note*
- zip(b[0],b[1]) has short cut of zip(*b)
- python can't save (serialization lambda) lambda use function instead
- it's OK to use validation set to pick threshold it should not overfit


### Jagons
- x: is independent variable
- y: is dependent variable
- Dataset: A collection that can be index into it and returns a tuple of your independent and dependent variable for a single item
- DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables
- one-hot-encoder: a vector that has all 0 but one 1 to represent the interested item to pick-out from other item



Fastai library
- Datasets: An object that contains a training Dataset and a validation Dataset part of fastai library
- DataLoaders: An object that contains a training DataLoader and a validation DataLoader part of fastai library






---
*------------------------------------------------------------------------------------------------------*
