{
  
    
        "post0": {
            "title": "RNN the Engine of NLP",
            "content": ". A Language Model from Scratch . Create test Data . simplest dataset we can that will allow us to try out methods quickly and easily, and interpret the results. . The Human Number dataset . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines text = &#39; . &#39;.join([l.strip() for l in lines]) text[:100] tokens = text.split(&#39; &#39;) tokens[:10] vocab = L(*tokens).unique() vocab word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . First Language Model from Scratch . Model to predict each word based on the previous three words create a list of every sequence of three words as our independent variables, and the next word after each sequence as the dependent variable . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) # tensor of numericalized value for model seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs bs = 64 cut = int(len(seqs) * 0.8) # create batch dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab . use three standard linear layers, but with two tweaks. . the first linear layer will use only the first word’s embedding as activations, the second layer will use the second word’s embedding plus the first layer’s output activations, and | the third layer will use the third word’s embedding plus the second layer’s output activations. | The key effect of this is that every word is interpreted in the information context of any words preceding it. | . | each of these three layers will use the same weight matrix The way that one word impacts the activations from previous words should not change depending on the position of a word. eventhough, | activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. | So, a layer does not learn one sequence position; it must learn to handle all positions. | . | The Language model . . # 3 layer model class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.input2_hidden = nn.Embedding(vocab_sz, n_hidden) self.hidden2_hidden = nn.Linear(n_hidden, n_hidden) self.hidden2_output = nn.Linear(n_hidden,vocab_sz) # input2_hidden is embedding layer # hidden2_hidden is linear layer # hidden2_output is linear layer def forward(self, x): h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0]))) h = h + self.input2_hidden(x[:,1]) h = F.relu(self.hidden2_hidden(h)) h = h + self.input2_hidden(x[:,2]) h = F.relu(self.hidden2_hidden(h)) return self.hidden2_output(h) learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) compare simplest model which always prdict the next word is &#39;thousand&#39; to see how it performs c = Counter(tokens[cut:]) mc = c.most_common(5) mc mc[0][1] / len(tokens[cut:]) n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() #index of the most common words (&#39;thousand&#39;) idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . The embedding layer (input2_hidden, for input to hidden) | The linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden) | A final linear layer to predict the fourth word (hidden2_output, for hidden to output) | They all use the same embedding since they come from same data . Refactor to use loop the RNN . . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.input2_hidden = nn.Embedding(vocab_sz, n_hidden) self.hidden2_hidden = nn.Linear(n_hidden, n_hidden) self.hidden2_output = nn.Linear(n_hidden,vocab_sz) # refactor to use for loop the RNN! def forward(self, x): h = 0. # using broascast for i in range(3): h = h + self.input2_hidden(x[:,i]) h = F.relu(self.hidden2_hidden(h)) return self.hidden2_output(h) learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . . Refactor to add memory to RNN . class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.input2_hidden = nn.Embedding(vocab_sz, n_hidden) self.hidden2_hidden = nn.Linear(n_hidden, n_hidden) self.hidden2_output = nn.Linear(n_hidden,vocab_sz) self.h = 0. # using broascast # refactor to use for loop the RNN! def forward(self, x): for i in range(3): self.h = self.h + self.input2_hidden(x[:,i]) self.h = F.relu(self.hidden2_hidden(h)) out = self.hidden2_output(self.h) self.h = self.h.detach() # do bptt return out def reset(self): self.h = 0. . BPTT Backpropagation through time . This model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT . # rearrange data so model see in particular sequence m = len(seqs)//bs m,bs,len(seqs) # reindex model see as contiguous batch with each epoch def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, # drop last batch that have diff shape shuffle=False) # maintain sequence learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) # callback to reset each epoch learn.fit_one_cycle(10, 3e-3) . Add More signal: keep the output . No longer throw away the output from previous run but add them as input to current run . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) # check if it still offset by 1 [L(vocab[o] for o in s) for s in seqs[0]] . Model now output every word instead of every 3 words in order to feed this into next run . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . Multi-Layer RNN . To improve the model further since the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to do this is to stack more layers and feed the output from one layer into the next layer so on. . Look at it in unrolling way . The new Model using PyTorch . The new model is too deep layers this lead to problem of vanishing or exploding gradient . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . Exploding or Disappearing Activations . One problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it’s depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage. . The impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from here. This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing gradient or the value get larger and larger exponentially or explode to infinite. . The is problem in the main reason why RNN type model is hard to train than CNN type model,however research is very active to try to way to reduce or avoid these problem. . Neural Network that have Memory . In Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular are LSTM and GRU. GRU is a simplify version of LSTM . . Comparison LSTM and GRU . . LSTM architecture . Internally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete. . sigmoid equation: . $f(x) = frac{1}{1 + e^{-x}} = frac{e^x}{e^x + 1} = frac12 + frac12 tanh left( frac{x}{2} right)$ . tanh equation: . $f(x) = tanh x = frac{e^x-e^{-x}}{e^x+e^{-x}}$ . The little NN is compose of gates call first gate,input gate, cell gate . The forget $f_{t}$ gate: . Take input $x$ add to hidden state $h_{t-1}$ and previous state call cell state $C_{t-1}$ the memory into current linear neural node then out via sigmoid ($ sigma$) to be multiply with the Cell state it decide should the value be kept or discarded if value closer to 1 the value is kept else the value is discarded this is how NN can forget or remember or tanh a sigmoid function rescaled to the range -1 to 1 to the next . The input $i_{t}$gate: . combine with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid . The cell $ tilde{C_{t}}$ gate: Decide what value to update from the range of -1 to 1 output from tanh function the value then add with cell state $C_{t-1}$ value to get $C_{t}$ the new value . The Output $o_{t}$ gate: . Decide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer. . . Another look at LSTM . GRU: Gate Recurrent Units architecture . . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.stack([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = outgate * torch.tanh(c) return h, (h,c) . Refactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task. . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state # One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . Train the LSTM . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . Regularizing an LSTM . Although hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. . Dropout . Dropout is a regularization technique is to randomly change some activations to zero at training time makes sure all neurons actively work toward the output, and relie on the input from the source that the neural regularly receive the input since these sources may not be there. Dropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating. . Jargons: . hidden state: The activations that are updated at each step of a recurrent neural network. | A neural network that is defined using a loop like this is called a recurrent neural network (RNN) | Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don’t store them. | . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/19/ML-RNN.html",
            "relUrl": "/ml-section/ml/2020/09/19/ML-RNN.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Natural Language Process in Machine Learning",
            "content": "NLP Deep Dive: RNN . Language Model is model that train to predict the next in the sentence in the text would be. This type of model is called self-supervise model since it’s doesn’t require label to guide it’s prediction by compare the prediction against the actual label which is the ground truth. It simply learn from the dataset in this case is text and hugh amount of texts. It has it own highly complicate woy of extract the label from the data. It’s also has its own way to understand the context in the language that it works with. Self-supervise is a highly active field in research community. Since is applicable in many domains other than text. . Pretrained NLP Model . Using pretrained Language model is widely used instead of train them from scratch since it’s highly resource and time-consuming. A pretrained language model in one task maybe retrain on different task e.g. retrain a model that was trained in Wikipedia on movies reviews database such as IMDb. As with all pretrained model it’s helpful to get insign into the dataset and familar with the style of data e.g. techical,formal,casual type. . The ULMFit approach . Have a good understanding of the language model foundation is useful to adapt and fine-tuning the pretrained model for new task that may not easily relate to original task. This technique of adapting or refining the pretrained model with data relate to new task prior to training it for the new task (do classification) is call Universal Language Model Fine-tuning. The ULMFIt paper indicate that this help improve the performance of the model remarkedbly. . Text preprocessing . From knowledge of using categorical variables as an independent variable for NN. . Make a list of all possible levels (words) of that categorical variable (we’ll call this list the vocab). | Replace each level (word) with its index in the vocab. | Create an embedding matrix for this containing a row for each level(word) (i.e., for each item of the vocab). | Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.) | . These are the basic approach for dealing with text with the exception of text has sequence to deal with sequence in text . concaternate all of the document in the dataset into one giant long string. | split the string into list of words | assign the independent variable as the sequence of words start from first word to second to last word | assign the dependent variable as the sequence of words start from second word ending at last word this will create an offset by 1 of independent variable and dependent variable | The vocab will contain both old words that were used to pretrain the model and new words that we’ve just created which the exception that new words won’t have any corresponding embedding matrix which will be filled with random value. the prodcess of create vocab: . tokenize the text by convert them into words | Numericalize by assign the new unique word an index (int) value | create a loader to these data with LMDataLoader class from fastai | create language model with RNN that can handle arbitrary size input list | Text Tokenization . Process of converting text a sequence of characters into group such as word (group of character) in order to assign numerical value. There are three approach: . word-based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning “don’t” into “do n’t”). Generally, punctuation marks are also split into separate tokens. | Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, “occasion” might be tokenized as “o c ca sion.” | Character-based:: Split a sentence into its individual characters. | Word Tokenization with fastai . fastai does not provide its own tokenizer but instead provide interface API such as WordTokenizer for third party tokenizer which let user choose their own tokenizer if prefered. The default tokenizer currently is spaCy library. It’s one of more sophistacated and flexible to handle English words. It could split word like it’s int it and s and many subtle task. fastai extends this library by adding it’s own functionality. It’s add special token and ruld such as xx in front of an uncommon word e.g. xxbos signal the start of document this tell the model to clear its own memory for new task. There are rules such as replace 4 consecutive sequence of !!!! with 1 ! follow by repeat character token and number 4. So the model can encode in its embeding matrix the info about general concept about repeat punctuation instead of separate token for each ! if it run into the same sequence again it doesn’t need to do anything saving computation time and . There are also rule for capitalization the word will be replace with special capitalization token follow by lowercase of the word so there’s only 1 lowercase version in the embedding matrix. example of special token: . xxbos:: Indicates the beginning of a text (here, a review) | xxmaj:: Indicates the next word begins with a capital (since we lowercased everything) | xxunk:: Indicates the next word is unknown | fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these) | replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word spec_add_spaces:: Adds spaces around / and # rm_useless_spaces:: Removes all repetitions of the space character replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxcap) in front of it replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos) | . from fastai.text.all import * path = untar_data(URLs.IMDB) files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) txt = files[0].open().read(); txt[:75] spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) # how it split this sentence first(spacy([&#39;The U.S. dollar $1 is $1.00.&#39;])) tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) # check the rule defaults.text_proc_rules coll_repr(tkn(&#39;&amp;copy; Fast.ai www.fast.ai/INDEX&#39;), 31) . Subword Tokenization . Word tokenization work well for English and language that relie on space as a word separator. But for language that don’t relie on space such as Thai, chinese, Japanese and many Asian languages subword tokenizer work better. It uses a two step process: . Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab. | Tokenize the corpus using this vocab of subword units. | # create vocab size 2000 words txts = L(o.open().read() for o in files[:2000]) def subword(sz): sp = SubwordTokenizer(vocab_sz=sz) sp.setup(txts) return &#39; &#39;.join(first(sp([txt]))[:40]) subword(1000) subword(200) subword(10000) . for subword tokenization fastai use “__” to indicate space in the text . Vocab size . larger vocab means fewer tokens per sentence which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn. . Numericalization with fastai . Numericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST: . Make a list of all possible levels of that categorical variable (the vocab). | Replace each level with its index in the vocab. | . toks = tkn(txt) print(coll_repr(tkn(txt), 31)) toks200 = txts[:200].map(tkn) toks200[0] num = Numericalize() # call setup to create the vocab num.setup(toks200) coll_repr(num.vocab,20) . In the output: . Our special rules tokens appear first, and then every word appears once, in frequency order. | The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. | fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter. | . # see if it converts to tensor nums = num(toks)[:20]; nums &#39; &#39;.join(num.vocab[o] for o in nums) . Batching texts . Since the text needs to be in sequential order so the model can predict the next word in sequence which means the new batch must begin right after the last one get cut-off. The whole array of text string after tokenization will be divided int sequence of the same equal length according to the number of batch and sequence length specify. . Here the code to process a sample of small text using 6 batch with each sequence length of 15 . stream = &quot;In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the PreProcessor used in the data block API. nThen we will study how we build a language model and train it for a while.&quot; tokens = tkn(stream) bs,seq_len = 6,15 d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . If the sequence length is too long to fit into memory of GPU it needs to be subdivide into shorter length there are multiple way to do this in fastai as shown here: . # method 1 bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) # method 2 bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) # method 3 bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . Each batch is considered a mini-stream. These mini-batch must be in sequence so it retain the meaning which the model can learn by reading them in in sequence. These process is automatically done by LMDataLoader as shown in sample code here: . nums200 = toks200.map(num) dl = LMDataLoader(nums200) x,y = first(dl) x.shape,y.shape # look at independent variable &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) # look at dependent variable &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . Training Text Classifier . The DataBlock API will auto tokenize and numericalize when TextBlock is passed in. . get_imdb = partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=128, seq_len=80) dls_lm.show_batch(max_n=2) . TextBlock internally does . It saves the tokenized documents in a temporary folder, so it doesn’t have to tokenize them more than once | It runs multiple tokenization processes in parallel, to take advantage of your computer’s CPUs | . Fine-Tuning the Language Model for new task . Using embedding matrix to convert integer into activation value for NN. The RNN network using is AWD-LSTM Merge the embedding of pretrained model with new embedding that fill with random value is done by language_model_learner . # auto call freeze for pretrained model learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() learn.fit_one_cycle(1, 2e-2) learn.save(&#39;1epoch&#39;) learn = learn.load(&#39;1epoch&#39;) learn.unfreeze() learn.fit_one_cycle(10, 2e-3) learn.save_encoder(&#39;finetuned&#39;) . Using cross-entropy as loss function and perplexity metric which is torch.exp(cross_entropy) | At the end save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. | . Text Generation . A sample test try to train it generate text after a sentence and a random word base on probability return by the model . TEXT = &quot;I liked this movie because&quot; # length of sentence (40 words) N_WORDS = 40 # numb of sentence N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] print(&quot; n&quot;.join(preds)) . Fine-tune the Classifier . The last step is to fine-tune the classifier . Create a dataloader for the classifier which very similar to vision version . dls_clas = DataBlock( # use this vocab that already fine-tune, don&#39;t generate new one #is_lm=False tell TextBlock don&#39;t use next token as label blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) dls_clas.show_batch(max_n=3) nums_samp = toks200[:10].map(num) nums_samp.map(len) . Special padding token is used To but ignore by the model make all the batch of same length require by PyTorch DataLoader. | Sorting by length to batch together document of the same length prior to each epoch. | all batchs don’t have to be in same size only the document in the batch | will pad all document to same length of the largest one | all these are done by DataBlock when TextBlock is passed-in and is_lm=False is set | . Tip: . When training uses discriminative learning rate and gradual unfreeze work well . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() learn = learn.load_encoder(&#39;finetuned&#39;) learn.fit_one_cycle(1, 2e-2) # try with new value with gradual unfreeze learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) # the whole body learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . Disinformation and Language Model . This will make it easier to generate fake news,info. . Jagons: . Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels | a word is a categorical variable | corpus: database of text | vocab: database of text that have been indexed | Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) | Token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character. | Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab | Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required | Language model creation:: We need a special kind of model that does something we haven’t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the &lt;&gt;, but for now, you can think of it as just another deep neural network. | RNN: recurrent neural network a kind of neural network that have momory to handle long sequence of text | BOS: is a standard NLP acronym that means “beginning of stream” of text | Document: contain stream of text that relate to each other and made up of that story. _ each epoch these ducument should be shuffle | encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but “encoder” tends to be more used for NLP and generative models. | temperature: refer to degree of randomization | language model predicts the next word of a document, it doesn’t need any external labels | classifier, predicts some external label | Wikipedia103 contain large amount of English text will cover almost all vocabular, slangs, idioms use currently. It’s a good starting point for model to learn from | . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/19/ML-NLP.html",
            "relUrl": "/ml-section/ml/2020/09/19/ML-NLP.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Working with Tabular Data",
            "content": ". Tabular Modeling Deep Dive . The objective is to predict the value in one column based on the values in the other columns. There are other technique in ML field such as random forest that also provide good or better result as DL which suite certain type of problem. . There are plethora of methods in ML many of them only applicable to certain situation. Most of the technique in machine learning can be boil-down to just two proven methods: . For structure data types: . Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies). Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles than DL. | There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation? | . | For unstructure data types: . Multilayered neural networks learned with SGD (Deep Learning) (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) DL also work well with structure data type may be slower to train and hard to intepret result than ensemble decision trees which also don’t need GPU, less parameter tuning,more mature ecosystem. . | There are some high-cardinality categorical variables that are very important (“cardinality” refers to the number of discrete levels representing categories, so a high-cardinality categorical variable is something like a zip code, which can take on thousands of possible levels). | . | There are some columns that contain data that would be best understood with a neural network, such as plain text data. | . | Ensemble Decision Trees . Decision tree don’t require matrix multiplication or derivative. So PyTorch is of no help. Scikit-learn is better suit for this task. . Data to use for Decision tree bluebookforbulldozers . Handling Different data types from tables Dataset: the Blue Book for Bulldozers Kaggle competition . df = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df.columns # inspect data df[&#39;ProductSize&#39;].unique() sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) dep_var = &#39;SalePrice df[dep_var] = np.log(df[dep_var]) . The most important data column is the dependent variable . selecting the metric is an important part of the project setup that use to gauge the performance of the model unless it’s already specify. this is set for root mean square error (RMSLE) between actual and predict value. . Decision Trees . This work like binary tree where parent node is the middle whith the left is &lt;= parent and right child node larger. Where the leaf node is the prediction . The basic steps to train a decision tree can be written down very easily: . Loop through each column of the dataset in turn. | For each column, loop through each possible level of that column in turn. | Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable). | Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group. | After looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model. | We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group. | Continue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it. | Handling Dates Since have some important in some context but may less important in different context, e.g. yesterday, tomorrow,lastweek, holiday, day of week, day of month fastai provide function add_datepart to do this task . df_test = pd.read_csv(path/&#39;Test.csv&#39;, low_memory=False) df_test = add_datepart(df_test, &#39;saledate&#39;) &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . Handling missing data and string . fastai provide TabularPandas, and TabularProc which has Categorify,and FillMissing functions. TabularProc: . It returns the exact same object that’s passed to it, after modifying the object in place. | It runs the transform once, when data is first passed in, rather than lazily as the data is accessed. | Categorify replaces a column with a numeric categorical column | FillMissing that replaces missing values with the median of that column and creates a new Boolean column that is set to True for any row where the value was missing procs = [Categorify, FillMissing] | . | . TabularPandas will also handle splitting the dataset into training and validation sets . cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . TabularPandas needs to be told which columns are continuous and which are categorical. We can handle that automatically using the helper function cont_cat_split: . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) len(to.train),len(to.valid) to.show(3) to1 = TabularPandas(df, procs, [&#39;state&#39;, &#39;ProductGroup&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;], [], y_names=dep_var, splits=splits) to1.show(3) ## check the underline value to.items.head(3) . the conversion process The conversion of categorical columns to numbers is done by simply replacing each unique level with a number The numbers associated with the levels are chosen consecutively as they are seen in a column, so there’s no particular meaning to the numbers in categorical columns after conversion.except if you first convert a column to a Pandas ordered category you must provide the ordering . to.classes[&#39;ProductSize&#39;] (path/&#39;to.pkl&#39;).save(to) # load back in to = (path/&#39;to.pkl&#39;).load() xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y m = DecisionTreeRegressor(max_leaf_nodes=4) # train m.fit(xs, y); draw_tree(m, xs, size=7, leaves_parallel=True, precision=2) . Categorical Variables The decision tree can handle these variable with ease since they are treat just another variables that may group into a node according the splitting criteria in which case may split down to the leaf node. . Random Forests . A baggin technique where the data was divide into subset then . randomly pick a subset of the row of data | train a model on that subset | save the model then repeat from step 1 a few time | This process will result in number of trained models them | make prediction from these models then take the average of these prediction this is call bagging Random forest is baggin with randomly choose subset of column making split in each decision tree. In essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. | Create a Random forest . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) # n_estimators defines the number of trees we want # max_samples defines how many rows to sample for training each tree # max_features defines how many columns to sample at each split point (where 0.5 means &quot;take half the total number of columns&quot;) # min_samples_leaf specify when to stop splitting the tree nodes limiting the depth of the tree # n_jobs=-1 to tell sklearn to use all our CPU in parallel m = rf(xs, y); m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) r_mse(preds.mean(0), valid_y) plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); r_mse(m.oob_prediction_, y) . Out-of-Bag Error . The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row’s error trees where that row was not included in training. allows us to see whether the model is overfitting, without needing a separate validation set. it only use the error from the tree don’t use that subset of data for training effectively a validation set . Model Interpretation . For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: . How confident are we in our predictions using a particular row of data? For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? Which columns are the strongest predictors, which can we ignore? Which columns are effectively redundant with each other, for purposes of prediction? How do predictions vary, as we vary these columns? . Tree Variance for Prediction Confidence . How confident of the model on the prediction. One simple way is to use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions. In general, we would want to be more cautious of using the results for rows where trees give very different results (higher standard deviations), compared to cases where they are more consistent (lower standard deviations). . # get the prediction from all trees preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) preds.shape preds_std = preds.std(0) preds_std[:5] . Feature Importance . want to know how it’s making predictions use the attribute feature_importances_ . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1. . Removing Low-Importance Variables . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] m = rf(xs_imp, y) m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y) len(xs.columns), len(xs_imp.columns) plot_fi(rf_feat_importance(m, xs_imp)); . Removing Redundant Features . By focusing on the most important variables, and removing some redundant ones, we’ve greatly simplified our model. By merging columns that most similar to each other start from the leaf . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ get_oob(xs_imp) # try removing each of our potentially redundant variables, one at a time: {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) # save (path/&#39;xs_final.pkl&#39;).save(xs_final) (path/&#39;valid_xs_final.pkl&#39;).save(valid_xs_final) # load back xs_final = (path/&#39;xs_final.pkl&#39;).load() valid_xs_final = (path/&#39;valid_xs_final.pkl&#39;).load() m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . Partial Dependence . Try to understand the relationship of the most important predictors . #count the # p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . Data Leakage . Data leakage is subtle and can take many forms. In particular, missing values often represent data leakage. . Tree Interpreter . !pip install treeinterpreter !pip install waterfallcharts . Use water fall to draw a chart to answer this question. For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? . import warnings warnings.simplefilter(&#39;ignore&#39;, FutureWarning) from treeinterpreter import treeinterpreter from waterfall_chart import plot as waterfall row = valid_xs_final.iloc[:5] prediction,bias,contributions = treeinterpreter.predict(m, row.values) prediction[0], bias[0], contributions[0].sum() waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . Extrapolation and Neural Networks . The value in the dataset determine the min and maximum value that Random forest can be. It cannot extrapolate value that beyon this boundary . Finding Out-of-Domain Data . The test dataset and train,validation dataset may not have the same distribution pattern. Especially,data that don’t fit the general norm of rest of dataset. If they are too much different this may skew the result. . Using Random forest to help find uneven distribution . If the either the test,validation set have the same distribution with the train set there should be no predicting power another word, it should be 0. step . combine the train and validation | create a dependent variable to reprent the data from each row | build the RF | check the model feature importance | if the value differ significantly this indicate different distribution | try to remove this value to see if it affect the outcome | df_dom = pd.concat([xs_final, valid_xs_final]) is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] # take the base line of original RMSE value for comparison m = rf(xs_final, y) print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) for c in (&#39;SalesID&#39;,&#39;saleElapsed&#39;,&#39;MachineID&#39;): m = rf(xs_final.drop(c,axis=1), y) print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y)) # look like these 2 variables may be removable with little effect time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) m = rf(xs_final_time, y) m_rmse(m, valid_xs_time, valid_y) # tip remove some old data that are less relevance to whole data xs[&#39;saleYear&#39;].hist(); # let retrain see if it improve! filt = xs[&#39;saleYear&#39;]&gt;2004 xs_filt = xs_final_time[filt] y_filt = y[filt] m = rf(xs_filt, y_filt) m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y) . Tabular with Neural Network . In NN categorical data is require the use of embedding. Fastai can convert categorical variable columns to embedding but it must be specified. It does this by comparing the number of distinct levels in the variable to the value of the max_card == 9000 parameter. If it’s lower, fastai will treat the variable as categorical else treat as continous variable. It creates embeding very large size but &lt; 10000 is best . df_nn = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] # max_card &gt; 9000 treat as continous else treat as categorical cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) # use this variable to predict future sale price # make it a continous variable #this col has &gt; 9000 but don&#39;t want in categorical, move to continous cont_nn.append(&#39;saleElapsed&#39;) # take out from categorical col cat_nn.remove(&#39;saleElapsed&#39;) # check its cardinallity of each variable df_nn_final[cat_nn].nunique() # hi # of card mean large # of row in embedding matrix # remove it but make sure it doesn&#39;t affect RF model xs_filt2 = xs_filt.drop(&#39;fiModelDescriptor&#39;, axis=1) valid_xs_time2 = valid_xs_time.drop(&#39;fiModelDescriptor&#39;, axis=1) m2 = rf(xs_filt2, y_filt) m_rmse(m, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y) #look ok, remove it cat_nn.remove(&#39;fiModelDescriptor&#39;) . Create Panda tabular with normalization (x - mean/std) for NN because care about the scale. However, RF only care about the order of values in the the variable . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . Can use large batch size since Tabular use less GPU ram than NN. . dls = to_nn.dataloaders(1024) y = to_nn.train.y # set y_range for regression model y.min(),y.max() . create the learner, set the loss function to MSE By default, for tabular data fastai creates a neural network with two hidden layers, with 200 and 100 activations, respectively for larger dataset set them higher . from fastai.tabular.all import * learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) learn.lr_find() learn.fit_one_cycle(5, 1e-2) preds,targs = learn.get_preds() r_mse(preds,targs) learn.save(&#39;nn&#39;) . Ensemble of Nueral Net and Random Forest get the best of both world? . Since Random Forest itself is comprise with group of Decision trees. . The bagging technique. . It’s reasonable to expect that add Neural network into the bag then average the result would be possible. The question is will it improve the performance. . One problem is that PyTorch use different tensor format e.g. rank-2,scikitlearn/Numpy use rank-1. The squeeze function come in handy to remove the extra axis from PyTorch to Numpy . rf_preds = m.predict(valid_xs_time) ens_preds = (to_np(preds.squeeze()) + rf_preds) /2 r_mse(ens_preds,valid_y) . The Boosting technique . Instead of averaging the result of the model as in RF. the step in this approach do: . Train a small model that underfits your dataset. | Calculate the predictions in the training set for this model. | Subtract the predictions from the targets; these are called the “residuals” and represent the error for each point in the training set. | . | Go back to step 1, but instead of using the original targets, use the residuals as the targets for the training. | Continue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse. | each new tree will be attempting to fit the error of all of the previous trees combined. | Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller. | To make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. | . Some well-known models and libraries . Gradient boosting machines (GBMs) and | gradient boosted decision trees (GBDTs) | XGBoost | . Combining Embeddings with Other Methods . Embedding (array lookup) may help improve the performance of Neural Network especially at inference time. . Note: . Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables… [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit… As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering. | In practice try both methods to see which one better suite for the task | Deep learning generally work well with highly complex data type | the leaf node is the predicting result | non leaf node must be divide further until leaf node | the criteria for splitting should make sense for the data type | check leaf node if there too many of them maybe the result of splitting on unsual condition | Randdom forest not very sensitive to the hyperparameter choices, such as max_features so the tree can be many as there time to train | the more tree the more accurate | set max_samples to default if data is less than 200000 otherwise set to 200000 | use setting for max_features=0.5,min_sample_leaf=4 work well | generally the first step to improving a model is simplifying it | Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. | PyTorch unsqueeze() to add axis to data or using x[:,None] in Python the None mean add a axis, to do the same job | boosting can lead to overfitting with a boosted ensemble, the more trees you have, the better the training error becomes, and | eventually you will see overfitting on the validation set. | . | . key insign: . an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer | the embedding transforms the categorical variables into inputs that are both continuous and meaningful. | Boosting is in very active research subjects | gradient boosted trees are extremely sensitive to the choices of the hyperparameters use a loop that tries a range of different hyperparameters to find the ones that work best. | . | Random forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods. | Gradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests. . Neural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting. . starting your analysis with a random forest. This will give you a strong baseline you can be confident that it’s a reasonable starting point. Then use that model for feature selection and partial dependence analysis, to get a better understanding of your data. | Then try neural nets and GBMs if they give significantly better results on your validation set in a reasonable amount of time, use them. | If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better. | . | How fastai treat Tabular Data . fastai’s Tabular Classes In fastai, a tabular model is simply a model that takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well. . The model created in tabular_learner is an object of class TabularModel. Take a look at the source for tabular_learner now (remember, that’s tabular_learner?? in Jupyter). You’ll see that like collab_learner, it first calls get_emb_sz to calculate appropriate embedding sizes (you can override these by using the emb_szs parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the TabularModel, and passes that to TabularLearner (note that TabularLearner is identical to Learner, except for a customized predict method). . That means that really all the work is happening in TabularModel, so take a look at the source for that now. With the exception of the BatchNorm1d and Dropout layers (which we’ll be learning about shortly), you now have the knowledge required to understand this whole class. Take a look at the discussion of EmbeddingNN at the end of the last chapter. Recall that it passed n_cont=0 to TabularModel. We now can see why that was: because there are zero continuous variables (in fastai the n_ prefix means “number of,” and cont is an abbreviation for “continuous”). . Jagons: . Continuous variables are numerical data, such as “age,” that can be directly fed to the model, | Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning | bagging process of random take rows of data to train model and make prediction then average over all prediction | . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/18/ML-tabular.html",
            "relUrl": "/ml-section/ml/2020/09/18/ML-tabular.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Collaborative Filter deep dive",
            "content": ". A general class of system known as Recommend system that recomend a product to a customer use in Amazon, a movie or video to viewer which is used in Netflix, what’s story to show in Facebook,Tweeter… Which is base the user own history, in some other users that may have similar preference. For example, Netflix recommend a movie to you base on other people who watch the same movie. Or Amazon recommend a product base on other who bought or view the same product. . It’s base on the idea of latent factors the unwritten or unspecify context that underline the item. . !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * rom fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() last_skywalker = np.array([0.98,0.9,-0.9]) user1 = np.array([0.9,0.8,-0.6]) (user1*last_skywalker).sum() casablanca = np.array([-0.99,-0.3,0.8]) (user1*casablanca).sum() . Learning the Latent Factors . DataLoaders . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() ratings = ratings.merge(movies) ratings.head() #build dataloaders #By default, it takes the first column for the user, the second column for the item dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() dls.classes n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . To calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. To do lookup in matrix form which model can calculate is to represent lookup with vector of one-hot encoding . one_hot_3 = one_hot(3, n_users).float() user_factors.t() @ one_hot_3 user_factors[3] . Embedding . A technique of look up item by using matrix multiplication with one-hot encode vector. PyTorch has a special layer that do this task in a fast and efficient way. . Collaborative by hand from scratch . class DotProduct(Module): # constructor # use slightly higher range since sigmoid max of 5 to get 5 need to over def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) # account for user bias self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) # account for bias in data self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range # callback from Pytorch def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users + movies).sum(dim=1,keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) #force those predictions to be between 0 and 5 with sigmoid_range return sigmoid_range(res, *self.y_range) model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . Overfitting . A phenomena that happen when the model start to recgonize the dataset and try to memorize them this make the result look very good for this particular dataset but fail when encounter different dataset. This is because the model fail to generalize the learning so that is applicable on different dataset. This usually happen when there not enough data or train too many times on the same dataset. . Regularization . A technique to reduce overfitting by add in a value that act as penalty when the model start overfit. . Weight Decay (L2 regularization) . Add the sum of all weight squared to the loss function to affect the gradient calculation when add the weight to reduce it value. Visually this is like make the canyon bigger in gradient curve space . loss_with_wd = loss + wd * (parameters**2).sum() . using derivative equivalence to . parameters.grad += wd * 2 * parameters . in practive just pick a value and double it . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . Understanding the bias grap some lowest value in bias vector data for a user . movie_bias = learn.model.movie_bias.squeeze() idxs = movie_bias.argsort()[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . analysis . even when a user is very well matched to its latent factors(action,age of movie) they still generally don’t like it. Meaning even if it is their kind of movie, but they don’t like these movies . grap some high bias value . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . this show even persons don’t like the kind of movie they may still enjoy it. . visualize latent factors with more data using PCA . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = np.random.choice(len(top_movies), 50, replace=False) idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . using fastai collab feature . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd=0.1) learn.model movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . Measuring Distance of embbeding value . If there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, and the viewer also would have the same likeness with same similarity vectors. Which mean the distance between the two movies are closer together. . # find the movie similar to silence of the lambs movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . The clean slate problem When starting out there no data relationship (latent factor) for the user, movie,product… The solution is use common sense e.g. let user pick their movies from a list, assign a product base on environment, collect as much info about them as possible. Don’t let a few group of user,items,products have too much influent which skew the whole dataset. . Deep Learning for Collaborative Filtering To create DL collaborative filter do: . concastenate activation value result from lookup together | these matrices don’t have be the same size (not doing dot-product) | use fastai function get_emb_sz to get the recommend sizeof embedding matrix for the dataset | embs = get_emb_sz(dls) embs class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) model = CollabNN(*embs) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) # or using fastai version if nn=True it will auto call get_emb_sz learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) # a look inside fastai internal code @delegates(TabularModel) class EmbeddingNN(TabularModel): def __init__(self, emb_szs, layers, **kwargs): super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) . Note . In PyTorch: To read parameter value use nn.Parameter which also auto call requires_grad_ | In Python:(variable length argument in C++) **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs . **kwargs in an argument list means “insert all key/value pairs in the kwargs dict as named arguments here” | this make argments obscure from the tool since they all pack into a dictionary | fastai use @delegates docorator to auto unpack it so the tool can see | . | . Jagons . item: refers to product,movie,story,a link, topic… | latent factor: and unspecify info that affect the item | Embedding: Multiplying by a one-hot-encoded matrix thaat is the same as lookup or index into array to get an item | weight decay (L2) wd in fastai control the sum of square value | PCA principle component analysis use to reduce the size of the matrix to smaller size | probabilistic matrix factorization (PMF) | .",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/17/ML-collaborative-filter.html",
            "relUrl": "/ml-section/ml/2020/09/17/ML-collaborative-filter.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Regression",
            "content": ". Dealing with value that are continuous instead of discrete which the realm of classification. Image regression is area of application that use image as independent variable (x) and floating object(continous value) on the image as dependent variable. Which can be treat as another CNN on top of data block API. . Data set use: the Biwi Kinect Head Pose dataset . DataBlock . code for image regression . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) im = PILImage.create(img_files[0]) im.shape im.to_thumb(160) # extract the head center point: cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) get_ctr(img_files[0]) # get_y, since it is responsible for labeling each item. biwi = DataBlock( # do image regression with x,y blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, # make sure validation set contain 1 or more person not in train set splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) # check the data xb,yb = dls.one_batch() xb.shape,yb.shape yb[0] learn = cnn_learner(dls, resnet18, y_range=(-1,1)) # define the range of dependent data that is expected in the data set # since PyTorch and fastai treat left bottom is -1,top/right +1 def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) # use default loss function dls.loss_func learn.lr_find() # try this value lr = 1e-2 learn.fine_tune(3, lr) learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . MSELoss is used trying to predict something as close as possible . Note . random splitter is not applicable since same person appear in multiple images | each folder in the dataset contain image of one person | create a splitter that return true for a person a validation set for just that person | second block is a Pointblock to let fastai know that the label represent coordinates when do augmentation it would apply the same to the image folder | .",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/16/ML-regression.html",
            "relUrl": "/ml-section/ml/2020/09/16/ML-regression.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The hot ML Language",
            "content": ". The new kid in town . The current standard bearer . Python . In the field of research especially ML the king is Python. . The Advantage . Python is easy to learn language. It was design for non professional programmer. Well those that don’t writing code all day. It’s a dynamic language in the Language world. It doesn’t petster coding to know in advance the specific type of their values are. You simply express what you want to do and the language try to obey. This’s great for beginner or infrequence coder who still lot of time thinking about code work on other fields of their interest. . Another advantage of Python is it’s ecosystem especially in Data Science field. It’s tooling and libraries are top-notch that would serve any fledging data scientist’s needed. For example, for matrix computational Numpy1 is the undispute king, for data base handling Panda2 is the champ. Want to display what you’re working on Maplotlib is the goto library. . Ofcourse, Jupyter notebook is the de-facto tool to write your code in, display the result, and fix the error in your code. You can add document along side your code so people who reading it can understand what it’s about. It can even publish your research. It can be passed along to your colleague any one who interest to try out or work on. It’s a live document . Frameworks and libraries that support Python . PyTorch . Pytorch is the current researcher favorite framework. Since it support most of their needed in a simple form of Python. The code that utilize PyTorch look like a normal Python code. PyTorch seemlessly blend into user code this increase clarity and understanding when reading the code. . TensorFlow . Another popular framework especially in enterprise. It has pletora of features that a professional MLer would needed. Although previous version of TensorFlow is a little harder to work with since it imposed certain workflow, the current version has fully adopt Python int the form of Kera which make coding it in Python a joy. . FastAI The current state-of-the-Art library that is a layer on top PyTorch. FastAI provide best-practice, utilizing years of experience work in the field of ML in the form of simplication and default setting which let new comer and old-hat alike write simpler code and achieve state-of-the-art result. And if they prefer to roll-up the sleep and dig into the gut of PyTorch the door is wide open. . The Disadvantage . This simplicity comes with a cost it makes the language slow and unreliable in production since it may crash in production due to its dynamic nature which mean the data value that user input is not verify as correct before its perform computation on. Since the machine always require a specific type of data to do computation if these data type is incorrect it simply give up which we call crash. . Therefore, profession programmer prefers language that verify the type of data as soon as possible. This mean the coder must know the type of data and specify them before any computation can be performed on them. This type of language we call them static language. Since static language require data type be specified it can do verification during “compile-time” this is the time where the user code get translate into intermediate code, this is different from dynamic language which skip this step, before it later on get translate into machine language. . Julia . Julia on the hand is a static language. It also has feature of dynamic language like Python, well it try to mimic Python to lua Python code to it. It does the data type verification in the background which is called type-inference then inform coder to correct it before proceed. . One of Julia claim to fame is its power. It’s is fast blazing fast, it can do drag racing with C the king of speed. One intesting feature is the code written in Julia look suprisingly like the math formula its try to do computation on. It’s ecosystem is growing fast especially in the scientific computation field. It becoming the favorite goto language beside Python in research community. . It can be “full stack” platform meaning you can write code in it, you can also write library code in it, you can even write driver code in it. The last two are not possible in Python. Which is the major drawing back of Python that the library code must be written in other language normally in C++ this is because Python is too slow. . Library code must be fast since it is intend to be used by many situations and environment such as in production code. The same go with driver code which control the underlining hardware. . Frameworks and libraries that support Julia . Flux . Flux is Julia home-grown library for ML. It’s design and written in Julia to work with Julia from top to bottom. This include user code, library code, and driver code all is written in Julia which is amazing in its own right. It’s popularity growing by leaf-and-bound. . FastAI . This is the planing state under the label of “under heavy construction”. . Swift . Another new comer in ML field it was develop by Apple as the replacement for their aging Objective-C language which born 30 years agos. . Swift come to fame is its speed that rival C++, it’s being push by Google especially by the Tensorflow team as their next generation language. However, its ecosystem is barren which make it hard for any fledging data scientist to work in. . Framework and libraries that support Swift . TensorFlow . This is not a hugh surprise mind you. It currently has auto differential library which allows user to skip writing backward pass if he/she prefers. This is an important step forward. . FastAI . This is the planing state under the label of “under heavy construction”. . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . numpy doc &#8617; &#8617;2 . | Panda doc &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/13/hotmllang.html",
            "relUrl": "/ml-section/ml/2020/09/13/hotmllang.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Which is my favorite language?",
            "content": ". The would be choice of mine . In the beginning . I have been writting code for a while now that I don’t have enough fingers to count the years. I have used and continue to use these languages since most of the time it dictate by the task and the environment it in. I definite have a favorite one, on the other hand it’s not that important since I rarely have to dicide. . Task to do would lead you to choose the language for example. When I decide to write and Android App it’ been decide which language to choose it going to be either Java or Kotlin this is because Android framework and ecosystem support these two, further more, in the working environment your colleague and company which refine your choice. If they more experience or comfort level in one of these languages. . However, If write this app on my personal time I use Kotlin since is newer and have less wart and annoying work around than Java. It integrate quite seemlessly functional contstruct and exciting new feature such as Asynchronous programming and it’s the future of Android development. . As any shiny new object it does have sharp edge and dark corner one have to be aware of. . Kotlin Vs Java battle of the isle . Both Kotlin and Jave name after patch of land that surround by the sea. Maybe the author of the language feel a little notstalgic when move to the main land. . Java . Java the old king. It’s design to be simple language with simpler set of rule calls syntax. It an Object Oriented paradyme language. It wordship the Object as the deitry beall and endall every thing and only thing in it is Object. You start out writing class and class and class and some more class since only class will set you free at least in Java world view. It also have comarade in war call C# which at war in the early day of C# (C# is copy-cat of Java create by Microsoft as competitor to Java which create by Sun at the time). . The most annoying feature of this is that it force user to repeat the code over. . Kotlin . The new comer . Kotlin remove majority of the paint point of Java and add some nicety of modern tech such as Asynchonous programming, type inference and functional construct. It also make threading concurrency programming a little easier to swallow. You don’t have to jump through loop of fire, and burn mid-night oil to get you paralell code to work correctly. . ——————————————————————————————————— . . Footnotes . markdown .",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/13/favoriteLang.html",
            "relUrl": "/coding-section/coding/2020/09/13/favoriteLang.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Build Web site with fastpages",
            "content": ". Creating Web site using fastpages and Jekyll . As first I thought this going to take alot of work building a web site on your own. Why not pay someone to build one for you I thought. Being a hand-on kind of person I decide to take the plung building my own site using fastpages and Jekyll software to build the blog and hosted it on github site which provide for free hosting. . Learning something new is going to take time and patient. After the initial hiccup* it’s sailing smoothly now. The default site that fastpages created is a little barren for my taste so I have made some modification to fastpages initial site styling to fit my need. I have add a logo image, section link, feature image, image to represent articles. I’ve also hooked up most of social media links. And you can be sure I’ll have more modification as I dig deep inside its gut. I am also planning to improve the site gradually as circumstance permit. . Set up custom domain . I’ve also decided to set up my own domain by purchasing domain name from google. Then set it to point to my github page. To have your very own domain you first have to come up with the domain name you like to use. Then to purchase a domain name googling for google’s Domains site where you can buy and register the domain. Once you purchase the name just fill in the form on the site to register your domain. To use your newly purchase domain instead of github.io domain for your github page you can set it up. This is a two step processes: . At google DNS site to make it point to your github site here | At your github page site to use your new domain instead of [yourname].github.io domain. | To do step 1 you can following this article . here some tips: . The ip address shown there are the ip address of the server at github.io page you have to type them in. | The dig command must be run in the terminal console on your machine this is used to check if your setting at google domain DNS server is correct. | . To do step 2 Goto your repo settings tab at the bottom of the page where the GitHub Pages is setup . Put your domain name in the custom domain box then click save as shown here | Click enforce HTTPS to secure your site | Add a CNAME file at the root folder of your repo (make sure it’s not in any folder) | In the CNAME file add your domain or subdomain (sub domain is the one with www) e.g. www.mydomain.com | Edit the _config.yml file To set the line url: &quot;www.mydomain.com&quot; to match what in the CNAME file | Remove or comment out the line &quot;baseurl&quot; don’t need this anymore see my hiccup* | . | note: . if it won’t let you click on enforce HTTPS you might have to remove your domain then save the empty box then try again until it works . The Hiccup . I thought switching to use my own domain instead of github provide domain should be easy, well until it’s not. First whenever I made changed to the code I’d like see what happen to it so I always open the actions tab on my repos to see the CI in action, well, it throw up a build error when I remove the baseurl, dig into it there is an assert line that check if the baseurl in the config file is the same as in some default settings that cause the error . `assert config[&#39;baseurl&#39;] == settings[&#39;DEFAULT&#39;][&#39;baseurl&#39;], errmsg` . which led me to believe that comment out the baseurl is a bad idea, well I later findout it’s the opposite. It must be comment out inorder to get my site working. This’s delayed my progess afew days. as I posted here . The technical behind the site . Under the hood fastpages use Jekyll as the engine to drive it. Jekyll is a static website and blogs post generator. It handle HTML, CSS, markdown Liquid and more to finally generate my blog. What’s more, I only have to learn Liquid yet another language to my annoyance as if the world needs another language to make it works. And ofcourse, Jekeyll come with its own set of rule here are some of them: . ——————————————————————————— . .",
            "url": "https://www.vanhp.com/general/2020/09/12/BuildSite.html",
            "relUrl": "/general/2020/09/12/BuildSite.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "My first Notebook Blog",
            "content": "About . How does it work I&#39;m confuse as I can work on it on my local machine then sync it to github inorder to see the result? Can I see the result of my editing on my local machine directly without push it to github first? . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.vanhp.com/jupyter/2020/09/10/firstnotebook.html",
            "relUrl": "/jupyter/2020/09/10/firstnotebook.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Activation Functions",
            "content": "Activation functions provide non-linearity to the system and act like a switch to the system. . Activation functions are classified into single kink and double kinks on their graph. The shape of the kink e.g. smooth vs hard kink is depend on scaling value equivalence hard kink does not change scaling factor, while the smooth kink will change the the output value scaling factor . Hard kink vs Soft kink it matter of scaling equivalent . Hard kink it provide scaling of output . when the input multiply by a scalar value the output also have the effect with multiply by same scalar . Soft Kink it change the behavior of output . When the input value e.g. multiply by 100 the output look like it have hard kink . When divide the input with the same value the output look smooth . Activation that have single kink in the graph . which does not change signal only the scale . Rectilinear Unit (ReLU) . There are multiple variation of ReLU activation functions . ReLU(x) = (x)+ = max(0,x) has 0 at bottom and sharp kink for all positive value . | Leaky ReLU (x) = {x if x ≥ 0, negative slope of x otherwise . | . Allow bottom (horizontal asymptote) to have negative value so there still chance to have gradient . PReLU (x) {x, if x ≥ 0 , ax otherwise a = alpha is a leakable parameter . | . The value does not change other than the scale of value a, which different input units . Value alpha may be fixed or learned. It can have different values or a may be shared with different units. A value may not have to be learn but preset to ensure the gradient value is non zero . SoftPlus(x) = 1/beta * log(1 + exp(beta* x)) . | . A smooth approximation of ReLU can be used to constrain the output to always be positive where the beta get large it closer to ReLU . RReLU(x) = {x if x≥ 0, ax otherwise randomize leaky ReLU the same as leaky ReLU . | ELU(x) = max(0,x) + min(0,a * (exp(x) - 1) a softer version of ReLU by adding a small constant to smoothing it. And the bottom(horizontal asymptote) can be negative it allow for system to have average output value to be 0 which is useful for certain situation which have both positive and negative value which allow for faster convergent . | CELU(x) = max(0,x) + min(0,a * (exp(x/a) - 1) have the same effect as ELU . | GELU(x) = x * phi(x) where phi(x) is the cumulative distribution function for Gaussian Distribution . | ReLU6(x) = min(max(0,x),6) has two kink point on the graph at 0 and at saturation point at 6 . | . Activation that have 2 kink in the graphs . Double kink has built in scale where input signal is very different from output signal . Sigmoid(x) = sigma(x) = 1/1+exp(-x) has smooth curve for value between 0 and 1 problem with gradient go to 0 fairly quickly as it get closer to asymptote so the weight of the unit become too large which saturate the unit so the gradient drop fast the learning is not efficient. Sigmoid doesn’t work well when the value normalizes (with batch/group norm) just before it. Since normalization fix the scaling make the mean 0 and variance constance so the system doesn’t depend on the weight but the system now loss the ability to choose with kink to use in the 2 kink like sigmoid . | tanh(x) = tanh(x) = exp(x) - exp(-x)/exp(x) + exp(-x) has graph that identical to sigmoid except it center at 0 and go from -1 to 1 has the output to be closer to 0 mean but not 0 which allow the weight value after it to see both + and - which converge faster. Unfortunately, for deep stacking layers they are not learning efficiently and must be careful in normalizing the value. Which is why the single kink function tend to do better with deep layer network . | Softsign(x) = x/1+ | x | similar to sigmoid except it doesn’t get to the asymptote as fast and doesn’t get stuck toward asymptote as quickly as sigmoid | . | HardTanh(x) = {1 if x ≥ 1, -1 if x ≤ -1,x otherwise the linear region range is -1,1. It has a sharp kink both at negative value at bottom and positive value at top. Work fairly well for small network where the weight is not saturate too much . | . Use in sparse coding . rarely use in neural network . Threshold y = { x, if x threshold, value otherwise . | . rarely use since gradient can not propagate through it . Tanhshrink(x) = x - tanh(x) use mostly on sparse coding to shrinking the latent variable . | softShrinkate(x) = { x - lambda, if x &gt; lambda, x+ lambda, if x &lt; -lambda, 0 otherwise it has sharp kink at both bottom and top of graph . | HardShrink(x) = {x - lambda, if x &gt; lambda, x+ lambda, if x &lt; -lambda, 0 otherwise where value between -lambda and +lambda are set to 0 . | LogSigmoid(x) = log(1/1+exp(-x)) mostly use for loss function instead . | softmin(xi) = exp(-xi)/sum(exp(-xj)) when apply to input Tensor it rescaling the element of vector to the range of [0,1] which the sum to 1 similar to softmax with minus sign in front xi value . | Softmax(xi) = exp(xi)/sum(exp(xj)) same with softmin . | . Temperature is redundant with incoming weight (beta value) matter when the input size is fixed due to normalization. It uses to control how hard(the different between large and small value in the output vector) the distribution of the output value will be it the system to make hard decision even though it doesn’t learn as well as before. It used in case of mixture of expert system when multiple subnetwork output are linear combine with coefficients from the output of the softmax which may be controlled by another NN with soft mixture with low beta then gradually increase to infinity the will force NN to choose one and ignore the rest this may safe compute time (call annealing) . LogSoftmax(xi) = log(exp(xi)/sum(exp(xj))) use at the output as part of the loss function . | .",
            "url": "https://www.vanhp.com/2020/09/10/Activation-Functions.html",
            "relUrl": "/2020/09/10/Activation-Functions.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Loss Functions",
            "content": "MSELoss() . Measure the mean square error (L2 norm) between each element in input x and target y . where N is the batch size . L1Loss() . Basically the absolute value of the different between desired output and actual output . where N is the batch size . Use for robust regression where small error to count more and large not as much to reduce noise in data e.g. to reduce the influent of a few outlier in dataset but L1 loss is not differentiable at the bottom when gradient get closer to the bottom normally is done with soft shrink, some fix of this problem e.g. smooth L1 loss to make bottom smooth like L2 with Hubert loss to protect against outliers . SmoothL1Loss() . otherwise . Note the average value make image blur . NLLLoss() . The negative log likelihood loss useful for reduce discrepancy among dataset . . Try to pick the correct value in the output vector as large as possible . If there is a negative in front it can be interpret x as energy . Useful for widely different frequency (size) data category this by assign larger weight for the category that have small dataset and smaller weight to category that have large among of data, but better solution is to increase the frequency of minimal category instead by reuse the same set of smaller dataset over until the larger dataset exhaust to equalize them out this method exploit SGD better.Never leave any data unused . CrossEntropyLoss() . Merging the log softmax(do log after the softmax) and NLLLoss is for numerical reasons . Since the log softmax step might have gradient value infinite in the middle of the step (because some values in softmax might be very small close to 0 value take log of this value result in negative infinity ) and gradient of this value also close to infinity Alway do the softmax and log in the same time to avoid this problem to get a more stable numerical value. By combining these 2 it makes the correct value largest as possible and suppresses all the others scores as small as possible due to normalization. The loss take and X factor and a category a desired category a class then compute the negative log of softmax applied to vector of scores the numerator is the X of the index of the correct class that is the loss the negative log of exponential score of the correct class divided by the sum of the exponential of all the scores. The X can be thought of as negative energy. The negative score of the correct class and to make this value small make the score large and all the sum of exponential of other value small this make the edge small . Cross entropy is the cross of distribution of the softmax vector and target value vector (one hot)that have one value that close to 1 . AdaptiveLogSoftmaxWithLoss() . Extension to log softmax useful with the dataset that have large categories e.g. in NLP by use some like ignore the small categories and focus on the large one . BCELoss() . . . KLDivLoss() . where N span all dimension of input and L have the same shape as input if reduction is not none . . May have numerical issue . BCEWithLogitsLoss() . . where N span all dimension of input and L have the same shape as input if reduction is not none . . It take value and pass to sigmoid and make sure the value is between 0 and 1 . MarginRankingLoss() . . Make one of the input larger than the other by value of the margin, y is the value that control x1 and x2 . TripletMarginLoss() . Use to measuring a relative similarity between samples. It compose of anchor,p positive,n negative . . Try to get the distance (value ) good pair smaller than the distance of the bad pair . SoftMarginLoss() . Optimize a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1) . . Which the positive value closer and negative value further . MultiLabelSoftMarginLoss() . Optimize a multi-label one-versus-all loss based on max-energy, between input x and target y of size (N,C). for each sample in the minibatch . %20%2B%20(1-y%5Bi%5D)%20*log(%20%5Cfrac%7B%20exp(-x%5Bi%5D)%7D%7B(1%2Bexp(-x%5Bi%5D))%7D%20)#0) . Where . Allow multiple correct output (want multiple categories can have high score and all lower score get suppressed) which for the desire category want the value to be higher and the undesired value suppress . MultiMarginLoss() . %5Ep%7D%7D%7B%7B%20x.size(0)%7D%7D#0) . Where . HingeEmbeddingLoss() . Measure the loss given an input tensor x and a label tensor y (containing 1 or -1). This is usually used for measuring whether two input are similar or dissimilar, e.g. using the L1 pairwise distance as x, and is typically used for learning nonlinear embeddings or semi-supervised learning . The loss function for n-th sample in the mini-batch is . . And the total loss function is . . Where . CosinEmbeddingLoss() . Measure the loss given input tensors x1,x2 and a Tensor label y with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning . The loss function for each sample is: . 1 - cos(x_1,x_2), &amp; quad text{if} y = 1 . max(0,cos(x_1,x_2) - margin) &amp; quad text{if} y = -1 . f(n) = . begin{cases} . n/2 &amp; quad text{if } n text{ is even} . -(n+1)/2 &amp; quad text{if } n text{ is odd} . end{cases} . For positive case make the two vector align a much as possible . For negative try to make the cos value orthogonal to each other . CTCLoss() (connectionist Temporal Classification loss) . Calculate loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many to one”, which limits the length of the target sequence such that it must be ≤ the input length . Use in waveform classification where the speed may play a role e.g. a word was pronoun slowly may generate multiple similar waveform/fast may generate less waveform should be map to the same word regardless of speed of speaker . Architecture and Loss Function . General form of energy base model . Family of energy function . . Training set . . Loss functional/Loss function . Functional is function of a function . . Measure the quality of an energy function on training set . Training . . Form of the loss function . Invariant under permutations and repetition of the samples . . Where L per sample loss . Y desired answer . E energy surface for a given Xi as Y varies . R regularizer . Energy Loss function example . Energy loss . . Simply pushes down on the energy o the correct answer . Negative Log-Likelihood Loss . . Have a behavior of pushing down the correct answer push up on energy of incorrect value in proportion to their probability . . A simpler Loss function: Perceptron Loss . . Push down the energy of right answer . | Push up on energy of wrong answer . | Always, positive . | Zero when answer is correct . | No margin does not prevent energy surface from being almost flat . | Work well in practice, particularly if the energy parameterization does not allow flat surfaces . | Call “discriminative Viterbi training” in speech and hand written literature . | Not work well with non linear system . | . Generalized Margin Loss . Most offending Incorrect answer: discrete case . Definition 1 Let Y be a discrete variable. Then for a training sample . The most offending incorrect answer is the answer that has the lowest energy among all the answer that are incorrect: . . Most offending Incorrect answer: discrete continuous case . Definition 2 Let Y be a continuous variable. Then for a training sample the most offending incorrect answer is the answer that ha the lowest energy among all answer that are at least in away from the correct answer; . . Hinge Loss . . With the linearly-parameterized binary classifier architecture, we get linear SVM . . Log loss a soft hinge loss . With the linearly-parameterized binary classifier architecture, we get linear Logistic Regression . Square-Square Loss . [](https://www.codecogs.com/eqnedit.php?latex=%20L_%7Bsq-sq%7D(W%2CY%5Ei%2CX%5Ei)%20%3D%20E(W%2CY%5Ei%2CX%5Ei)%5E2%20%2B%20(%20max(0%2Cm%20%2B%20E(W%2CY%5Ei%2CX%5Ei)%20)%5E2%20#0) . Appropriate for positive energy function . A more general form of Hinge type Loss . . H is Hinge energy . E another energy . C margin .",
            "url": "https://www.vanhp.com/2020/09/09/Loss-Functions.html",
            "relUrl": "/2020/09/09/Loss-Functions.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The math and code don't mix?",
            "content": ". Take the red pill! . Anyone who take the red pill and jump into ML pool will soon swim amoung the stream of equations, formula. there no avoiding it. Since you’re already take the pill you might as well get use to it. If your day of school are long behind you. The best advice is go to Khan academy site to refresh your memory on them. Two of the main math subjects you need to have firm grasp of are Linear algebra, and differentiation. . Linear algebra . Matrix and Tensor Matrix operations are essential. Matrix play a major roll in Neural network operation give you some insight into how neural network works. And allow you to manupulate them to work for you. . linear function L = mx + b . def linear = weight@input + bias . gradient . def grad(xb,yb,model): pred = model(xb) loss = mnist_loss(pred,yb) loss.backward() . Calculus . Do you still remember the chain-rule? If this didn’t ring a bell, it’s time to bush-up on it. It’s at the heart of how the Neural network learn in the process call SGD and it is the logo of this site, take a little peek at the top of the page. The great news is that you don’t have to do the math calculation yourself the Framework whichever one you choose will do this for you, your job is to understand them inorder to take full advantage of the tool you use. . logarithm log(a*b) = log(a)+log(b) | . Statistic . Probability, Median, average/mean, variance, standard deviation … use for analysis of data and the result. . Sigmoid equation in math: . $ sigmoid(x) = frac{1}{1 + ln^{-x}}$ . Sigmoid function definition . in Python . def sigmoid(x): return 1/(1 + torch.exp(x)) . Jagons: . Python broadcasting vectorization elementwise operation einsum *kwarg *arg decorator partial (partial application) . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . lambda . . ———————————————————————————- . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/07/ML-mathcode.html",
            "relUrl": "/ml-section/ml/2020/09/07/ML-mathcode.html",
            "date": " • Sep 7, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "The Polyglot Coder Journey 1",
            "content": ". Welcome to my Polyglot Coder view . The rear view mirror . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/05/coding-post1.html",
            "relUrl": "/coding-section/coding/2020/09/05/coding-post1.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "The OOP vs Functional fight!",
            "content": ". Example Markdown Post . Basic setup . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/02/coding-post3.html",
            "relUrl": "/coding-section/coding/2020/09/02/coding-post3.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "How many languages do you need to succeed?",
            "content": ". Example Markdown Post . Basic setup . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/08/18/coding-post2.html",
            "relUrl": "/coding-section/coding/2020/08/18/coding-post2.html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "The Polyglot Coder General rant!",
            "content": "Welcome to my random rant! anything that don’t fell into either ML or programming . The rear view mirror . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/general/2020/08/16/general-post1.html",
            "relUrl": "/general/2020/08/16/general-post1.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Training Model",
            "content": ". Basic step in training model:&lt;/span&gt; . Initialize the weights | For each input image use these weights to predict whether the outcome e.g. (0…9) | Based on the prediction calculate how good the predict is what’s the loss (error) | Calculate the gradient derive from each weight to see how changing the weight affect the loss | Update the weights base on the calculation | Repeat step 2-5 for all images | Keep doing these until outcome is satisfy or quit | All images . must be in the same dimension(size)can collate into tensors to be passed to the GPU | will be converted into matrice | . Initialization . must be initialized with random value | . Matrix multiplication is usually in dot-product e.g. m dot x, or m@x in Python . gradient is calculated using the Calculus chain-rule: . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . def simpleNN(xb): l1 = xb@w1 + b1 l1 = l1.max(tensor(0.0)) make neg value into 0 ignore + (relu) l1 = l1@w2 + b2 return l1 . this simple little has some interesting features . It work like universal approximation theorem whhich can be used to approx any function no matter how complicate it is | It call Neural Network | It build via composition which take the result of one function then pass it into a new function on and on PyTorch has name this as sequential | Linear = x@w + bias . non linear . Non linearity is needed to turn the network into more useful function, since linear + linear -&gt; another linear function . reLu: max(tensor(0,0)) . sigmoid: output binary categories between 0 and 1 torch.where can be used to pick from one of these (output must contain only 2 categories) . def sigmoid(x): return 1/(1 + torch.exp(x)) . softmax: output more than 2 categories which have value all add up to 1 has a side effect of exagerate the large value and diminist smaller value ‘cause of exponential effect . def softmax(x): return exp(x)/exp(x).sum(dim=1,keepdim=true) . It works even when our dependent variable has more than two categories. It results in faster and more reliable training. In order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like. . Loglikelyhood is and indexer which use to pick a value from the list of value output from softmax PyTorch provide nll_loss assumes that you already took the log of the softmax, so it doesn’t actually do the logarithm must use log_softmax (faster and more accurate to take a log at this stage) before nll_loss Negativeloglikelyhood NLL this function simply apply minus to the value to remove negative value . Cross-Entropy Loss nn.CrossEntropyLoss (does log_softmax and then nll_loss) Cross-entropy loss is a loss function that is similar to the one we used in the previous chapter, but (as we’ll see) has two benefits: When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss. . DataLoader will iterate over the collection input and return tuple of data in collection . Helpers function for code simplication: DataLoaders learner . create a simple neural network by hand . Note: Pytorch Dataset return a tuple when indexing into it . Data Handling . Data augmentation: . Process to increase dataset where the data is hard to collect and help improve model performance by manipulate the images in a natural ways. . Data cleaning . A process to remove incorrect,missing,unnecessary info in the data set collect and make them usable by the model . tip: fastai can help in data cleaning by let the model work on the data then fix the error where the model has problem with . DataBlock . use summary to debug . Presizing: . A special technique use by fastai to improve image manipulation with high efficiency and minimal degradation It uses strategies: . Resize images to relatively “large” dimensions—that is, dimensions significantly larger than the target training dimensions to have spare margin for more transformation with no empty zone by resizing to a square using a large random crop size that span the smaller width or height | . | Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times. | Training the model: . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Visualizing Data and result . To understand and diagnose the output from the model various visualization are available . Confusion matrix show grid of predict vs actual value most_confused for model get the most often wrong answer (interp.most_confused(min_val=5)) to show model confusion the diagonal line is correct a 1 is any place else show mis-prediction | . | . How the model learn . from Zeiler and Fergus paper which show image of how the model learn . The earlier layer learn about structure e.g. line,circle, edge, area | Each layer after that learn more more semantic by using info from earlier layer to form meaning | the lastest layer are closer to actual object | . Improve model performance . Learning rate . the right learning rate is important the help improve model performance . Low learning rate may increase training epoch or overfitting | Large learning rate may overshoot the minimum loss may decrease performance | Use the learning rate finder (Learner.lr_find) tool will do these step: start with a very, very small learning rate then double the size for each mini-batch | Check the loss if it get worse stop then back-up to the last mini loss that the best one Then start with value smaller than this value pick either | One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10 `1e-6 to 1e-3) | The last point where the loss was clearly decreasing | . | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . return the minimum point and steep point in log scale steep point show the model making great progress (learning) minimum show the model stop making progress (not learning) . Transfer Learning . the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task . By remove the head which consist of the last layer that do the predicion (softMax layer) which specific to a task | replace the head with new one (with random weight) that suitable to new task tell the optimizer to only update the weights in those randomly added final layers | Don’t change the weights in the rest of the neural network by freeze them | . | Keep the body | . Fastai Learner.fine_tune methods . Trains the randomly added layers for one epoch, with all other layers frozen | Unfreezes (learner.unfreeze) all of the layers, and trains them all for the number of epochs requested fine_tune function does: | freeze (body) | fit_one_cycle | base_lr/2 | unfreeze (body) | fit_one_cycle (train all parts together) note after fine_tune you may need to pick a new learning rate to help improve the performance since all parts may not have trained enough together discriminative learning rate is more appropriate at this point | . Fastai 1 cycle policy . A fastai technique for training the model It start with low learning rate then gradually increase the learning rate until it reach the max value specify by the user then it stop for the first 1/3 of the batch, for the rest of the batch it gradually decrease the learning rate Learner.fit_one_cycle . note: The recommend approach is to retrain with smaller epoch with the model overfit usally the validation loss start getting worse than train error . Discriminative Learning Rates . Since the body has already been trained it doesn’t need the same learning rate the head portion Discriminative Learning Rates allow for training the head and body with different learning rate . the body is trained with lower learning rate | the head is trained with higher learning rate | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . slice(learning_rate for body,learning_rate for head) . Training period . if you find that you have overfit, what you should actually do is . retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found. | use a deeper architecture (more layers) it takes more resources memory,CPU/GPU cycles | take more time (use fp16 to speed up) | . | . Choosing the right architect . Many achitect comes with various level of layers e.g. ResNet 18,34,50,101 pretrained on ImageNet Pick the right one is a trial-error process . Pandas and DataFrames . Note in Numpy,Panda,PyTorch (trailing : is optional) e.g. df.iloc[0,:] -&gt; df.iloc[0] . Jargons: . parameter are weight and bias | SGD : stochastic gradient decent calculate the gradient using a small set of data in practice is using a loop over a mini-batch to calculate the GD | GD: gradient descent calculate using the whole dataset all at once | broadcasting a matrix calculation to speed up without using loop by multiply the same scalar to all the value in the matrix then add them up | mini-batch: a small set of data with label | RelU: function that convert - value to 0 leave + value alone | forward pass: the process that calculate the prediction value | loss function: function that calculate the error the different between actual value - predict value | backward pass: the process that calculate the adjustment value use to update the parameter to reduce the loss | learning rate: size of SGD value use to update the parameters for every loop | activation value: these number are calculated from output of linear and nonlinear | parameter value: adjustable value use to improve the performance | Tensor: multidimention arrays with regular shape rank 0: scalar | rank 1: vector | rank 2: marix | rank 3: 3D tensor | rank 4: 4D tensor | . | Neural network: comprise with layers | layer: groups of neural (node) arrange as linear and non-linear output of a layer is pass-on to the layer next to it | fit: is same training | Dataset: a colllection of data that returns a tuple of x,y variable of a single item in the collection | DataLoader: an iterator that return stream of mini-batch. Where each minibatch is comprise with batch of x,y variables | . ————————————————————————————————— . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html",
            "relUrl": "/ml-section/ml/2020/05/12/ML-train.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Multi-label Classification",
            "content": ". Refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object or more or no object present. This simple solution is not at all widely understood or appreciated! . Pandas and DataFrames . Pandas a Python library for database, it’s fast and flexible library but has non-intuitive interface. It’s used to stored manage analize structure data type. It’s also useful in ML to handle tabular and timeseries data type. . Dataframe is the main class that represent data in table of row and columns. It can import data from CSV file, from directories, and many others sources. And Jupyter is happily work with it. . import Pandas as pd df = pd.read_csv(path/&#39;train.csv&#39;) df.head() tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) . DataBlock . A fastai versatile,flexible object that manage data. it has facility to handle Datasets,dataframe. It smart enough to understand data set that is working with. It’s also auto split data into train 80% validate 20% . # empty datablock dblock = DataBlock() dsets = dblock.datasets(df) len(dsets.train),len(dsets.valid) x,y = dsets.train[0] #take lambda/ or function as argument to retrieve filename and label dblock2 = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets2 = dblock.datasets(df) dsets2.train[0] # take default name dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . Binary cross entropy Loss . One of the problem with multiple label is finding appropiate loss function that can handle the possibility of multi-label and no label may present in data. The regular cross entropy loss is only able to deal with single label. Finding loss function that suitable for case where there more than one label or no label present is needed. . Learner object . The Learner object inherit from nn.Module comprise with: . The model | DataLoaders object | Optimizer | Loss function | It can receive x as a mini-batch and return activation value . learn = cnn_learner(dls, resnet18) activ = learn.model(x) activ.shape def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() # using PyTorch version # by default fastai will use this version loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . due to one-hot-encoded dependent variable because there may be more than object the cross_entropy is not appropriate In the case where more than one object is possible, or no object present may want the sum to be &lt; 1 this excluse softmax and nll_loss . softmax: limitation: . all value must sum to 1 | the largest value get exagerate nll_loss limitation: | it return only one value correspond to the single label we need to return more than 1 label | . Comparable PyTorch versions that work with one-hot encoding: . F.binary_cross_entropy (no sigmoid) | nn.BCELoss (nosigmoid) | F.binary_cross_entropy_with_logits | nn.BCEWithLogitsLoss Single label version: F.nll_loss nn.NLLLoss (no softmax) F.cross_entropy nn.CrossEntropyLoss | . Measurement metric . Since there maybe more than one or object present need a new metric for accuracy . # value &gt; thresh is consider 1, else consider 0, default threshold = 0.5 def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() # train the model learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . High threshold select the high confident of model Low threshold select low confident of model . find the best threshold by trying a few levels and seeing what works best . # this apply sigmoid by default preds,targs = learn.get_preds() # no sigmoid accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) # plot it xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs) . Note . zip(b[0],b[1]) has short cut of zip(*b) | python can’t save (serialization lambda) lambda use function instead | it’s OK to use validation set to pick threshold it should not overfit | . Jagons . x: is independent variable | y: is dependent variable | Dataset: A collection that can be index into it and returns a tuple of your independent and dependent variable for a single item | DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | one-hot-encoder: a vector that has all 0 but one 1 to represent the interested item to pick-out from other item | . Fastai library . Datasets: An object that contains a training Dataset and a validation Dataset part of fastai library | DataLoaders: An object that contains a training DataLoader and a validation DataLoader part of fastai library | . . —————————————————————————————————— .",
            "url": "https://www.vanhp.com/ml-section/ml/2020/03/16/ML-info.html",
            "relUrl": "/ml-section/ml/2020/03/16/ML-info.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.vanhp.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "The first step in long journey with ML",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/02/12/ML-firststep.html",
            "relUrl": "/ml-section/ml/2020/02/12/ML-firststep.html",
            "date": " • Feb 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "under heavy construction. Please be patient! . I’ve been programming for over decades now. My Journey start from C to C++ then on to Java, C#, F#, python, JavaScript, Julia, Swift, Scala, Kotlin. I’m using them on DOS, Windows 95/98/NT/8,10, Sun solaris, Linux variance such as Slackware,Gentoo,Redhat,fedora,ubuntu and Mac. . Programming Languages and API: . • C# 6 years ● .NET 8 years ● C/C++ 7 years ● Object Oriented ● Java 5 years ● Python 3 years ● SDLC • JavaScript 3 years ● bash 4 years ● Kotlin 2 years ● Octave/Matlab 1 year ● Swift 1 year • Networking TCP/IP/UDP/HTTP/FTP ● WCF ● Multi-thread ● RDBMS/ORM ● Windows GUI . Platform and Frameworks: . - Operating Systems: ▪ Linux: Ubuntu, Red Hat Enterprise ▪ Windows: 98/NT/2000/XP/Vista/7/8/10 ▪ Mac • Framework/libraries: Android, dotNet Framework 2/3/3.5/4/4.5, WCF, MFC,WPF, win32 API • Database/ORM: MS SQL 2008/12, Oracle 10g, Entity Framework 4.1/6, SQLite • AI/Machine Learning: PyTorch, TensorFlow, FastAI, Numpy, Panda . Tools and Testing: . • Source control: Git, TFS, CVS/SVN/Rational ClearCase, itrack, JIRA/Agile/Scrum • Testing: NUnit/MSTest/JUnit . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.vanhp.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
  

  
  

  

  
      ,"page5": {
          "title": "Resources",
          "content": ". Here are some of the useful sites that I have come across: . AI/ML field . Jeremy Howard He is one of the expert in the AI/ML field. The greating about it is that he give all of this execllent material for free. If you want to learn about AI/ML here is the place to start. And his fastai library which current reach version 2 is the state-of-art in the field that is rapidly moving. And ofcourse, fastpages the library this site is used the testament to his brilliant. . Andrew Ng The founder of Coursera and the pre-eminent current AI expert. You can some of his material on Youtube and on Coursera. . Yann Lecun The Turing Award winner and some calls him the father of modern AI/ML. You can find his lecture of Youtube especially on current hot topic in self-supervise learning. . fastai github . visualdtree . Computational Linear Algebra for Coders. . selfsupervise . matrix-explained . blog . linear algebra . paper1 . paper2 . fastai-julia . androiod-ML . setupML-project . TF-mnist . f#-ML . data augment NLP . . Software development field . Douglas Schmidt is one of prolific professor of computer science. He has ton of contents about sofware and especially in C++ and Java, Android. This is where I come when I feel a little rusty on C++ and Java. His material are up up to date and top-notch. . . Books . Data science and Panda,Numpy Python for Data Analysis . . Music . Coding is tough job you evitably run into hard problem to can’t solve in timely maner. You’ll need some music to soothing your pain and smooth out the day. . Some of my favorite tunes: . Relax music for focus during coding session relax music . Thai music . Nanglen Old timer that make music for life as much as for ears ดอกไม้ในที่ลับตา เพราะเธอ . pop music . Num Kala เธอเป็นแฟนฉันแล้ว . Country music . Takatan รอคนถัดไป รอคนถัดไป ขอจองไว้ในใจ .",
          "url": "https://www.vanhp.com/Resource-section/",
          "relUrl": "/Resource-section/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "ML",
          "content": "Welcome to my Machine Learning blog Home page .",
          "url": "https://www.vanhp.com/ML-section/",
          "relUrl": "/ML-section/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Programming",
          "content": "Welcome to my Programming blog Home page .",
          "url": "https://www.vanhp.com/Coding-section/",
          "relUrl": "/Coding-section/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page16": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.vanhp.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}