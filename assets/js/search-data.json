{
  
    
        "post0": {
            "title": "Working with Tabular Data",
            "content": ". Tabular Modeling Deep Dive . The objective is to predict the value in one column based on the values in the other columns. There are other technique in ML field such as random forest that also provide good or better result as DL which suite certain type of problem. . There are plethora of methods in ML many of them only applicable to certain situation. Most of the technique in machine learning can be boil-down to just two proven methods: . For structure data types: . Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies). Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles than DL. | There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation? | . | For unstructure data types: . Multilayered neural networks learned with SGD (Deep Learning) (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) DL also work well with structure data type may be slower to train and hard to intepret result than ensemble decision trees which also don’t need GPU, less parameter tuning,more mature ecosystem. . | There are some high-cardinality categorical variables that are very important (“cardinality” refers to the number of discrete levels representing categories, so a high-cardinality categorical variable is something like a zip code, which can take on thousands of possible levels). | . | There are some columns that contain data that would be best understood with a neural network, such as plain text data. | . | Ensemble Decision Trees . Decision tree don’t require matrix multiplication or derivative. So PyTorch is of no help. Scikit-learn is better suit for this task. . Handling Different data types from tables Dataset: the Blue Book for Bulldozers Kaggle competition . Note . Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables… [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit… As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering. | In practice try both methods to see which one better suite for the task | Deep learning generally work well with highly complex data type | . key insign: . an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer | the embedding transforms the categorical variables into inputs that are both continuous and meaningful. | Jagons: . Continuous variables are numerical data, such as “age,” that can be directly fed to the model, | Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning | . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/18/ML-tabular.html",
            "relUrl": "/ml-section/ml/2020/09/18/ML-tabular.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Collaborative Filter deep dive",
            "content": ". A general class of system known as Recommend system that recomend a product to a customer use in Amazon, a movie or video to viewer which is used in Netflix, what’s story to show in Facebook,Tweeter… Which is base the user own history, in some other users that may have similar preference. For example, Netflix recommend a movie to you base on other people who watch the same movie. Or Amazon recommend a product base on other who bought or view the same product. . It’s base on the idea of latent factors the unwritten or unspecify context that underline the item. . !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * rom fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() last_skywalker = np.array([0.98,0.9,-0.9]) user1 = np.array([0.9,0.8,-0.6]) (user1*last_skywalker).sum() casablanca = np.array([-0.99,-0.3,0.8]) (user1*casablanca).sum() . Learning the Latent Factors . DataLoaders . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() ratings = ratings.merge(movies) ratings.head() #build dataloaders #By default, it takes the first column for the user, the second column for the item dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() dls.classes n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . To calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. To do lookup in matrix form which model can calculate is to represent lookup with vector of one-hot encoding . one_hot_3 = one_hot(3, n_users).float() user_factors.t() @ one_hot_3 user_factors[3] . Embedding . A technique of look up item by using matrix multiplication with one-hot encode vector. PyTorch has a special layer that do this task in a fast and efficient way. . Collaborative by hand from scratch . class DotProduct(Module): # constructor # use slightly higher range since sigmoid max of 5 to get 5 need to over def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) # account for user bias self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) # account for bias in data self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range # callback from Pytorch def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users + movies).sum(dim=1,keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) #force those predictions to be between 0 and 5 with sigmoid_range return sigmoid_range(res, *self.y_range) model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . Overfitting . A phenomena that happen when the model start to recgonize the dataset and try to memorize them this make the result look very good for this particular dataset but fail when encounter different dataset. This is because the model fail to generalize the learning so that is applicable on different dataset. This usually happen when there not enough data or train too many times on the same dataset. . Regularization . A technique to reduce overfitting by add in a value that act as penalty when the model start overfit. . Weight Decay (L2 regularization) . Add the sum of all weight squared to the loss function to affect the gradient calculation when add the weight to reduce it value. Visually this is like make the canyon bigger in gradient curve space . loss_with_wd = loss + wd * (parameters**2).sum() . using derivative equivalence to . parameters.grad += wd * 2 * parameters . in practive just pick a value and double it . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . Understanding the bias grap some lowest value in bias vector data for a user . movie_bias = learn.model.movie_bias.squeeze() idxs = movie_bias.argsort()[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . analysis . even when a user is very well matched to its latent factors(action,age of movie) they still generally don’t like it. Meaning even if it is their kind of movie, but they don’t like these movies . grap some high bias value . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . this show even persons don’t like the kind of movie they may still enjoy it. . visualize latent factors with more data using PCA . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = np.random.choice(len(top_movies), 50, replace=False) idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . using fastai collab feature . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) learn.fit_one_cycle(5, 5e-3, wd=0.1) learn.model movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . Measuring Distance of embbeding value . If there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, and the viewer also would have the same likeness with same similarity vectors. Which mean the distance between the two movies are closer together. . # find the movie similar to silence of the lambs movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . The clean slate problem When starting out there no data relationship (latent factor) for the user, movie,product… The solution is use common sense e.g. let user pick their movies from a list, assign a product base on environment, collect as much info about them as possible. Don’t let a few group of user,items,products have too much influent which skew the whole dataset. . Deep Learning for Collaborative Filtering To create DL collaborative filter do: . concastenate activation value result from lookup together | these matrices don’t have be the same size (not doing dot-product) | use fastai function get_emb_sz to get the recommend sizeof embedding matrix for the dataset | embs = get_emb_sz(dls) embs class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) model = CollabNN(*embs) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) # or using fastai version if nn=True it will auto call get_emb_sz learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) # a look inside fastai internal code @delegates(TabularModel) class EmbeddingNN(TabularModel): def __init__(self, emb_szs, layers, **kwargs): super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) . Note . In PyTorch: To read parameter value use nn.Parameter which also auto call requires_grad_ | In Python:(variable length argument in C++) **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs . **kwargs in an argument list means “insert all key/value pairs in the kwargs dict as named arguments here” | this make argments obscure from the tool since they all pack into a dictionary | fastai use @delegates docorator to auto unpack it so the tool can see | . | . Jagons . item: refers to product,movie,story,a link, topic… | latent factor: and unspecify info that affect the item | Embedding: Multiplying by a one-hot-encoded matrix thaat is the same as lookup or index into array to get an item | weight decay (L2) wd in fastai control the sum of square value | PCA principle component analysis use to reduce the size of the matrix to smaller size | probabilistic matrix factorization (PMF) | .",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/17/ML-collaborative-filter.html",
            "relUrl": "/ml-section/ml/2020/09/17/ML-collaborative-filter.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Regression",
            "content": ". Dealing with value that are continuous instead of discrete which the realm of classification. Image regression is area of application that use image as independent variable (x) and floating object(continous value) on the image as dependent variable. Which can be treat as another CNN on top of data block API. . Data set use: the Biwi Kinect Head Pose dataset . DataBlock . code for image regression . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) im = PILImage.create(img_files[0]) im.shape im.to_thumb(160) # extract the head center point: cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) get_ctr(img_files[0]) # get_y, since it is responsible for labeling each item. biwi = DataBlock( # do image regression with x,y blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, # make sure validation set contain 1 or more person not in train set splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) # check the data xb,yb = dls.one_batch() xb.shape,yb.shape yb[0] learn = cnn_learner(dls, resnet18, y_range=(-1,1)) # define the range of dependent data that is expected in the data set # since PyTorch and fastai treat left bottom is -1,top/right +1 def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) # use default loss function dls.loss_func learn.lr_find() # try this value lr = 1e-2 learn.fine_tune(3, lr) learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . MSELoss is used trying to predict something as close as possible . Note . random splitter is not applicable since same person appear in multiple images | each folder in the dataset contain image of one person | create a splitter that return true for a person a validation set for just that person | second block is a Pointblock to let fastai know that the label represent coordinates when do augmentation it would apply the same to the image folder | .",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/16/ML-regression.html",
            "relUrl": "/ml-section/ml/2020/09/16/ML-regression.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The hot ML Language",
            "content": ". The new kid in town . The current standard bearer . Python . In the field of research especially ML the king is Python. . The Advantage . Python is easy to learn language. It was design for non professional programmer. Well those that don’t writing code all day. It’s a dynamic language in the Language world. It doesn’t petster coding to know in advance the specific type of their values are. You simply express what you want to do and the language try to obey. This’s great for beginner or infrequence coder who still lot of time thinking about code work on other fields of their interest. . Another advantage of Python is it’s ecosystem especially in Data Science field. It’s tooling and libraries are top-notch that would serve any fledging data scientist’s needed. For example, for matrix computational Numpy1 is the undispute king, for data base handling Panda2 is the champ. Want to display what you’re working on Maplotlib is the goto library. . Ofcourse, Jupyter notebook is the de-facto tool to write your code in, display the result, and fix the error in your code. You can add document along side your code so people who reading it can understand what it’s about. It can even publish your research. It can be passed along to your colleague any one who interest to try out or work on. It’s a live document . Frameworks and libraries that support Python . PyTorch . Pytorch is the current researcher favorite framework. Since it support most of their needed in a simple form of Python. The code that utilize PyTorch look like a normal Python code. PyTorch seemlessly blend into user code this increase clarity and understanding when reading the code. . TensorFlow . Another popular framework especially in enterprise. It has pletora of features that a professional MLer would needed. Although previous version of TensorFlow is a little harder to work with since it imposed certain workflow, the current version has fully adopt Python int the form of Kera which make coding it in Python a joy. . FastAI The current state-of-the-Art library that is a layer on top PyTorch. FastAI provide best-practice, utilizing years of experience work in the field of ML in the form of simplication and default setting which let new comer and old-hat alike write simpler code and achieve state-of-the-art result. And if they prefer to roll-up the sleep and dig into the gut of PyTorch the door is wide open. . The Disadvantage . This simplicity comes with a cost it makes the language slow and unreliable in production since it may crash in production due to its dynamic nature which mean the data value that user input is not verify as correct before its perform computation on. Since the machine always require a specific type of data to do computation if these data type is incorrect it simply give up which we call crash. . Therefore, profession programmer prefers language that verify the type of data as soon as possible. This mean the coder must know the type of data and specify them before any computation can be performed on them. This type of language we call them static language. Since static language require data type be specified it can do verification during “compile-time” this is the time where the user code get translate into intermediate code, this is different from dynamic language which skip this step, before it later on get translate into machine language. . Julia . Julia on the hand is a static language. It also has feature of dynamic language like Python, well it try to mimic Python to lua Python code to it. It does the data type verification in the background which is called type-inference then inform coder to correct it before proceed. . One of Julia claim to fame is its power. It’s is fast blazing fast, it can do drag racing with C the king of speed. One intesting feature is the code written in Julia look suprisingly like the math formula its try to do computation on. It’s ecosystem is growing fast especially in the scientific computation field. It becoming the favorite goto language beside Python in research community. . It can be “full stack” platform meaning you can write code in it, you can also write library code in it, you can even write driver code in it. The last two are not possible in Python. Which is the major drawing back of Python that the library code must be written in other language normally in C++ this is because Python is too slow. . Library code must be fast since it is intend to be used by many situations and environment such as in production code. The same go with driver code which control the underlining hardware. . Frameworks and libraries that support Julia . Flux . Flux is Julia home-grown library for ML. It’s design and written in Julia to work with Julia from top to bottom. This include user code, library code, and driver code all is written in Julia which is amazing in its own right. It’s popularity growing by leaf-and-bound. . FastAI . This is the planing state under the label of “under heavy construction”. . Swift . Another new comer in ML field it was develop by Apple as the replacement for their aging Objective-C language which born 30 years agos. . Swift come to fame is its speed that rival C++, it’s being push by Google especially by the Tensorflow team as their next generation language. However, its ecosystem is barren which make it hard for any fledging data scientist to work in. . Framework and libraries that support Swift . TensorFlow . This is not a hugh surprise mind you. It currently has auto differential library which allows user to skip writing backward pass if he/she prefers. This is an important step forward. . FastAI . This is the planing state under the label of “under heavy construction”. . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . numpy doc &#8617; &#8617;2 . | Panda doc &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/13/hotmllang.html",
            "relUrl": "/ml-section/ml/2020/09/13/hotmllang.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Which is my favorite language?",
            "content": ". The would be choice of mine . In the beginning . I have been writting code for a while now that I don’t have enough fingers to count the years. I have used and continue to use these languages since most of the time it dictate by the task and the environment it in. I definite have a favorite one, on the other hand it’s not that important since I rarely have to dicide. . Task to do would lead you to choose the language for example. When I decide to write and Android App it’ been decide which language to choose it going to be either Java or Kotlin this is because Android framework and ecosystem support these two, further more, in the working environment your colleague and company which refine your choice. If they more experience or comfort level in one of these languages. . However, If write this app on my personal time I use Kotlin since is newer and have less wart and annoying work around than Java. It integrate quite seemlessly functional contstruct and exciting new feature such as Asynchronous programming and it’s the future of Android development. . As any shiny new object it does have sharp edge and dark corner one have to be aware of. . Kotlin Vs Java battle of the isle . Both Kotlin and Jave name after patch of land that surround by the sea. Maybe the author of the language feel a little notstalgic when move to the main land. . Java . Java the old king. It’s design to be simple language with simpler set of rule calls syntax. It an Object Oriented paradyme language. It wordship the Object as the deitry beall and endall every thing and only thing in it is Object. You start out writing class and class and class and some more class since only class will set you free at least in Java world view. It also have comarade in war call C# which at war in the early day of C# (C# is copy-cat of Java create by Microsoft as competitor to Java which create by Sun at the time). . The most annoying feature of this is that it force user to repeat the code over. . Kotlin . The new comer . Kotlin remove majority of the paint point of Java and add some nicety of modern tech such as Asynchonous programming, type inference and functional construct. It also make threading concurrency programming a little easier to swallow. You don’t have to jump through loop of fire, and burn mid-night oil to get you paralell code to work correctly. . ——————————————————————————————————— . . Footnotes . markdown .",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/13/favoriteLang.html",
            "relUrl": "/coding-section/coding/2020/09/13/favoriteLang.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Build Web site with fastpages",
            "content": ". Creating Web site using fastpages and Jekyll . As first I thought this going to take alot of work building a web site on your own. Why not pay someone to build one for you I thought. Being a hand-on kind of person I decide to take the plung building my own site using fastpages and Jekyll software to build the blog and hosted it on github site which provide for free hosting. . Learning something new is going to take time and patient. After the initial hiccup* it’s sailing smoothly now. The default site that fastpages created is a little barren for my taste so I have made some modification to fastpages initial site styling to fit my need. I have add a logo image, section link, feature image, image to represent articles. I’ve also hooked up most of social media links. And you can be sure I’ll have more modification as I dig deep inside its gut. I am also planning to improve the site gradually as circumstance permit. . Set up custom domain . I’ve also decided to set up my own domain by purchasing domain name from google. Then set it to point to my github page. To have your very own domain you first have to come up with the domain name you like to use. Then to purchase a domain name googling for google’s Domains site where you can buy and register the domain. Once you purchase the name just fill in the form on the site to register your domain. To use your newly purchase domain instead of github.io domain for your github page you can set it up. This is a two step processes: . At google DNS site to make it point to your github site here | At your github page site to use your new domain instead of [yourname].github.io domain. | To do step 1 you can following this article . here some tips: . The ip address shown there are the ip address of the server at github.io page you have to type them in. | The dig command must be run in the terminal console on your machine this is used to check if your setting at google domain DNS server is correct. | . To do step 2 Goto your repo settings tab at the bottom of the page where the GitHub Pages is setup . Put your domain name in the custom domain box then click save as shown here | Click enforce HTTPS to secure your site | Add a CNAME file at the root folder of your repo (make sure it’s not in any folder) | In the CNAME file add your domain or subdomain (sub domain is the one with www) e.g. www.mydomain.com | Edit the _config.yml file To set the line url: &quot;www.mydomain.com&quot; to match what in the CNAME file | Remove or comment out the line &quot;baseurl&quot; don’t need this anymore see my hiccup* | . | note: . if it won’t let you click on enforce HTTPS you might have to remove your domain then save the empty box then try again until it works . The Hiccup . I thought switching to use my own domain instead of github provide domain should be easy, well until it’s not. First whenever I made changed to the code I’d like see what happen to it so I always open the actions tab on my repos to see the CI in action, well, it throw up a build error when I remove the baseurl, dig into it there is an assert line that check if the baseurl in the config file is the same as in some default settings that cause the error . `assert config[&#39;baseurl&#39;] == settings[&#39;DEFAULT&#39;][&#39;baseurl&#39;], errmsg` . which led me to believe that comment out the baseurl is a bad idea, well I later findout it’s the opposite. It must be comment out inorder to get my site working. This’s delayed my progess afew days. as I posted here . The technical behind the site . Under the hood fastpages use Jekyll as the engine to drive it. Jekyll is a static website and blogs post generator. It handle HTML, CSS, markdown Liquid and more to finally generate my blog. What’s more, I only have to learn Liquid yet another language to my annoyance as if the world needs another language to make it works. And ofcourse, Jekeyll come with its own set of rule here are some of them: . ——————————————————————————— . .",
            "url": "https://www.vanhp.com/general/2020/09/12/BuildSite.html",
            "relUrl": "/general/2020/09/12/BuildSite.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "My first Notebook Blog",
            "content": "About . How does it work I&#39;m confuse as I can work on it on my local machine then sync it to github inorder to see the result? Can I see the result of my editing on my local machine directly without push it to github first? . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.vanhp.com/jupyter/2020/09/10/firstnotebook.html",
            "relUrl": "/jupyter/2020/09/10/firstnotebook.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Activation Functions",
            "content": "Activation functions provide non-linearity to the system and act like a switch to the system. . Activation functions are classified into single kink and double kinks on their graph. The shape of the kink e.g. smooth vs hard kink is depend on scaling value equivalence hard kink does not change scaling factor, while the smooth kink will change the the output value scaling factor . Hard kink vs Soft kink it matter of scaling equivalent . Hard kink it provide scaling of output . when the input multiply by a scalar value the output also have the effect with multiply by same scalar . Soft Kink it change the behavior of output . When the input value e.g. multiply by 100 the output look like it have hard kink . When divide the input with the same value the output look smooth . Activation that have single kink in the graph . which does not change signal only the scale . Rectilinear Unit (ReLU) . There are multiple variation of ReLU activation functions . ReLU(x) = (x)+ = max(0,x) has 0 at bottom and sharp kink for all positive value . | Leaky ReLU (x) = {x if x ≥ 0, negative slope of x otherwise . | . Allow bottom (horizontal asymptote) to have negative value so there still chance to have gradient . PReLU (x) {x, if x ≥ 0 , ax otherwise a = alpha is a leakable parameter . | . The value does not change other than the scale of value a, which different input units . Value alpha may be fixed or learned. It can have different values or a may be shared with different units. A value may not have to be learn but preset to ensure the gradient value is non zero . SoftPlus(x) = 1/beta * log(1 + exp(beta* x)) . | . A smooth approximation of ReLU can be used to constrain the output to always be positive where the beta get large it closer to ReLU . RReLU(x) = {x if x≥ 0, ax otherwise randomize leaky ReLU the same as leaky ReLU . | ELU(x) = max(0,x) + min(0,a * (exp(x) - 1) a softer version of ReLU by adding a small constant to smoothing it. And the bottom(horizontal asymptote) can be negative it allow for system to have average output value to be 0 which is useful for certain situation which have both positive and negative value which allow for faster convergent . | CELU(x) = max(0,x) + min(0,a * (exp(x/a) - 1) have the same effect as ELU . | GELU(x) = x * phi(x) where phi(x) is the cumulative distribution function for Gaussian Distribution . | ReLU6(x) = min(max(0,x),6) has two kink point on the graph at 0 and at saturation point at 6 . | . Activation that have 2 kink in the graphs . Double kink has built in scale where input signal is very different from output signal . Sigmoid(x) = sigma(x) = 1/1+exp(-x) has smooth curve for value between 0 and 1 problem with gradient go to 0 fairly quickly as it get closer to asymptote so the weight of the unit become too large which saturate the unit so the gradient drop fast the learning is not efficient. Sigmoid doesn’t work well when the value normalizes (with batch/group norm) just before it. Since normalization fix the scaling make the mean 0 and variance constance so the system doesn’t depend on the weight but the system now loss the ability to choose with kink to use in the 2 kink like sigmoid . | tanh(x) = tanh(x) = exp(x) - exp(-x)/exp(x) + exp(-x) has graph that identical to sigmoid except it center at 0 and go from -1 to 1 has the output to be closer to 0 mean but not 0 which allow the weight value after it to see both + and - which converge faster. Unfortunately, for deep stacking layers they are not learning efficiently and must be careful in normalizing the value. Which is why the single kink function tend to do better with deep layer network . | Softsign(x) = x/1+ | x | similar to sigmoid except it doesn’t get to the asymptote as fast and doesn’t get stuck toward asymptote as quickly as sigmoid | . | HardTanh(x) = {1 if x ≥ 1, -1 if x ≤ -1,x otherwise the linear region range is -1,1. It has a sharp kink both at negative value at bottom and positive value at top. Work fairly well for small network where the weight is not saturate too much . | . Use in sparse coding . rarely use in neural network . Threshold y = { x, if x threshold, value otherwise . | . rarely use since gradient can not propagate through it . Tanhshrink(x) = x - tanh(x) use mostly on sparse coding to shrinking the latent variable . | softShrinkate(x) = { x - lambda, if x &gt; lambda, x+ lambda, if x &lt; -lambda, 0 otherwise it has sharp kink at both bottom and top of graph . | HardShrink(x) = {x - lambda, if x &gt; lambda, x+ lambda, if x &lt; -lambda, 0 otherwise where value between -lambda and +lambda are set to 0 . | LogSigmoid(x) = log(1/1+exp(-x)) mostly use for loss function instead . | softmin(xi) = exp(-xi)/sum(exp(-xj)) when apply to input Tensor it rescaling the element of vector to the range of [0,1] which the sum to 1 similar to softmax with minus sign in front xi value . | Softmax(xi) = exp(xi)/sum(exp(xj)) same with softmin . | . Temperature is redundant with incoming weight (beta value) matter when the input size is fixed due to normalization. It uses to control how hard(the different between large and small value in the output vector) the distribution of the output value will be it the system to make hard decision even though it doesn’t learn as well as before. It used in case of mixture of expert system when multiple subnetwork output are linear combine with coefficients from the output of the softmax which may be controlled by another NN with soft mixture with low beta then gradually increase to infinity the will force NN to choose one and ignore the rest this may safe compute time (call annealing) . LogSoftmax(xi) = log(exp(xi)/sum(exp(xj))) use at the output as part of the loss function . | .",
            "url": "https://www.vanhp.com/2020/09/10/Activation-Functions.html",
            "relUrl": "/2020/09/10/Activation-Functions.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Loss Functions",
            "content": "MSELoss() . Measure the mean square error (L2 norm) between each element in input x and target y . where N is the batch size . L1Loss() . Basically the absolute value of the different between desired output and actual output . where N is the batch size . Use for robust regression where small error to count more and large not as much to reduce noise in data e.g. to reduce the influent of a few outlier in dataset but L1 loss is not differentiable at the bottom when gradient get closer to the bottom normally is done with soft shrink, some fix of this problem e.g. smooth L1 loss to make bottom smooth like L2 with Hubert loss to protect against outliers . SmoothL1Loss() . otherwise . Note the average value make image blur . NLLLoss() . The negative log likelihood loss useful for reduce discrepancy among dataset . . Try to pick the correct value in the output vector as large as possible . If there is a negative in front it can be interpret x as energy . Useful for widely different frequency (size) data category this by assign larger weight for the category that have small dataset and smaller weight to category that have large among of data, but better solution is to increase the frequency of minimal category instead by reuse the same set of smaller dataset over until the larger dataset exhaust to equalize them out this method exploit SGD better.Never leave any data unused . CrossEntropyLoss() . Merging the log softmax(do log after the softmax) and NLLLoss is for numerical reasons . Since the log softmax step might have gradient value infinite in the middle of the step (because some values in softmax might be very small close to 0 value take log of this value result in negative infinity ) and gradient of this value also close to infinity Alway do the softmax and log in the same time to avoid this problem to get a more stable numerical value. By combining these 2 it makes the correct value largest as possible and suppresses all the others scores as small as possible due to normalization. The loss take and X factor and a category a desired category a class then compute the negative log of softmax applied to vector of scores the numerator is the X of the index of the correct class that is the loss the negative log of exponential score of the correct class divided by the sum of the exponential of all the scores. The X can be thought of as negative energy. The negative score of the correct class and to make this value small make the score large and all the sum of exponential of other value small this make the edge small . Cross entropy is the cross of distribution of the softmax vector and target value vector (one hot)that have one value that close to 1 . AdaptiveLogSoftmaxWithLoss() . Extension to log softmax useful with the dataset that have large categories e.g. in NLP by use some like ignore the small categories and focus on the large one . BCELoss() . . . KLDivLoss() . where N span all dimension of input and L have the same shape as input if reduction is not none . . May have numerical issue . BCEWithLogitsLoss() . . where N span all dimension of input and L have the same shape as input if reduction is not none . . It take value and pass to sigmoid and make sure the value is between 0 and 1 . MarginRankingLoss() . . Make one of the input larger than the other by value of the margin, y is the value that control x1 and x2 . TripletMarginLoss() . Use to measuring a relative similarity between samples. It compose of anchor,p positive,n negative . . Try to get the distance (value ) good pair smaller than the distance of the bad pair . SoftMarginLoss() . Optimize a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1) . . Which the positive value closer and negative value further . MultiLabelSoftMarginLoss() . Optimize a multi-label one-versus-all loss based on max-energy, between input x and target y of size (N,C). for each sample in the minibatch . %20%2B%20(1-y%5Bi%5D)%20*log(%20%5Cfrac%7B%20exp(-x%5Bi%5D)%7D%7B(1%2Bexp(-x%5Bi%5D))%7D%20)#0) . Where . Allow multiple correct output (want multiple categories can have high score and all lower score get suppressed) which for the desire category want the value to be higher and the undesired value suppress . MultiMarginLoss() . %5Ep%7D%7D%7B%7B%20x.size(0)%7D%7D#0) . Where . HingeEmbeddingLoss() . Measure the loss given an input tensor x and a label tensor y (containing 1 or -1). This is usually used for measuring whether two input are similar or dissimilar, e.g. using the L1 pairwise distance as x, and is typically used for learning nonlinear embeddings or semi-supervised learning . The loss function for n-th sample in the mini-batch is . . And the total loss function is . . Where . CosinEmbeddingLoss() . Measure the loss given input tensors x1,x2 and a Tensor label y with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning . The loss function for each sample is: . 1 - cos(x_1,x_2), &amp; quad text{if} y = 1 . max(0,cos(x_1,x_2) - margin) &amp; quad text{if} y = -1 . f(n) = . begin{cases} . n/2 &amp; quad text{if } n text{ is even} . -(n+1)/2 &amp; quad text{if } n text{ is odd} . end{cases} . For positive case make the two vector align a much as possible . For negative try to make the cos value orthogonal to each other . CTCLoss() (connectionist Temporal Classification loss) . Calculate loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many to one”, which limits the length of the target sequence such that it must be ≤ the input length . Use in waveform classification where the speed may play a role e.g. a word was pronoun slowly may generate multiple similar waveform/fast may generate less waveform should be map to the same word regardless of speed of speaker . Architecture and Loss Function . General form of energy base model . Family of energy function . . Training set . . Loss functional/Loss function . Functional is function of a function . . Measure the quality of an energy function on training set . Training . . Form of the loss function . Invariant under permutations and repetition of the samples . . Where L per sample loss . Y desired answer . E energy surface for a given Xi as Y varies . R regularizer . Energy Loss function example . Energy loss . . Simply pushes down on the energy o the correct answer . Negative Log-Likelihood Loss . . Have a behavior of pushing down the correct answer push up on energy of incorrect value in proportion to their probability . . A simpler Loss function: Perceptron Loss . . Push down the energy of right answer . | Push up on energy of wrong answer . | Always, positive . | Zero when answer is correct . | No margin does not prevent energy surface from being almost flat . | Work well in practice, particularly if the energy parameterization does not allow flat surfaces . | Call “discriminative Viterbi training” in speech and hand written literature . | Not work well with non linear system . | . Generalized Margin Loss . Most offending Incorrect answer: discrete case . Definition 1 Let Y be a discrete variable. Then for a training sample . The most offending incorrect answer is the answer that has the lowest energy among all the answer that are incorrect: . . Most offending Incorrect answer: discrete continuous case . Definition 2 Let Y be a continuous variable. Then for a training sample the most offending incorrect answer is the answer that ha the lowest energy among all answer that are at least in away from the correct answer; . . Hinge Loss . . With the linearly-parameterized binary classifier architecture, we get linear SVM . . Log loss a soft hinge loss . With the linearly-parameterized binary classifier architecture, we get linear Logistic Regression . Square-Square Loss . [](https://www.codecogs.com/eqnedit.php?latex=%20L_%7Bsq-sq%7D(W%2CY%5Ei%2CX%5Ei)%20%3D%20E(W%2CY%5Ei%2CX%5Ei)%5E2%20%2B%20(%20max(0%2Cm%20%2B%20E(W%2CY%5Ei%2CX%5Ei)%20)%5E2%20#0) . Appropriate for positive energy function . A more general form of Hinge type Loss . . H is Hinge energy . E another energy . C margin .",
            "url": "https://www.vanhp.com/2020/09/09/Loss-Functions.html",
            "relUrl": "/2020/09/09/Loss-Functions.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "The math and code don't mix?",
            "content": ". Take the red pill! . Anyone who take the red pill and jump into ML pool will soon swim amoung the stream of equations, formula. there no avoiding it. Since you’re already take the pill you might as well get use to it. If your day of school are long behind you. The best advice is go to Khan academy site to refresh your memory on them. Two of the main math subjects you need to have firm grasp of are Linear algebra, and differentiation. . Linear algebra . Matrix and Tensor Matrix operations are essential. Matrix play a major roll in Neural network operation give you some insight into how neural network works. And allow you to manupulate them to work for you. . linear function L = mx + b . def linear = weight@input + bias . gradient . def grad(xb,yb,model): pred = model(xb) loss = mnist_loss(pred,yb) loss.backward() . Calculus . Do you still remember the chain-rule? If this didn’t ring a bell, it’s time to bush-up on it. It’s at the heart of how the Neural network learn in the process call SGD and it is the logo of this site, take a little peek at the top of the page. The great news is that you don’t have to do the math calculation yourself the Framework whichever one you choose will do this for you, your job is to understand them inorder to take full advantage of the tool you use. . logarithm log(a*b) = log(a)+log(b) | . Statistic . Probability, Median, average/mean, variance, standard deviation … use for analysis of data and the result. . Sigmoid equation in math: . $ sigmoid(x) = frac{1}{1 + ln^{-x}}$ . Sigmoid function definition . in Python . def sigmoid(x): return 1/(1 + torch.exp(x)) . Jagons: . Python broadcasting vectorization elementwise operation einsum *kwarg *arg decorator partial (partial application) . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . lambda . . ———————————————————————————- . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/09/07/ML-mathcode.html",
            "relUrl": "/ml-section/ml/2020/09/07/ML-mathcode.html",
            "date": " • Sep 7, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "The Polyglot Coder Journey 1",
            "content": ". Welcome to my Polyglot Coder view . The rear view mirror . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/05/coding-post1.html",
            "relUrl": "/coding-section/coding/2020/09/05/coding-post1.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The OOP vs Functional fight!",
            "content": ". Example Markdown Post . Basic setup . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/09/02/coding-post3.html",
            "relUrl": "/coding-section/coding/2020/09/02/coding-post3.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "How many languages do you need to succeed?",
            "content": ". Example Markdown Post . Basic setup . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/coding-section/coding/2020/08/18/coding-post2.html",
            "relUrl": "/coding-section/coding/2020/08/18/coding-post2.html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "The Polyglot Coder General rant!",
            "content": "Welcome to my random rant! anything that don’t fell into either ML or programming . The rear view mirror . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/general/2020/08/16/general-post1.html",
            "relUrl": "/general/2020/08/16/general-post1.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Training Model",
            "content": ". Basic step in training model:&lt;/span&gt; . Initialize the weights | For each input image use these weights to predict whether the outcome e.g. (0…9) | Based on the prediction calculate how good the predict is what’s the loss (error) | Calculate the gradient derive from each weight to see how changing the weight affect the loss | Update the weights base on the calculation | Repeat step 2-5 for all images | Keep doing these until outcome is satisfy or quit | All images . must be in the same dimension(size)can collate into tensors to be passed to the GPU | will be converted into matrice | . Initialization . must be initialized with random value | . Matrix multiplication is usually in dot-product e.g. m dot x, or m@x in Python . gradient is calculated using the Calculus chain-rule: . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . def simpleNN(xb): l1 = xb@w1 + b1 l1 = l1.max(tensor(0.0)) make neg value into 0 ignore + (relu) l1 = l1@w2 + b2 return l1 . this simple little has some interesting features . It work like universal approximation theorem whhich can be used to approx any function no matter how complicate it is | It call Neural Network | It build via composition which take the result of one function then pass it into a new function on and on PyTorch has name this as sequential | Linear = x@w + bias . non linear . Non linearity is needed to turn the network into more useful function, since linear + linear -&gt; another linear function . reLu: max(tensor(0,0)) . sigmoid: output binary categories between 0 and 1 torch.where can be used to pick from one of these (output must contain only 2 categories) . def sigmoid(x): return 1/(1 + torch.exp(x)) . softmax: output more than 2 categories which have value all add up to 1 has a side effect of exagerate the large value and diminist smaller value ‘cause of exponential effect . def softmax(x): return exp(x)/exp(x).sum(dim=1,keepdim=true) . It works even when our dependent variable has more than two categories. It results in faster and more reliable training. In order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like. . Loglikelyhood is and indexer which use to pick a value from the list of value output from softmax PyTorch provide nll_loss assumes that you already took the log of the softmax, so it doesn’t actually do the logarithm must use log_softmax (faster and more accurate to take a log at this stage) before nll_loss Negativeloglikelyhood NLL this function simply apply minus to the value to remove negative value . Cross-Entropy Loss nn.CrossEntropyLoss (does log_softmax and then nll_loss) Cross-entropy loss is a loss function that is similar to the one we used in the previous chapter, but (as we’ll see) has two benefits: When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss. . DataLoader will iterate over the collection input and return tuple of data in collection . Helpers function for code simplication: DataLoaders learner . create a simple neural network by hand . Note: Pytorch Dataset return a tuple when indexing into it . Data Handling . Data augmentation: . Process to increase dataset where the data is hard to collect and help improve model performance by manipulate the images in a natural ways. . Data cleaning . A process to remove incorrect,missing,unnecessary info in the data set collect and make them usable by the model . tip: fastai can help in data cleaning by let the model work on the data then fix the error where the model has problem with . DataBlock . use summary to debug . Presizing: . A special technique use by fastai to improve image manipulation with high efficiency and minimal degradation It uses strategies: . Resize images to relatively “large” dimensions—that is, dimensions significantly larger than the target training dimensions to have spare margin for more transformation with no empty zone by resizing to a square using a large random crop size that span the smaller width or height | . | Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times. | Training the model: . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Visualizing Data and result . To understand and diagnose the output from the model various visualization are available . Confusion matrix show grid of predict vs actual value most_confused for model get the most often wrong answer (interp.most_confused(min_val=5)) to show model confusion the diagonal line is correct a 1 is any place else show mis-prediction | . | . How the model learn . from Zeiler and Fergus paper which show image of how the model learn . The earlier layer learn about structure e.g. line,circle, edge, area | Each layer after that learn more more semantic by using info from earlier layer to form meaning | the lastest layer are closer to actual object | . Improve model performance . Learning rate . the right learning rate is important the help improve model performance . Low learning rate may increase training epoch or overfitting | Large learning rate may overshoot the minimum loss may decrease performance | Use the learning rate finder (Learner.lr_find) tool will do these step: start with a very, very small learning rate then double the size for each mini-batch | Check the loss if it get worse stop then back-up to the last mini loss that the best one Then start with value smaller than this value pick either | One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10 `1e-6 to 1e-3) | The last point where the loss was clearly decreasing | . | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . return the minimum point and steep point in log scale steep point show the model making great progress (learning) minimum show the model stop making progress (not learning) . Transfer Learning . the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task . By remove the head which consist of the last layer that do the predicion (softMax layer) which specific to a task | replace the head with new one (with random weight) that suitable to new task tell the optimizer to only update the weights in those randomly added final layers | Don’t change the weights in the rest of the neural network by freeze them | . | Keep the body | . Fastai Learner.fine_tune methods . Trains the randomly added layers for one epoch, with all other layers frozen | Unfreezes (learner.unfreeze) all of the layers, and trains them all for the number of epochs requested fine_tune function does: | freeze (body) | fit_one_cycle | base_lr/2 | unfreeze (body) | fit_one_cycle (train all parts together) note after fine_tune you may need to pick a new learning rate to help improve the performance since all parts may not have trained enough together discriminative learning rate is more appropriate at this point | . Fastai 1 cycle policy . A fastai technique for training the model It start with low learning rate then gradually increase the learning rate until it reach the max value specify by the user then it stop for the first 1/3 of the batch, for the rest of the batch it gradually decrease the learning rate Learner.fit_one_cycle . note: The recommend approach is to retrain with smaller epoch with the model overfit usally the validation loss start getting worse than train error . Discriminative Learning Rates . Since the body has already been trained it doesn’t need the same learning rate the head portion Discriminative Learning Rates allow for training the head and body with different learning rate . the body is trained with lower learning rate | the head is trained with higher learning rate | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . slice(learning_rate for body,learning_rate for head) . Training period . if you find that you have overfit, what you should actually do is . retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found. | use a deeper architecture (more layers) it takes more resources memory,CPU/GPU cycles | take more time (use fp16 to speed up) | . | . Choosing the right architect . Many achitect comes with various level of layers e.g. ResNet 18,34,50,101 pretrained on ImageNet Pick the right one is a trial-error process . Pandas and DataFrames . Note in Numpy,Panda,PyTorch (trailing : is optional) e.g. df.iloc[0,:] -&gt; df.iloc[0] . Jargons: . parameter are weight and bias | SGD : stochastic gradient decent calculate the gradient using a small set of data in practice is using a loop over a mini-batch to calculate the GD | GD: gradient descent calculate using the whole dataset all at once | broadcasting a matrix calculation to speed up without using loop by multiply the same scalar to all the value in the matrix then add them up | mini-batch: a small set of data with label | RelU: function that convert - value to 0 leave + value alone | forward pass: the process that calculate the prediction value | loss function: function that calculate the error the different between actual value - predict value | backward pass: the process that calculate the adjustment value use to update the parameter to reduce the loss | learning rate: size of SGD value use to update the parameters for every loop | activation value: these number are calculated from output of linear and nonlinear | parameter value: adjustable value use to improve the performance | Tensor: multidimention arrays with regular shape rank 0: scalar | rank 1: vector | rank 2: marix | rank 3: 3D tensor | rank 4: 4D tensor | . | Neural network: comprise with layers | layer: groups of neural (node) arrange as linear and non-linear output of a layer is pass-on to the layer next to it | fit: is same training | Dataset: a colllection of data that returns a tuple of x,y variable of a single item in the collection | DataLoader: an iterator that return stream of mini-batch. Where each minibatch is comprise with batch of x,y variables | . ————————————————————————————————— . . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/05/12/ML-train.html",
            "relUrl": "/ml-section/ml/2020/05/12/ML-train.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Multi-label Classification",
            "content": ". Refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object or more or no object present. This simple solution is not at all widely understood or appreciated! . Pandas and DataFrames . Pandas a Python library for database, it’s fast and flexible library but has non-intuitive interface. It’s used to stored manage analize structure data type. It’s also useful in ML to handle tabular and timeseries data type. . Dataframe is the main class that represent data in table of row and columns. It can import data from CSV file, from directories, and many others sources. And Jupyter is happily work with it. . import Pandas as pd df = pd.read_csv(path/&#39;train.csv&#39;) df.head() tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) . DataBlock . A fastai versatile,flexible object that manage data. it has facility to handle Datasets,dataframe. It smart enough to understand data set that is working with. It’s also auto split data into train 80% validate 20% . # empty datablock dblock = DataBlock() dsets = dblock.datasets(df) len(dsets.train),len(dsets.valid) x,y = dsets.train[0] #take lambda/ or function as argument to retrieve filename and label dblock2 = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets2 = dblock.datasets(df) dsets2.train[0] # take default name dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . Binary cross entropy Loss . One of the problem with multiple label is finding appropiate loss function that can handle the possibility of multi-label and no label may present in data. The regular cross entropy loss is only able to deal with single label. Finding loss function that suitable for case where there more than one label or no label present is needed. . Learner object . The Learner object inherit from nn.Module comprise with: . The model | DataLoaders object | Optimizer | Loss function | It can receive x as a mini-batch and return activation value . learn = cnn_learner(dls, resnet18) activ = learn.model(x) activ.shape def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() # using PyTorch version # by default fastai will use this version loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . due to one-hot-encoded dependent variable because there may be more than object the cross_entropy is not appropriate In the case where more than one object is possible, or no object present may want the sum to be &lt; 1 this excluse softmax and nll_loss . softmax: limitation: . all value must sum to 1 | the largest value get exagerate nll_loss limitation: | it return only one value correspond to the single label we need to return more than 1 label | . Comparable PyTorch versions that work with one-hot encoding: . F.binary_cross_entropy (no sigmoid) | nn.BCELoss (nosigmoid) | F.binary_cross_entropy_with_logits | nn.BCEWithLogitsLoss Single label version: F.nll_loss nn.NLLLoss (no softmax) F.cross_entropy nn.CrossEntropyLoss | . Measurement metric . Since there maybe more than one or object present need a new metric for accuracy . # value &gt; thresh is consider 1, else consider 0, default threshold = 0.5 def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() # train the model learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . High threshold select the high confident of model Low threshold select low confident of model . find the best threshold by trying a few levels and seeing what works best . # this apply sigmoid by default preds,targs = learn.get_preds() # no sigmoid accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) # plot it xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs) . Note . zip(b[0],b[1]) has short cut of zip(*b) | python can’t save (serialization lambda) lambda use function instead | it’s OK to use validation set to pick threshold it should not overfit | . Jagons . x: is independent variable | y: is dependent variable | Dataset: A collection that can be index into it and returns a tuple of your independent and dependent variable for a single item | DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | one-hot-encoder: a vector that has all 0 but one 1 to represent the interested item to pick-out from other item | . Fastai library . Datasets: An object that contains a training Dataset and a validation Dataset part of fastai library | DataLoaders: An object that contains a training DataLoader and a validation DataLoader part of fastai library | . . —————————————————————————————————— Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/03/16/ML-info.html",
            "relUrl": "/ml-section/ml/2020/03/16/ML-info.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.vanhp.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "The first step in long journey with ML",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.vanhp.com/ml-section/ml/2020/02/12/ML-firststep.html",
            "relUrl": "/ml-section/ml/2020/02/12/ML-firststep.html",
            "date": " • Feb 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "under heavy construction. Please be patient! . I’ve been programming for over decades now. My Journey start from C to C++ then on to Java, C#, F#, python, JavaScript, Julia, Swift, Scala, Kotlin. I’m using them on DOS, Windows 95/98/NT/8,10, Sun solaris, Linux variance such as Slackware,Gentoo,Redhat,fedora,ubuntu and Mac. . Programming Languages and API: . • C# 6 years ● .NET 8 years ● C/C++ 7 years ● Object Oriented ● Java 5 years ● Python 3 years ● SDLC • JavaScript 3 years ● bash 4 years ● Kotlin 2 years ● Octave/Matlab 1 year ● Swift 1 year • Networking TCP/IP/UDP/HTTP/FTP ● WCF ● Multi-thread ● RDBMS/ORM ● Windows GUI . Platform and Frameworks: . - Operating Systems: ▪ Linux: Ubuntu, Red Hat Enterprise ▪ Windows: 98/NT/2000/XP/Vista/7/8/10 ▪ Mac • Framework/libraries: Android, dotNet Framework 2/3/3.5/4/4.5, WCF, MFC,WPF, win32 API • Database/ORM: MS SQL 2008/12, Oracle 10g, Entity Framework 4.1/6, SQLite • AI/Machine Learning: PyTorch, TensorFlow, FastAI, Numpy, Panda . Tools and Testing: . • Source control: Git, TFS, CVS/SVN/Rational ClearCase, itrack, JIRA/Agile/Scrum • Testing: NUnit/MSTest/JUnit . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.vanhp.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
  

  
  

  

  
      ,"page5": {
          "title": "Resources",
          "content": ". Here are some of the useful sites that I have come across: . AI/ML field . Jeremy Howard He is one of the expert in the AI/ML field. The greating about it is that he give all of this execllent material for free. If you want to learn about AI/ML here is the place to start. And his fastai library which current reach version 2 is the state-of-art in the field that is rapidly moving. And ofcourse, fastpages the library this site is used the testament to his brilliant. . Andrew Ng The founder of Coursera and the pre-eminent current AI expert. You can some of his material on Youtube and on Coursera. . Yann Lecun The Turing Award winner and some calls him the father of modern AI/ML. You can find his lecture of Youtube especially on current hot topic in self-supervise learning. . Software development field . Douglas Schmidt is one of prolific professor of computer science. He has ton of contents about sofware and especially in C++ and Java, Android. This is where I come when I feel a little rusty on C++ and Java. His material are up up to date and top-notch. . Computational Linear Algebra for Coders. https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html https://explained.ai/matrix-calculus/ https://drscotthawley.github.io/blog/ https://betterexplained.com/articles/linear-algebra-guide/ https://arxiv.org/abs/2006.03511 https://arxiv.org/pdf/2005.13754.pdf https://github.com/FluxML/FastAI.jl https://developer.android.com/ml https://course.fullstackdeeplearning.com/course-content/setting-up-machine-learning-projects https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#4 https://github.com/spaceflint7/bluebonnet . . Books . Data science and Panda,Numpy Python for Data Analysis . . Music . Coding is tough job you evitably run into hard problem to can’t solve in timely maner. You’ll need some music to soothing your pain and smooth out the day. . Some of my favorite tunes: . Relax music for focus during coding session relax music . Thai music . Nanglen Old timer that make music for life as much as for ears ดอกไม้ในที่ลับตา เพราะเธอ . pop music . Num Kala เธอเป็นแฟนฉันแล้ว . Country music . Takatan รอคนถัดไป รอคนถัดไป ขอจองไว้ในใจ .",
          "url": "https://www.vanhp.com/Resource-section/",
          "relUrl": "/Resource-section/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "ML",
          "content": "Welcome to my Machine Learning blog Home page .",
          "url": "https://www.vanhp.com/ML-section/",
          "relUrl": "/ML-section/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Programming",
          "content": "Welcome to my Programming blog Home page .",
          "url": "https://www.vanhp.com/Coding-section/",
          "relUrl": "/Coding-section/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page16": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.vanhp.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}